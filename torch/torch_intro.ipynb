{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '/Users/dc/miniconda3/envs/torch/lib/python36.zip',\n",
       " '/Users/dc/miniconda3/envs/torch/lib/python3.6',\n",
       " '/Users/dc/miniconda3/envs/torch/lib/python3.6/lib-dynload',\n",
       " '/Users/dc/miniconda3/envs/torch/lib/python3.6/site-packages',\n",
       " '/Users/dc/miniconda3/envs/torch/lib/python3.6/site-packages/torchvision-0.1.9-py3.6.egg',\n",
       " '/Users/dc/miniconda3/envs/torch/lib/python3.6/site-packages/IPython/extensions',\n",
       " '/Users/dc/.ipython',\n",
       " '']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      "[torch.IntTensor of size 5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "#do not do this.. but it runs\n",
    "\n",
    "one = torch.IntTensor(1)\n",
    "five = torch.IntTensor(5)\n",
    "\n",
    "result = one * five\n",
    "print(result)\n",
    "\n",
    "# the result is the multiplication and broadcating of 2 unitialized arrays of size 1 and 5. The result is a random result. This\n",
    "#is different everytime you run it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum: \n",
      " 2\n",
      "[torch.IntTensor of size 1]\n",
      "\n",
      "result: \n",
      " 4\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "#have to use numpy notation to create a tensor initialized to a value, if you do IntTensor[1] this just creates\n",
    "#space for the tensor w/o values. \n",
    "\n",
    "one = torch.IntTensor([1])\n",
    "two = torch.IntTensor([2]) \n",
    "sum = one * two\n",
    "print('sum:',sum) \n",
    "\n",
    "#this wont run on colab.. back to the server...\n",
    "#type torch.cuda.FloatTensor to get this to run on the GPU\n",
    "one_gpu = torch.FloatTensor([1]).type(torch.cuda.FloatTensor)\n",
    "two_gpu = torch.FloatTensor([4]).type(torch.cuda.FloatTensor)\n",
    "\n",
    "result = one_gpu * two_gpu\n",
    "print('result:',result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.grad:  Variable containing:\n",
      " 2\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "w.grad: Variable containing:\n",
      " 1\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "b.grad:  Variable containing:\n",
      " 6\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "-----------------\n",
      "x.grad.data:  \n",
      " 2\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "w.grad.data:  \n",
      " 1\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "b.grad.data:  \n",
      " 6\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# What is autograd? Based on a module autograd which calculates the gradients based on defining a DAG using foward propagation or\n",
    "# the linear equation below. Once the graph is defined the autograd calculates the backprop iterations. \n",
    "# there are multiple components to autograd \n",
    "#\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "\n",
    "#define the graph with Variables=Nodes.\n",
    "x = Variable(torch.Tensor([1]), requires_grad = True)\n",
    "w = Variable(torch.Tensor([2]), requires_grad = True)\n",
    "b = Variable(torch.Tensor([3]), requires_grad = True)\n",
    "\n",
    "#graph edges are + and *\n",
    "y = w * x + b*b\n",
    "\n",
    "#backward() calculates the derivative WRT all variables with requires_grad=True \n",
    "y.backward()\n",
    "print('x.grad: ', x.grad)\n",
    "print('w.grad:', w.grad)\n",
    "print('b.grad: ', b.grad)\n",
    "print('-----------------')\n",
    "print('x.grad.data: ', x.grad.data)\n",
    "print('w.grad.data: ', w.grad.data)\n",
    "print('b.grad.data: ', b.grad.data) #very cool 2*b is derivative of dy/db. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(use_cuda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "/home/dc/DeepLearning/torch\n",
      "Read 135842 sentence pairs\n",
      "Trimmed to 10853 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "fra 4489\n",
      "eng 2925\n",
      "['tu m ennuies .', 'you re boring .']\n"
     ]
    }
   ],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# http://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s\n",
    "\n",
    "def readLangs(lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines\n",
    "    print(os.getcwd())\n",
    "    #os.list(os.path.join(os.getcwd(),'data/eng-fra.txt'))\n",
    "    lines = open('/home/dc/DeepLearning/torch/data/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
    "        read().strip().split('\\n')\n",
    "\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "MAX_LENGTH = 10\n",
    "\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s\",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "\n",
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH and \\\n",
    "        p[1].startswith(eng_prefixes)\n",
    "\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]\n",
    "\n",
    "\n",
    "def prepareData(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData('eng', 'fra', True)\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers=1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        for i in range(self.n_layers):\n",
    "            output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        result = Variable(torch.zeros(1, 1, self.hidden_size))\n",
    "        if use_cuda:\n",
    "            return result.cuda()\n",
    "        else:\n",
    "            return result\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, n_layers=1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        for i in range(self.n_layers):\n",
    "            output = F.relu(output)\n",
    "            output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        result = Variable(torch.zeros(1, 1, self.hidden_size))\n",
    "        if use_cuda:\n",
    "            return result.cuda()\n",
    "        else:\n",
    "            return result\n",
    "\n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, n_layers=1, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_output, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)))\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        for i in range(self.n_layers):\n",
    "            output = F.relu(output)\n",
    "            output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]))\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        result = Variable(torch.zeros(1, 1, self.hidden_size))\n",
    "        if use_cuda:\n",
    "            return result.cuda()\n",
    "        else:\n",
    "            return result     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "\n",
    "def variableFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    result = Variable(torch.LongTensor(indexes).view(-1, 1))\n",
    "    if use_cuda:\n",
    "        return result.cuda()\n",
    "    else:\n",
    "        return result\n",
    "\n",
    "\n",
    "def variablesFromPair(pair):\n",
    "    input_variable = variableFromSentence(input_lang, pair[0])\n",
    "    target_variable = variableFromSentence(output_lang, pair[1])\n",
    "    return (input_variable, target_variable)\n",
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "\n",
    "def train(input_variable, target_variable, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_variable.size()[0]\n",
    "    target_length = target_variable.size()[0]\n",
    "\n",
    "    encoder_outputs = Variable(torch.zeros(max_length, encoder.hidden_size))\n",
    "    encoder_outputs = encoder_outputs.cuda() if use_cuda else encoder_outputs\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_variable[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0][0]\n",
    "\n",
    "    decoder_input = Variable(torch.LongTensor([[SOS_token]]))\n",
    "    decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_output, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_variable[di])\n",
    "            decoder_input = target_variable[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_output, encoder_outputs)\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            ni = topi[0][0]\n",
    "\n",
    "            decoder_input = Variable(torch.LongTensor([[ni]]))\n",
    "            decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "\n",
    "            loss += criterion(decoder_output, target_variable[di])\n",
    "            if ni == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.data[0] / target_length\n",
    "\n",
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
    "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    training_pairs = [variablesFromPair(random.choice(pairs))\n",
    "                      for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_variable = training_pair[0]\n",
    "        target_variable = training_pair[1]\n",
    "\n",
    "        loss = train(input_variable, target_variable, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:tornado.application:Exception in callback functools.partial(<function wrap.<locals>.null_wrapper at 0x7fae7ea63d08>)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3/dist-packages/tornado/ioloop.py\", line 592, in _run_callback\n",
      "    ret = callback()\n",
      "  File \"/usr/lib/python3/dist-packages/tornado/stack_context.py\", line 275, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/usr/lib/python3/dist-packages/IPython/kernel/zmq/ipkernel.py\", line 262, in enter_eventloop\n",
      "    self.eventloop(self)\n",
      "  File \"/usr/lib/python3/dist-packages/IPython/kernel/zmq/eventloops.py\", line 151, in loop_tk\n",
      "    kernel.timer = Timer(doi)\n",
      "  File \"/usr/lib/python3/dist-packages/IPython/kernel/zmq/eventloops.py\", line 139, in __init__\n",
      "    self.app = Tk()\n",
      "  File \"/usr/lib/python3.5/tkinter/__init__.py\", line 1871, in __init__\n",
      "    self.tk = _tkinter.create(screenName, baseName, className, interactive, wantobjects, useTk, sync, use)\n",
      "_tkinter.TclError: no display name and no $DISPLAY environment variable\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "    input_variable = variableFromSentence(input_lang, sentence)\n",
    "    input_length = input_variable.size()[0]\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_outputs = Variable(torch.zeros(max_length, encoder.hidden_size))\n",
    "    encoder_outputs = encoder_outputs.cuda() if use_cuda else encoder_outputs\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_variable[ei],\n",
    "                                                 encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_outputs[ei] + encoder_output[0][0]\n",
    "\n",
    "    decoder_input = Variable(torch.LongTensor([[SOS_token]]))  # SOS\n",
    "    decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    decoded_words = []\n",
    "    decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "    for di in range(max_length):\n",
    "        decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "            decoder_input, decoder_hidden, encoder_output, encoder_outputs)\n",
    "        decoder_attentions[di] = decoder_attention.data\n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "        ni = topi[0][0]\n",
    "        if ni == EOS_token:\n",
    "            decoded_words.append('<EOS>')\n",
    "            break\n",
    "        else:\n",
    "            decoded_words.append(output_lang.index2word[ni])\n",
    "\n",
    "        decoder_input = Variable(torch.LongTensor([[ni]]))\n",
    "        decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "\n",
    "    return decoded_words, decoder_attentions[:di + 1]\n",
    "\n",
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 8s (- 10442m 49s) (1 0%) 7.9747\n",
      "0m 8s (- 5250m 11s) (2 0%) 7.9085\n",
      "0m 8s (- 3525m 22s) (3 0%) 7.8888\n",
      "0m 8s (- 2653m 52s) (4 0%) 2.9332\n",
      "0m 8s (- 2136m 51s) (5 0%) 7.8095\n",
      "0m 8s (- 1797m 48s) (6 0%) 7.8638\n",
      "0m 8s (- 1553m 22s) (7 0%) 7.5794\n",
      "0m 8s (- 1368m 34s) (8 0%) 3.3064\n",
      "0m 8s (- 1233m 31s) (9 0%) 7.7814\n",
      "0m 8s (- 1123m 55s) (10 0%) 7.6058\n",
      "0m 9s (- 1036m 23s) (11 0%) 7.5798\n",
      "0m 9s (- 959m 11s) (12 0%) 7.5942\n",
      "0m 9s (- 891m 11s) (13 0%) 3.1201\n",
      "0m 9s (- 836m 34s) (14 0%) 7.6638\n",
      "0m 9s (- 786m 6s) (15 0%) 6.9348\n",
      "0m 9s (- 740m 45s) (16 0%) 4.1981\n",
      "0m 9s (- 701m 55s) (17 0%) 4.3298\n",
      "0m 9s (- 667m 51s) (18 0%) 3.9792\n",
      "0m 9s (- 636m 39s) (19 0%) 3.3565\n",
      "0m 9s (- 609m 5s) (20 0%) 3.5288\n",
      "0m 9s (- 583m 32s) (21 0%) 3.2057\n",
      "0m 9s (- 560m 33s) (22 0%) 7.3018\n",
      "0m 9s (- 539m 25s) (23 0%) 2.5519\n",
      "0m 10s (- 521m 42s) (24 0%) 6.5120\n",
      "0m 10s (- 504m 0s) (25 0%) 2.2865\n",
      "0m 10s (- 488m 54s) (26 0%) 6.9623\n",
      "0m 10s (- 473m 34s) (27 0%) 4.5783\n",
      "0m 10s (- 459m 6s) (28 0%) 3.6834\n",
      "0m 10s (- 446m 45s) (29 0%) 6.2199\n",
      "0m 10s (- 435m 41s) (30 0%) 5.0636\n",
      "0m 10s (- 425m 10s) (31 0%) 5.4273\n",
      "0m 10s (- 414m 35s) (32 0%) 5.6440\n",
      "0m 10s (- 406m 30s) (33 0%) 7.0155\n",
      "0m 10s (- 397m 47s) (34 0%) 1.8961\n",
      "0m 10s (- 389m 51s) (35 0%) 5.6461\n",
      "0m 10s (- 381m 27s) (36 0%) 4.8433\n",
      "0m 11s (- 373m 34s) (37 0%) 5.7030\n",
      "0m 11s (- 366m 22s) (38 0%) 4.7430\n",
      "0m 11s (- 358m 33s) (39 0%) 3.3072\n",
      "0m 11s (- 351m 36s) (40 0%) 4.7524\n",
      "0m 11s (- 346m 34s) (41 0%) 5.9820\n",
      "0m 11s (- 339m 45s) (42 0%) 3.7077\n",
      "0m 11s (- 335m 1s) (43 0%) 6.5282\n",
      "0m 11s (- 329m 56s) (44 0%) 5.2201\n",
      "0m 11s (- 323m 53s) (45 0%) 3.0081\n",
      "0m 11s (- 318m 8s) (46 0%) 4.3232\n",
      "0m 11s (- 312m 49s) (47 0%) 2.4508\n",
      "0m 11s (- 307m 38s) (48 0%) 4.2387\n",
      "0m 11s (- 303m 16s) (49 0%) 3.4105\n",
      "0m 11s (- 299m 20s) (50 0%) 3.8708\n",
      "0m 12s (- 296m 0s) (51 0%) 5.2122\n",
      "0m 12s (- 291m 50s) (52 0%) 3.9839\n",
      "0m 12s (- 288m 21s) (53 0%) 6.1151\n",
      "0m 12s (- 284m 51s) (54 0%) 3.5174\n",
      "0m 12s (- 281m 44s) (55 0%) 2.7280\n",
      "0m 12s (- 279m 15s) (56 0%) 3.8333\n",
      "0m 12s (- 276m 37s) (57 0%) 4.9671\n",
      "0m 12s (- 274m 7s) (58 0%) 4.6664\n",
      "0m 12s (- 270m 39s) (59 0%) 2.7529\n",
      "0m 12s (- 267m 1s) (60 0%) 2.3709\n",
      "0m 12s (- 264m 13s) (61 0%) 5.6038\n",
      "0m 12s (- 261m 26s) (62 0%) 5.0351\n",
      "0m 13s (- 259m 13s) (63 0%) 4.3428\n",
      "0m 13s (- 256m 15s) (64 0%) 3.2081\n",
      "0m 13s (- 253m 57s) (65 0%) 4.0541\n",
      "0m 13s (- 251m 19s) (66 0%) 3.2111\n",
      "0m 13s (- 248m 50s) (67 0%) 2.7635\n",
      "0m 13s (- 246m 14s) (68 0%) 3.2863\n",
      "0m 13s (- 244m 7s) (69 0%) 6.3219\n",
      "0m 13s (- 242m 41s) (70 0%) 5.1889\n",
      "0m 13s (- 240m 41s) (71 0%) 3.7640\n",
      "0m 13s (- 239m 6s) (72 0%) 4.0469\n",
      "0m 13s (- 237m 59s) (73 0%) 5.6248\n",
      "0m 14s (- 236m 25s) (74 0%) 5.3017\n",
      "0m 14s (- 234m 42s) (75 0%) 3.1211\n",
      "0m 14s (- 233m 41s) (76 0%) 6.0321\n",
      "0m 14s (- 232m 45s) (77 0%) 5.6689\n",
      "0m 14s (- 231m 18s) (78 0%) 4.5293\n",
      "0m 14s (- 229m 27s) (79 0%) 3.0132\n",
      "0m 14s (- 228m 30s) (80 0%) 4.6277\n",
      "0m 14s (- 226m 36s) (81 0%) 3.8265\n",
      "0m 14s (- 224m 38s) (82 0%) 5.5501\n",
      "0m 14s (- 222m 45s) (83 0%) 3.6564\n",
      "0m 14s (- 221m 6s) (84 0%) 3.0130\n",
      "0m 14s (- 219m 32s) (85 0%) 2.8004\n",
      "0m 15s (- 218m 31s) (86 0%) 3.9170\n",
      "0m 15s (- 217m 5s) (87 0%) 3.5453\n",
      "0m 15s (- 215m 24s) (88 0%) 3.0090\n",
      "0m 15s (- 214m 35s) (89 0%) 4.7785\n",
      "0m 15s (- 213m 14s) (90 0%) 2.5867\n",
      "0m 15s (- 212m 34s) (91 0%) 4.1384\n",
      "0m 15s (- 211m 16s) (92 0%) 3.9121\n",
      "0m 15s (- 210m 25s) (93 0%) 3.4446\n",
      "0m 15s (- 209m 53s) (94 0%) 5.7146\n",
      "0m 15s (- 208m 57s) (95 0%) 2.4348\n",
      "0m 15s (- 207m 44s) (96 0%) 3.5481\n",
      "0m 16s (- 206m 25s) (97 0%) 4.0081\n",
      "0m 16s (- 205m 6s) (98 0%) 2.9547\n",
      "0m 16s (- 204m 24s) (99 0%) 4.2870\n",
      "0m 16s (- 203m 26s) (100 0%) 4.1904\n",
      "0m 16s (- 202m 13s) (101 0%) 3.5389\n",
      "0m 16s (- 200m 59s) (102 0%) 4.5338\n",
      "0m 16s (- 199m 50s) (103 0%) 4.3953\n",
      "0m 16s (- 199m 12s) (104 0%) 3.8831\n",
      "0m 16s (- 198m 32s) (105 0%) 5.6925\n",
      "0m 16s (- 197m 42s) (106 0%) 3.8382\n",
      "0m 16s (- 196m 49s) (107 0%) 3.0527\n",
      "0m 16s (- 195m 53s) (108 0%) 2.5500\n",
      "0m 17s (- 194m 55s) (109 0%) 4.3825\n",
      "0m 17s (- 193m 56s) (110 0%) 3.5740\n",
      "0m 17s (- 193m 26s) (111 0%) 4.4564\n",
      "0m 17s (- 192m 24s) (112 0%) 3.7584\n",
      "0m 17s (- 191m 16s) (113 0%) 3.6397\n",
      "0m 17s (- 190m 6s) (114 0%) 2.8943\n",
      "0m 17s (- 189m 48s) (115 0%) 4.1159\n",
      "0m 17s (- 189m 11s) (116 0%) 4.5126\n",
      "0m 17s (- 188m 14s) (117 0%) 2.6724\n",
      "0m 17s (- 187m 17s) (118 0%) 3.9959\n",
      "0m 17s (- 186m 24s) (119 0%) 4.2410\n",
      "0m 17s (- 185m 41s) (120 0%) 3.6832\n",
      "0m 17s (- 184m 59s) (121 0%) 3.1935\n",
      "0m 18s (- 184m 29s) (122 0%) 4.4715\n",
      "0m 18s (- 184m 21s) (123 0%) 4.1816\n",
      "0m 18s (- 183m 38s) (124 0%) 3.9975\n",
      "0m 18s (- 183m 13s) (125 0%) 5.7196\n",
      "0m 18s (- 182m 32s) (126 0%) 1.6428\n",
      "0m 18s (- 182m 10s) (127 0%) 4.0152\n",
      "0m 18s (- 181m 31s) (128 0%) 4.0170\n",
      "0m 18s (- 181m 6s) (129 0%) 2.7276\n",
      "0m 18s (- 180m 26s) (130 0%) 3.9630\n",
      "0m 18s (- 179m 39s) (131 0%) 3.3028\n",
      "0m 18s (- 178m 52s) (132 0%) 3.8811\n",
      "0m 19s (- 178m 15s) (133 0%) 4.5787\n",
      "0m 19s (- 177m 38s) (134 0%) 2.9427\n",
      "0m 19s (- 177m 0s) (135 0%) 3.3023\n",
      "0m 19s (- 176m 16s) (136 0%) 3.7814\n",
      "0m 19s (- 175m 43s) (137 0%) 4.2929\n",
      "0m 19s (- 175m 7s) (138 0%) 2.8035\n",
      "0m 19s (- 174m 35s) (139 0%) 4.3762\n",
      "0m 19s (- 174m 16s) (140 0%) 4.1418\n",
      "0m 19s (- 174m 2s) (141 0%) 5.1011\n",
      "0m 19s (- 173m 52s) (142 0%) 4.1288\n",
      "0m 19s (- 173m 27s) (143 0%) 3.8436\n",
      "0m 19s (- 172m 55s) (144 0%) 3.5607\n",
      "0m 20s (- 172m 16s) (145 0%) 2.8971\n",
      "0m 20s (- 171m 57s) (146 0%) 4.7434\n",
      "0m 20s (- 171m 28s) (147 0%) 4.0326\n",
      "0m 20s (- 171m 23s) (148 0%) 4.1861\n",
      "0m 20s (- 170m 59s) (149 0%) 3.1721\n",
      "0m 20s (- 170m 26s) (150 0%) 2.7604\n",
      "0m 20s (- 169m 57s) (151 0%) 3.4013\n",
      "0m 20s (- 169m 23s) (152 0%) 4.3162\n",
      "0m 20s (- 168m 58s) (153 0%) 4.3025\n",
      "0m 20s (- 168m 24s) (154 0%) 2.6933\n",
      "0m 20s (- 167m 51s) (155 0%) 2.9037\n",
      "0m 20s (- 167m 23s) (156 0%) 2.6944\n",
      "0m 21s (- 167m 7s) (157 0%) 3.9385\n",
      "0m 21s (- 166m 38s) (158 0%) 2.8221\n",
      "0m 21s (- 166m 9s) (159 0%) 2.5941\n",
      "0m 21s (- 165m 56s) (160 0%) 3.1978\n",
      "0m 21s (- 165m 29s) (161 0%) 3.2038\n",
      "0m 21s (- 165m 4s) (162 0%) 2.5229\n",
      "0m 21s (- 164m 53s) (163 0%) 4.2850\n",
      "0m 21s (- 164m 37s) (164 0%) 3.0212\n",
      "0m 21s (- 164m 15s) (165 0%) 4.9928\n",
      "0m 21s (- 164m 0s) (166 0%) 2.5785\n",
      "0m 21s (- 163m 37s) (167 0%) 3.6949\n",
      "0m 21s (- 163m 17s) (168 0%) 2.9190\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-01174e7b98d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mattn_decoder1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_decoder1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtrainIters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_decoder1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m75000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-22-3e986041680e>\u001b[0m in \u001b[0;36mtrainIters\u001b[0;34m(encoder, decoder, n_iters, print_every, plot_every, learning_rate)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         loss = train(input_variable, target_variable, encoder,\n\u001b[0;32m--> 112\u001b[0;31m                      decoder, encoder_optimizer, decoder_optimizer, criterion)\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0mprint_loss_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0mplot_loss_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-3e986041680e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(input_variable, target_variable, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length)\u001b[0m\n\u001b[1;32m     69\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0mencoder_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dc/virtualenvs/torch/lib/python3.5/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \"\"\"\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dc/virtualenvs/torch/lib/python3.5/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 98\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hidden_size = 256\n",
    "encoder1 = EncoderRNN(input_lang.n_words, hidden_size)\n",
    "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words,\n",
    "                               1, dropout_p=0.1)\n",
    "\n",
    "if use_cuda:\n",
    "    encoder1 = encoder1.cuda()\n",
    "    attn_decoder1 = attn_decoder1.cuda()\n",
    "\n",
    "trainIters(encoder1, attn_decoder1, 75000, print_every=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Baseline\n",
      "==============================\n",
      "-=-=-=-=-=-=-=-=-=-=-=-\n",
      "Testing for Mixture of Softmaxes (k=1)\n",
      "Rank of the resulting matrix (size [2048, 1000]): 34\n",
      "-=-=-=-=-=-=-=-=-=-=-=-\n",
      "Testing for Mixture of Softmaxes (k=2)\n",
      "Rank of the resulting matrix (size [2048, 1000]): 34\n",
      "-=-=-=-=-=-=-=-=-=-=-=-\n",
      "Testing for Mixture of Softmaxes (k=3)\n",
      "Rank of the resulting matrix (size [2048, 1000]): 34\n",
      "-=-=-=-=-=-=-=-=-=-=-=-\n",
      "Testing for Mixture of Softmaxes (k=4)\n",
      "Rank of the resulting matrix (size [2048, 1000]): 34\n",
      "-=-=-=-=-=-=-=-=-=-=-=-\n",
      "Testing for Mixture of Softmaxes (k=5)\n",
      "Rank of the resulting matrix (size [2048, 1000]): 34\n",
      "\n",
      "Testing MoC\n",
      "==============================\n",
      "-=-=-=-=-=-=-=-=-=-=-=-\n",
      "Testing for Mixture of Softmaxes (k=1)\n",
      "Rank of the resulting matrix (size [2048, 1000]): 34\n",
      "-=-=-=-=-=-=-=-=-=-=-=-\n",
      "Testing for Mixture of Softmaxes (k=2)\n",
      "Rank of the resulting matrix (size [2048, 1000]): 34\n",
      "-=-=-=-=-=-=-=-=-=-=-=-\n",
      "Testing for Mixture of Softmaxes (k=3)\n",
      "Rank of the resulting matrix (size [2048, 1000]): 34\n",
      "-=-=-=-=-=-=-=-=-=-=-=-\n",
      "Testing for Mixture of Softmaxes (k=4)\n",
      "Rank of the resulting matrix (size [2048, 1000]): 34\n",
      "-=-=-=-=-=-=-=-=-=-=-=-\n",
      "Testing for Mixture of Softmaxes (k=5)\n",
      "Rank of the resulting matrix (size [2048, 1000]): 34\n",
      "\n",
      "Testing MoS\n",
      "==============================\n",
      "-=-=-=-=-=-=-=-=-=-=-=-\n",
      "Testing for Mixture of Softmaxes (k=1)\n",
      "Rank of the resulting matrix (size [2048, 1000]): 34\n",
      "-=-=-=-=-=-=-=-=-=-=-=-\n",
      "Testing for Mixture of Softmaxes (k=2)\n",
      "Rank of the resulting matrix (size [2048, 1000]): 627\n",
      "-=-=-=-=-=-=-=-=-=-=-=-\n",
      "Testing for Mixture of Softmaxes (k=3)\n",
      "Rank of the resulting matrix (size [2048, 1000]): 970\n",
      "-=-=-=-=-=-=-=-=-=-=-=-\n",
      "Testing for Mixture of Softmaxes (k=4)\n",
      "Rank of the resulting matrix (size [2048, 1000]): 999\n",
      "-=-=-=-=-=-=-=-=-=-=-=-\n",
      "Testing for Mixture of Softmaxes (k=5)\n",
      "Rank of the resulting matrix (size [2048, 1000]): 1000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "VOCAB = 1000\n",
    "HIDDEN = 32\n",
    "SAMPLES = 2048\n",
    "\n",
    "def logsumexp(x):\n",
    "    max_x, _ = torch.max(x, dim=1, keepdim=True)\n",
    "    part = torch.log(torch.sum(torch.exp(x - max_x), dim=1, keepdim=True))\n",
    "    return max_x + part\n",
    "\n",
    "\n",
    "l = torch.nn.Linear(HIDDEN, VOCAB)\n",
    "x = torch.autograd.Variable(torch.rand(SAMPLES, HIDDEN), volatile=True)\n",
    "\n",
    "for model_type in ['Baseline', 'MoC', 'MoS']:\n",
    "    print()\n",
    "    print('Testing {}'.format(model_type))\n",
    "    print('==============================')\n",
    "\n",
    "    for MIXTURES in range(1, 6):\n",
    "        print('-=-=-=-=-=-=-=-=-=-=-=-')\n",
    "        print('Testing for Mixture of Softmaxes (k={})'.format(MIXTURES))\n",
    "\n",
    "        # The mixer is used in MoC and MoS\n",
    "        # It takes a context vector and produces multiple of them\n",
    "        mixer = torch.nn.Linear(HIDDEN, MIXTURES * HIDDEN)\n",
    "\n",
    "        if MIXTURES == 1:\n",
    "            mixed = x\n",
    "        else:\n",
    "            if model_type == 'Baseline':\n",
    "                mixed = torch.cat([x] * MIXTURES, dim=1)\n",
    "            else:\n",
    "                mixed = torch.tanh(mixer(x))\n",
    "        parts = torch.chunk(mixed, MIXTURES, dim=1)\n",
    "\n",
    "        # For the different components, we're assuming equal mixing\n",
    "        prior = torch.autograd.Variable(torch.ones(1, MIXTURES, 1) * (1 / MIXTURES))\n",
    "\n",
    "        if model_type in ['Baseline', 'MoS']:\n",
    "            # For each of the components, we get an entirely new full softmax result\n",
    "            results = []\n",
    "            for part in parts:\n",
    "                results.append(torch.nn.functional.log_softmax(l(part)))\n",
    "            result = torch.cat(results, dim=1).view(-1, MIXTURES, VOCAB)\n",
    "            # We're in log probability space from the log_softmax, so adding the logpriors is equivalent to multiplication in prob space\n",
    "            result = torch.log(prior) + result\n",
    "            #print(result[0, :, :]) # Sanity check for baseline - these should be duplicate vectors\n",
    "\n",
    "            # We're unlikely to see a difference but this compares naive logsumexp to a stable one\n",
    "            result = logsumexp(result).view(-1, VOCAB)\n",
    "            #naive_logsumexp_result = torch.log(torch.sum(torch.exp(result), dim=1)).view(-1, VOCAB)\n",
    "            #print('Difference between naive and stable logsumexp: {}'.format((result - naive_logsumexp_result).sum().data[0]))\n",
    "\n",
    "        elif model_type == 'MoC':\n",
    "            # Mixture of Contexts is easy - just take the average of the different produced contexts\n",
    "            # This makes it clearly obvious why it doesn't help with rank\n",
    "            contexts = torch.cat(parts, dim=1).view(-1, MIXTURES, HIDDEN)\n",
    "            moc = torch.sum(prior * contexts, dim=1)\n",
    "            result = torch.nn.functional.log_softmax(l(moc))\n",
    "\n",
    "        # Calculate the rank of the resulting matrix\n",
    "        # Note: numerical precision / tolerance is important and I've not put an academic level of thought into these selections\n",
    "        # Refer to proper material if you're using this within a paper\n",
    "        # TODO: This is slowwwwwwwwwwwwwwwww for larger contexts ...\n",
    "        rank = np.linalg.matrix_rank(result.data.numpy(), tol=1e-3)\n",
    "        print('Rank of the resulting matrix (size {}): {}'.format(list(result.size()), rank))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 30])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "m = torch.nn.Linear(20,30,bias=False)\n",
    "input = torch.autograd.Variable(torch.ones(128,20))\n",
    "output = m(input)\n",
    "print(output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
