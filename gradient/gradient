class gradient(object)
    def __init__(self):
        pass
    def a3(self, wd_coefficient, n_hid, n_iters, learning_rate,
                    momentum_multiplier, do_early_stopping, mini_batch_size):
        """

        """
        model = initial_model(self, n_hid)
        from_data_file = load(data.mat)
        datas = fom_data_file.data
        n_training_cases = size(datas.training.inputs,2)
        if (n_iters!=0):
            test_gradient(self,model,datas.training,wd_coefficient)
        theta = model_to_theta(model)
        momentum_speed = theta * 0
        training_data_losses = []
        validation_data_losses = []
        if do_early_stopping:
            best_so_far_theta = -1
            best_so_far_validation = inf #find replacement for later
            best_so_far.after_n_iters = -1
        for optimization_iteration_i in range(1, n_iters):
            model = theta_to_model(theta)
            training_batch_start = (optimization_iteration_i-1) * mini_batch_size % n_training_cases +1
            training_batch.inputs = dates.training.inputs(:,training_batch_start:training_batch_start + mini_batch_size-1)
            training_batch.targets = datas.training.inputs(:, training_batch_start:training_batch_start + mini_batch_size -1)
            gradient = model_to_theta(d_loss_by_d_model(model, training_batch, wd_coefficient))
            momentum_speed = momentum_speed * momentum_multiplier - gradient
            theta = theta + momentum_speed * learning_rate

            model=theta_to_model(theta)
            training_data_losses = [training_data_losses, loss(model,datas.training,wd_coefficient)]
            validation_data_losses = [validation_data_losses, loss(model,datas.validation,wd_coefficient)]
            if (do_early_stopping and validation_data_losses(end) < best_so_far_validation_loss):
                best_so_far_theta = theta
                best_so_far_validation_loss = validation_data_losses(end)
                best_so_far.after_n_iters = optimization_iteration_i
            if ( (optimization_iteration_i % round(n_iters/10)) == 0):
                fprintf('after %d optimization iterations, training data loss is %f and validation loss is:%f', optimization_iteration_i, training_data_losses(end), validation_data_losses(end) )

        if (n_iters !=0):
            test_gradient(model, datas.training, wd_coefficient )
        if(do_early_stopping):
            fprintf("early stopping: validation loss was lowest after %d iteration", best_so_far.after_n_iters)
            theta = best_so_far.theta
        #optimization finished do some reporting
        model = theta_to_model(theta)
        if(n_iters!=0):
            plot('training_data_losses','b')
            plot('validation_data)losses', 'r')
            legend('training', 'validation')
            ylabel('loss')
            xlabel('iteration number')

        datas2 = {datas.training, datas.validation. datas.test}
        data_names = {'training','validation', 'test'}
        for data_i in range(1,3):
            data = datas2(data_i)
            data_name = data_names(data_i)
            fprintf('the loss on teh %s data is %f', data_name, loss(model, data, wd_coefficient))
        fprintf('the classification error rate on the %s data is %f', data_name, classification_performance(model, data))
        pass
    def test_gradient(self, model, train, wd_coefficient):
        base_theta = model_to_theta(model)
        h = 1e-2
        correctness_threshold = 1e-5
        analytic_gradient = model_to_theta(d_loss_by_d_model(model, data, wd_coefficient))
        #test the gradient for only a  few elements
        for i in range(1 to 100):
            test_index = i * 1299721 % size(base_theta,1) + 1
            analytic_here = analytic_gradient(test_index)
            theta_step = base_theta * 0
            theta_step(test_index) = h
            contribution_distances = [-4:1, 1:4]
            contribution_weights = [1/280, -4/105, 1/5, -4/5, 4/5, -1/5, 4/105, -1/280]
            temp = 0
            for contribution_index in range(1,8):
                temp = temp + loss(theta_to_model(base_theta + theta_step*contribution_distances(contribution_index)),
                                   data, wd_coefficient)* contribution_weights(contribution_index)
            fd_here = temp/h
            diff = abs(analytic_here - fd_here)
            if (diff < correctness_threshold):
                continue
            if (diff/(abs(analytic_here)+abs(fd_here)) < correctness_threshold):
                continue
            error(sprintf('error'))
        fprintf('gradient test passed')
        pass
    def logistic(self,input):
        return 1./(1+np.exp(-input))
    def log_sum_exp_over_rows(self,a):
        #this computes log(sum(exp(a),1) in a numerically stable way
        maxs_small = max(a,[],1)
        maxs_big = repmat(maxs_small, [size(a,1), 1])
        return np.log(np.sum(np.exp(a-maxs_big),1))+ maxs_small
    def loss(self,model, data, wd_coefficient):
        hid_input = modelinput_to_hid  * data.inputs
        hid_output = logistic(hid_input)
        class_normalizer = log_sum_exp_over_rows(class_input)
        log_class_prob = class_input - repmat(class_normalizer, [size(class_input,1),1])
        class_prob = np.exp(log_class_prob)
        classification_loss = -np.mean(np.sum(log_class_prob. * data.targets,1))
        wd_loss = np.sum(model_to_theta(model).^2/2*wd_coefficient)
        E = 1/2*wd_coefficient*theta^2
        return classification_loss + wd_loss
    def d_loss_by_d_model(self,model, data, wd_coefficient):
        
    def load(self,file):
        pass
    def initial_model(self, n_hid):
        n_params = (256 + 10)* n_hid
        as_row_vector = cos(0:(n_params-1))
        ret = theta_to_model(self,as_row_vector(:)*0.1)
        return ret

    def model_to_theta(self,model):

        pass
    def theta_to_model(self,theta):
