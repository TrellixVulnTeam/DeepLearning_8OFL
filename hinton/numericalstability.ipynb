{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Numerical Stability</h4>\n",
    "<p></p>\n",
    "Example taken from Vincent Vander...\n",
    "<p></p>\n",
    "Adding small numbers to very large numbers does not scale well. \n",
    "For example adding a small number to a billion then subtracting a billion does not return the same small number. \n",
    "1000000000 + .000001 -1000000000 = .953\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Validation set size</h4>\n",
    "If you use the 30 samples necessary for statistical significance then you need 30k examples in the validation set\n",
    "to make a .15 accuracy meaningful. .1*x/100=30; x=30000. If the validation set is 10% of the sample set size you need \n",
    "300k examples!!! If the classes are not well balanced then this heuristic is not useful!!!! Will have to do cross validation\n",
    "as a solution if not well balanced... \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "<h4>Gradient </h4>\n",
    "If computing loss takes N operations then computing gradient takes 3x as many computations. We we are unable to \n",
    "compute the loss over the entire training set and the 3x computationally intensive gradient for each step then \n",
    "we randomly sample the training set and calculate the gradient for the sample. The tradeoff is sometimes we \n",
    "may go in the wrong direction so we multiply by a small number, the learning rate to make these changes small. Eventually\n",
    "if we converge we win. If we do not pick the samples randomly enough this method does not work. (add examples). The only\n",
    "optimizer which scales with data and model size but comes with a lot of issues(not covered). \n",
    "<p></p>\n",
    "Momentum: Use the running average of the gradient. Not the actual direction. Beneficial to make learning rate smaller \n",
    "    as you train. Lowering over time is key. \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
