{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> outline<h2>\n",
    "<h4> 3 independent concepts</h4> \n",
    "<li>multiclass classifiers softmax regression</li> \n",
    "<li>backprop/vanishing gradients</li>\n",
    "<li>how to set weights/gradient descent(full training set)/SGD(online)</li>\n",
    "<h4> linear/logistic regression</h4>\n",
    "<h4> softmax regression</h4>\n",
    "<h4> cross entropy and softmax</h4>\n",
    "<h4> backpropagation in NN</h4>\n",
    "<h4> vanishing gradient demo</h4>\n",
    "<h4> relu</h4>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](nn.png \"NN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<p>There are K inputs, N hidden neurons and M outputs. Assume K=N=M. The hidden layer is a function of the output y. Let's add a change of variable to u\n",
    "for the weights between the hidden layer and the output</p>\n",
    "\n",
    "${h}_i=f({u}_i)=f(\\sum_{k=1}^{K}w_{ki}x_{k})$\n",
    "\n",
    "<p>where we defined:</p>\n",
    "    \n",
    "${u}_i = \\sum_{k=1}^{K}w_{ki}x_{k}$\n",
    "\n",
    "<p>The output layer:</p>\n",
    "\n",
    "${y}_i=f({v}_i) = f(\\sum_{k=1}^{K}w^{\\prime}_{ij}h_{k})$\n",
    "\n",
    "<p>where h is the output of the logistic unit. If there is additipnal processing such as whitening; this is preformed \n",
    "here and output as hk. We define v as:</p>\n",
    "\n",
    "    \n",
    "${v}_i = \\sum_{k=1}^{K}w^{\\prime}_{ij}x_{k}$    \n",
    "    \n",
    "    \n",
    "<p>Minimize the error between the training set output and actual</p>\n",
    "\n",
    "$E=\\frac{1}{2}\\sum_{n=1}^{K}({t}_n - {y}_n)^2 $\n",
    "\n",
    "$from \\; y=\\frac{1}{e^{-z}+1} \\;and\\; \\frac{dy}{dz}=y(1-y)$\n",
    "\n",
    "<p> we can calculate dy/dv</p>\n",
    "\n",
    "$\\frac{dy}{dv} \\;=\\; y(1-y)$\n",
    "\n",
    "\n",
    "<p>At the ouptut node y changes as a function of the weights and input to the hidden layer.\n",
    "There are 2 layers we have to work through; first from the output of the hidden layer with weights wij prime then \n",
    "to the input of the hidden layer with weights wki. \n",
    "</p>\n",
    "\n",
    "\n",
    "$\\frac{\\partial E}{\\partial y} = \\sum_{n=1}^{M}  -({t}_n - {y}_n) $\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>We want to determine the effect of changing weights on the output. There are 2 sets of weights, the weights after\n",
    "the hidden layer and the set of weights before the hidden layer. Changing the value of theseweights should narrow or widen\n",
    "the difference beween the training sample and output summed over all samples. We use the chain rule in 2 steps to first \n",
    "rewrite the dE/dy term in terms of the w prime weights then using the same logic the w set of weights</p>\n",
    "\n",
    "$\\frac{\\partial{E}}{\\partial {w^{\\prime}_{ij}}}=\\frac{\\partial E}{\\partial y_{j}} \\frac{dy}{dv_{j}} \\frac{\\partial v_j}{\\partial {w^{\\prime}_ij}}$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "<p>substituting from above:</p>\n",
    "    \n",
    "$\\frac {\\partial E}{\\partial w^{\\prime}_{ij}} = -(t_n - y_n) \\cdot y_{j}(1-y_{j}) \\cdot h_{i}$    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>From the equation above we can see given an observed output from the NN we have an expression for the weights which\n",
    "depends on the layer before and the observed output value. This process of converting the output values to get the change\n",
    "in weights based on weight values and input of the previous stage is backpropagation and depends on the chain rule. To update\n",
    "the weights we add an additional term epsilon indicating the new weights are a small change to the desired output</p>\n",
    "\n",
    "\n",
    "$w^{\\prime updated}_{ki} = w^{\\prime old}_{ki}  -\\epsilon \\cdot (t_n - y_n) \\cdot y_{j}(1-y_{j}) \\cdot h_{i}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "<p>the update rules for the next set of weights are backpropagated from this hidden layer stage to the input</p>\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tf35]",
   "language": "python",
   "name": "conda-env-tf35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
