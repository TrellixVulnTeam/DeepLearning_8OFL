{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40, 10) (40,) (40, 10) (40,)\n"
     ]
    }
   ],
   "source": [
    "#http://edwardlib.org/tutorials/probabilistic-pca\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def data(N, w, noise_std=0.1):\n",
    "    \"\"\"\n",
    "    2 dimensional random data\n",
    "    \"\"\"\n",
    "    #normal dist for dim1 \n",
    "    x = np.random.rand(N,len(w))\n",
    "    y = np.dot(x,w) + np.random.normal(0,noise_std,size=N)\n",
    "    return x,y\n",
    "\n",
    "N = 40  # number of data points\n",
    "D = 10  # number of features\n",
    "\n",
    "w_true = np.random.randn(D)\n",
    "X_train, y_train = data(N, w_true)\n",
    "X_test, y_test = data(N, w_true)\n",
    "print(X_train.shape,y_train.shape,X_test.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#https://www.periscope.tv/hugo_larochelle/1ypKdAVmbEpGW\n",
    "#https://docs.google.com/presentation/d/1ACaON1lY9uVPALSmGIvMunAzhWAC_KFIeZVJTaBA4z0/edit#slide=id.g19f631ee32_0_0\n",
    "Slides taken from google docs link above. \n",
    "<h6>Generative Models with RealNVP</h6>\n",
    "\n",
    "<img src=\"laurentdinh1.png\">\n",
    "From data space to latent space Like ICA. From Latent space to input like VAE/GANS. Learn using MLE with \n",
    "constraints on function we are going to define. \n",
    "<img src=\"laurentdinh2.png\">\n",
    "The function g(x) is a NN with some constraings. There are 2 parts. The Pz(z) is a gaussian. The second part is a NN. \n",
    "Computing a Jacobian of a NN can be expensive and computing Det of large matix is also hard. Impose constraint on G\n",
    "to make learning tractable. A simple case of tractable g is a linear model where the matrix diagonals. The determinant\n",
    "is the product of the diagonals. Define a coupling layer. \n",
    "<img src=\"laurentdinh3.png\">\n",
    "The coupling layer produces an intermediate transformation from the input X to Y. Y is not the latent space. \n",
    "s and t are both differentiable and are represented by NNs. The coupling equations can also be defined by \n",
    "a binary mask. \n",
    "<img src=\"laurentdinh4a.png\">\n",
    "binary mask.  Not in later slides. Nobody got ti. \n",
    "<img src=\"laurentdinh4.png\">\n",
    "The 2 parts are 1) the tractable jacobian has the form, identity matrixx1=y1. \n",
    "2) the dY2/dX1 is triangular. The second part exp(s(1))is a diagonal matrix, The cooefficent of diagoanl matraix is exp(s(x1)) \n",
    "<img src=\"laurentdinh5.png\">\n",
    "The problem with the coupling layer is it leaves part of the input unchanged. So add another coupling layer. \n",
    "<img src=\"laurentdinh6.png\">\n",
    "<img src=\"laurentdinh7.png\">\n",
    "<img src=\"laurentdinh8.png\">\n",
    "<img src=\"laurentdinh9.png\">\n",
    "<img src=\"laurentdinh10.png\">\n",
    "\n",
    "Transform using a change of variable to equation 3.47(Goodfellow book). This tranformation function is predefined\n",
    "to be invertible and is a neural network. The Jacobian of an arbitrary NN is hard. So simplify the NN to Ax+b. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://www.youtube.com/watch?v=KgZy9mN2SeA"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
