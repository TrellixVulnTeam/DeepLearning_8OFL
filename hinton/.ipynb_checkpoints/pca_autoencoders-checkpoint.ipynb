{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>PCA and Autoencoders</h6>\n",
    "\n",
    "<p>When high dimensional data is near a linear manifold; if we can find this linear\n",
    "manifold then we can project data with N dims onto the manifold reducing the data to M dims</p>\n",
    "<p>In directions orthogonal to the data there is not much variation in the data</p>\n",
    "<p>Try data reduction on rossman starting w/variable importance dims</p>\n",
    "<p>Can do this efficiently w/PCA algorithm, inefficiently with 1 hidden layer NN</p>\n",
    "<p>Advantage w/NN is we can generalize to DNN to deal with curved manifolds</p>\n",
    "\n",
    "<p>PCA -- start w/N dim data</p>\n",
    "<p>Reconstrct using mean vlue oer all the data on the M_N directions not represented</p>\n",
    "https://stats.stackexchange.com/questions/229092/how-to-reverse-pca-and-reconstruct-original-variables-from-several-principal-com\n",
    "\n",
    "<img src=\"hinton15_pca1.png\">\n",
    "<p>If we view the projecion of N onto M as a MSE distance we can use a MSE loss function\n",
    "in an sutoencoder to mimic the PCA decomposition. This won't be exactly the same but the\n",
    "weights will give us a reduced dimensionlaity</p>\n",
    "<img src=\"hinton15_pca2.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Adding an encoder and decoder to reproduce the input as closely as possible with M hidden nodes in 1 hidden layer. \n",
    "\n",
    "Example of linear AE and PCA.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "<h6>PCA w/Deep AE</h6>\n",
    "Deep Autoencoders HInton: after pretraining layers of autoencoders he was able to demonstrate better percormance \n",
    "from Deep AE than PCA. The learning time is linear or better than the number of training cases. The final model\n",
    "is compact and fast. Very difficult to optimize AE using BP alone. If you use small initial weights, the BP gradient\n",
    "vanished or died. You can use unsupervised layer by layer pretraining or initilaiE weights carefully \n",
    "as in echo state networks\n",
    "MNIST 2006 Salakhutdinov was first successful deep autoencoder. \n",
    "<h6>Deep AE demo with MNIST</h6>\n",
    "Start with MNIST in 784 -> 1000->500->250 in 3 layers in 30 linear units. Then decoded back to 784. Used stack of\n",
    "RBMS to initialize the weights for encoding. Then took the transposes of these weights to init the decoder for them. \n",
    "After RBMs are trained; then apply backprop to minimize reconstruction error. Use cross entropy error. Once started bp, \n",
    "the weights for encoding and decoding diverged. \n",
    "<img src=\"hintonpca2.png\">\n",
    "Can see from the image above the 30-D DNN AE is better than the original bc the 8 does not have the missing edge\n",
    "artifact. It was able to reconstruct the edge. The 30-D Deep AE is visibly better than the 30-D PCA. \n",
    "\n",
    "<h6>LSA - Latent Semantic Analysis</h6>\n",
    "Using Deep AE for document retrieval. Deep AE should work better than LSI. Using database of documents \n",
    "showed 10 components extracted from Deep AE was better\n",
    "than 50 components extracted with a linear method like Latent Semantic Analysis. \n",
    "<p>Convert each document to Bag of words remove stop words and count occurrence of words</p>\n",
    "<p></p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
