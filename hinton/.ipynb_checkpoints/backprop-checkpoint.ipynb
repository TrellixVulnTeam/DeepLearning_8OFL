{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Backpropagation in supervised neural nets is used to set the weights given a set of training data. The most common use\n",
    "case for NN are in using feedforward networks with hidden layers to model multiple functions. Hidden layers can have a \n",
    "width or number of neurons and depth. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"bp0.png\">\n",
    "\n",
    "\n",
    "Backpropagation is the chain rule applied to the above graph. \n",
    "$y=g(f(x))$\n",
    "\n",
    "$\\frac{dy}{dx} = \\frac{dg}{dx} = \\frac{dg}{df} \\; \\frac{df}{dx}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Linear Neuron \n",
    "\n",
    "\n",
    "\n",
    "<img src=\"bplin.png\">\n",
    "\n",
    "The linear neuron converts the b in $z=\\sum\\limits_{i=n}^Nw_ix_i+b$ into a weight term by adding a 1 into the \n",
    "input of the neuron. This converts b into $w_0$\n",
    "\n",
    "The notation from Hinton is for a linear neuron: $z=\\sum\\limits_{i=1}^N w_ix_i + b = \\sum\\limits_{i=0}^N w_ix_i$\n",
    "\n",
    "To find the weights of the linear neuron given a set of training/test data we apply the delta rule(Hinton Coursera)\n",
    "which is a 2 step process of 1) defining a square error loss and 2) taking a derivative and using the delta rule to \n",
    "determine an iterative procedure as a substitute for an exact analytical solution. The delta rule shows how much\n",
    "to change the weights as we apply each training sample. \n",
    "\n",
    "\n",
    "<li>define the square error loss</li>\n",
    "$E = \\frac{1}{2}\\sum\\limits_{n in training}(t_n-y_n)^2$\n",
    "<p>verbatim Hinton Courser lecture3.pdf</p>\n",
    "or the total error over the entire training set of size N is:\n",
    "$E_{total} = \\frac{1}{2}\\sum\\limits_{i=0}^N (t_i - w_ix_i)^2$\n",
    "<p></p>\n",
    "We can also define the error for a single training sample:\n",
    "$E_i = \\frac{1}{2} (t_i-y_i)^2$\n",
    "<p></p>\n",
    "<li>Take the derivative of the total sum error loss over the entire training set</li>\n",
    "We can take the derivative of either the total loss or each step. It does not matter as long\n",
    "as we are consistent with the notation and clear which option we take. \n",
    "\n",
    "\n",
    "<li>chain rule (copied verbatim from Hinton Coursera lecture3.pdf)</li>\n",
    "<p><div> </div></p>\n",
    "$\\frac{\\partial E}{\\partial w_i}=\\sum\\limits_{n} \\frac{\\partial E}{\\partial y^n} \\cdot \\frac{\\partial y^n}{\\partial w_i}$\n",
    "<p><div> </div></p>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Here is an example of the delta rule in action on a XOR gate:\n",
    "<li>add link</li>\n",
    "    \n",
    "    \n",
    "Instead of summing over all the training samples we could have just used one. Here is an example of the XOR\n",
    "case. \n",
    "<li>add link</il>\n",
    "\n",
    "\n",
    "This process of using the chain rule to calculate the $\\delta w_i$ generalizes to more layers. \n",
    "There are 2 steps used to calculate the weights by observing onlyt the output of a NN. \n",
    "1) Use the chain rule/backpropagation for each layer.  \n",
    "2) Iterating to a approximate solution using some form of SGD. \n",
    "\n",
    "Examples of linear combinations of linear neurons:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Linear Neuron with 1 hidden layer\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Logistic neuron\n",
    "<img src=\"bplog.png\">\n",
    "\n",
    "\n",
    "\n",
    "Logistic Neuron \n",
    "\n",
    "From the Hinton Notation: $z = \\frac{1}{1+exp(-z)}$\n",
    "From the Ng Notation: $y = \\frac{1}{1+exp(-\\theta_i \\cdot \\x_i)}$\n",
    "\n",
    "The derivative of the logistic WRT z. \n",
    "$\\frac{dy}{dz} = y(1-y)$\n",
    "\n",
    "$z = b + \\sum\\limits_{i=1}^N x_iw_i = \\sum\\limits_{i=0}^N x_iw_i$\n",
    "\n",
    "Most use the notation start the index from 0 and do not bother to write out the b term. \n",
    "\n",
    "\n",
    "$\\frac{\\partialz}{\\partial w_i}= x_i$\n",
    "$\\frac{\\partialz}{\\partial x_i} = w_i$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "![alt text](nn.png \"NN\" height=\"500px\" width=\"500px\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This is a 2 layer network with a logistic in the output of the hidden layer and a LOGISTIC at the outpue node. Usually there is a linear FC layer here. There are K inputs, N hidden neurons and M outputs.  The hidden layer is a function of the output $y_i$. Let's add a change of variable to $u_i$ for the hidden logistic node and a change of variable to v for the output logistic unit $y_i$. \n",
    "\n",
    "${h}_i=f({u}_i)=f(\\sum_{k=1}^{K}w_{ki}x_{k}) = \\frac{1}{1+e^{-u_i}}$\n",
    "\n",
    "where we defined:\n",
    "    \n",
    "${u}_i = \\sum_{k=1}^{K}w_{ki}x_{k}$\n",
    "\n",
    "The output layer:\n",
    "\n",
    "${y}_i=f({v}_i) = f(\\sum_{k=1}^{K}w^{\\prime}_{ij}h_{k})$\n",
    "\n",
    "where $h_i$ is the output of the logistic unit. If there is additipnal processing such as whitening; this is preformed \n",
    "here and output as hk. We define v as:\n",
    "\n",
    "    \n",
    "${v}_i = \\sum_{k=1}^{K}w^{\\prime}_{ij}x_{k}= \\frac{1}{1+e^{-v_i}}$    \n",
    "    \n",
    "    \n",
    "Minimize the squared error between the training set output and actual:\n",
    "\n",
    "$E=\\frac{1}{2}\\sum_{n=1}^{K}({t}_n - {y}_n)^2 $\n",
    "\n",
    "$from \\; y=\\frac{1}{e^{-z}+1} \\;and\\; \\frac{dy}{dz}=y(1-y)$\n",
    "\n",
    "we can calculate $\\frac{dy_i}{dv_i}$\n",
    "\n",
    "$\\frac{dy}{dv} \\;=\\; y(1-y)$\n",
    "\n",
    "\n",
    "At the ouptut node y changes as a function of the weights and input to the hidden layer.\n",
    "There are 2 layers we have to work through; first from the output of the hidden layer with weights wij prime then \n",
    "to the input of the hidden layer with weights wki. \n",
    "\n",
    "\n",
    "$\\frac{\\partial E}{\\partial y} = \\sum_{n=1}^{M}  -({t}_n - {y}_n) $\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>We want to determine the effect of changing weights on the output. There are 2 sets of weights, the weights after\n",
    "the hidden layer and the set of weights before the hidden layer. Changing the value of theseweights should narrow or widen\n",
    "the difference beween the training sample and output summed over all samples. We use the chain rule in 2 steps to first \n",
    "rewrite the dE/dy term in terms of the w prime weights then using the same logic the w set of weights</p>\n",
    "\n",
    "$\\frac{\\partial{E}}{\\partial {w^{\\prime}_{ij}}}=\\frac{\\partial E}{\\partial y_{j}} \\frac{dy}{dv_{j}} \\frac{\\partial v_j}{\\partial {w^{\\prime}_ij}}$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>substituting from above:</p>\n",
    "    \n",
    "$\\frac {\\partial E}{\\partial w^{\\prime}_{ij}} = -(t_n - y_n) \\cdot y_{j}(1-y_{j}) \\cdot h_{i}$    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>From the equation above we can see given an observed output from the NN we have an expression for the weights which\n",
    "depends on the layer before and the observed output value. This process of converting the output values to get the change\n",
    "in weights based on weight values and input of the previous stage is backpropagation and depends on the chain rule. To update\n",
    "the weights we add an additional term epsilon indicating the new weights are a small change to the desired output</p>\n",
    "\n",
    "\n",
    "$w^{\\prime updated}_{ki} = w^{\\prime old}_{ki}  -\\epsilon \\cdot (t_n - y_n) \\cdot y_{j}(1-y_{j}) \\cdot h_{i}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>the update rules for the next set of weights are backpropagated from this hidden layer stage to the input</p>\n",
    "\n",
    "$\\frac{\\partial E}{\\partial w_{ki}} =\\sum_{i=1}^{M}(\\frac{\\partial E}{\\partial y_i} \\cdot \\frac{\\partial y_i}{\\partial v_i} \\cdot \\frac{\\partial v_i}{\\partial h_i} )\\cdot \\frac{\\partial h_i}{\\partial u_i}\\frac{\\partial u_i}{\\partial w_{ki}}  $\n",
    "\n",
    "\n",
    "<p>The first part in parenthesis is what we just computed. The next 2 derivatives:</p>\n",
    "\n",
    "$\\frac{\\partial h_i}{\\partial u_i} = h_i \\cdot (1-h_i)$  where $h_i$ is the logistic\n",
    "\n",
    "$\\frac{\\partial u_i}{\\partial w_{ki} }  = x_i$\n",
    "\n",
    "$\\frac{\\partial E}{\\partial w_{ki}} = \\sum_{i=1}^{M}(t_i - y_i) \\cdot y_{i}(1-y_i) \\cdot\n",
    "    \n",
    "<p>The weight update becomes:</p> \n",
    "\n",
    "$w^{updated}_ki = w^{old}_ki - -\\epsilon \\cdot (t_n - y_n) \\cdot y_{j}(1-y_{j}) \\cdot h_{i} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h4>BackPropagation Examples with weight sharing</h4>\n",
    "<img src=\"bp1.png\">\n",
    "<img src=\"bp1a.png\">\n",
    "<img src=\"bp2.png\">\n",
    "<img src=\"bp3.png\">\n",
    "<img src=\"bp4.png\">\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Vanishing gradient\n",
    "https://github.com/dmlc/mxnet-notebooks/blob/master/python/tutorials/understanding_vanish_gradient.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
