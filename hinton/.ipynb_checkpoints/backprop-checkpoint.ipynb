{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Run in tf35. If you run in python 27 you will see artifacts like vertical lines at the end of Latek equations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Backpropagation in supervised neural nets is used to set the weights given a set of training data. The most common use\n",
    "case for NN are in using feedforward networks with hidden layers to model multiple functions. Hidden layers can have a \n",
    "width or number of neurons and depth. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"bp0.png\">\n",
    "\n",
    "\n",
    "Backpropagation is the chain rule applied to the above graph. \n",
    "$y=g(f(x))$\n",
    "\n",
    "$\\frac{dy}{dx} = \\frac{dg}{dx} = \\frac{dg}{df} \\; \\frac{df}{dx}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "<h4>Linear Weight Backpropagation </h4>\n",
    "<p>Note: the definition of a neuron requires a set of weights and an activation function. To keep things\n",
    "    as simple as possible there is no activation\n",
    "    function in this example. This is backpropagation applied to a linear multiplicatoin </p>\n",
    "\n",
    "\n",
    "<img src=\"bplin.png\">\n",
    "\n",
    "The linear operation $w_ix_i+b$ converts the b in $z=\\sum\\limits_{i=n}^Nw_ix_i+b$ into a weight term by adding a 1 into the \n",
    "input of the neuron. This converts b into $w_0$\n",
    "\n",
    "The notation for a linear neuron can have\n",
    "subscripts starting from 1 or 0 and may/maynot explicitly\n",
    "mention the existence of a b term: $z=\\sum\\limits_{i=1}^N w_ix_i + b = \\sum\\limits_{i=0}^N w_ix_i$\n",
    "\n",
    "To find the weights of the linear neuron given a set of training/test data we apply the delta rule(Hinton Coursera)\n",
    "which is a 2 step process of 1) defining a square error loss and 2) taking a derivative and using the delta rule to \n",
    "determine an iterative procedure as a substitute for an exact analytical solution. The delta rule shows how much\n",
    "to change the weights as we apply each training sample. We simplify this and call the operation without an \n",
    "activation function a linear operation. \n",
    "\n",
    "\n",
    "<li>define the square error loss</li>\n",
    "$E = \\frac{1}{2}\\sum\\limits_{n in training}(t_n-y_n)^2$\n",
    "<p>verbatim Hinton Courser lecture3.pdf</p>\n",
    "or the total error over the entire training set of size N is:\n",
    "$E_{total} = \\frac{1}{2}\\sum\\limits_{i=0}^N (t_i -y_i)^2$\n",
    "<p></p>\n",
    "We can also define the error for a single training sample:\n",
    "$E_i = \\frac{1}{2} (t_i-y_i)^2$\n",
    "<p></p>\n",
    "<li>Take the derivative of the total sum error loss over the entire training set</li>\n",
    "We can take the derivative of either the total loss or each step. It does not matter as long\n",
    "as we are consistent with the notation and clear which option we take. \n",
    "\n",
    "\n",
    "<li>Chain rule (copied verbatim from Hinton Coursera lecture3.pdf)</li>\n",
    "<p><div> </div></p>\n",
    "$\\frac{\\partial E}{\\partial w_i}=\\sum\\limits_{n} \\frac{\\partial E}{\\partial y^n} \\cdot \\frac{\\partial y^n}{\\partial w_i}$\n",
    "<p><div> </div></p>\n",
    "\n",
    "<p></p>\n",
    "We can calculate the 2 above parts separately \n",
    "<p></p>\n",
    "\n",
    "$\\frac{\\partial E}{\\partial y_n}=\\frac{1}{2}(2)(t_n-y_n)(-1) = -(t_n-y_n)$\n",
    "<p></p>\n",
    "<p></p>\n",
    "The above is Hinton notation. Many use the opposite with the training samples $t_n$ reversed from the\n",
    "training set labels $y_n$. \n",
    "Will not matter can adjust when defining delta rule\n",
    "<p></p>\n",
    "\n",
    "$\\frac{\\partial E}{\\partial y_n} = -(t_n-y_n)$\n",
    "<p></p>\n",
    "$\\frac{\\partial y}{\\partial w_i} = x_i$\n",
    "<p></p>\n",
    "\n",
    "$\\frac{\\partial E}{\\partial w_i}= -x_i(t_n-y_n)$\n",
    "<p></p>\n",
    "\n",
    "The delta rule takes the above and multiplies a learning rate, epsilon to the expression:\n",
    "    \n",
    "$\\delta w_i = w_i + \\epsilon x_i(t_n-y_n)$    \n",
    "\n",
    "<p></p>   \n",
    "    \n",
    "    \n",
    "The above example was shown without an activation function. To be a neuron we need\n",
    "a binary threshold activation function or something which mimics a binary threshold function like a \n",
    "logistic. The Udactiy hw assignment adds a linear activation function $y=f(x)$ to do regression. Technically\n",
    "this is not a neuron because a neuron is modeled after the brain neuron which \n",
    "requires a binary threshold operator. A linear function $y=f(x)$ is not a threshold operator. \n",
    "Adding a linear activation function for a single weight and input look like this: \n",
    " \n",
    "<img src=\"bplin1a.png\" >    \n",
    "\n",
    "<p>This is not the Udacity hw1 neuron. This is a simplified example with multiple inputs and input weights, \n",
    "1 hidden node, oue hidden node output and one output node y. This reduces the summations to \n",
    "only the ones at the input node to keep things simple. </p>\n",
    "\n",
    "This adds an additional set of weights $w_h$\n",
    "$E = \\frac{1}{2}{t_n-y_n}^2$\n",
    "$y=\\sum\\limits_{ho=0}^N w_ho h_ho$\n",
    "$\\frac{\\partial E}{\\partial w_ho} = \\frac{\\partial E}{\\partial y} frac{\\partial y}{\\partial w_ho}$\n",
    "$y=\\sum\\limits_{hi=0}^N w_hi x_i$\n",
    "$\\frac{\\partial E}{\\partial w_i} = \\frac{}{} frac{}{} frac{}{}$\n",
    "\n",
    "$$\n",
    "$$\n",
    "\n",
    "Adding multiple neurons together adds a summation operation in the path between the hidden layers to \n",
    "the activation function. This expands on the above example by adding a new set of summation operators. \n",
    "\n",
    "\n",
    "\n",
    "<img src=\"bplin1b.png\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Here is an example of the delta rule in action on a XOR gate:\n",
    "<li>add link</li>\n",
    "    \n",
    "    \n",
    "Instead of summing over all the training samples we could have just used one. Here is an example of the XOR\n",
    "case. \n",
    "<li>add link</il>\n",
    "\n",
    "\n",
    "This process of using the chain rule to calculate the $\\delta w_i$ generalizes to more layers. \n",
    "There are 2 steps used to calculate the weights by observing onlyt the output of a NN. \n",
    "1) Use the chain rule/backpropagation for each layer.  \n",
    "2) Iterating to a approximate solution using some form of SGD. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Logistic neuron</h4>\n",
    "<p>A logistic neuron requires a set of weights and an Activation function at the output of y. Without\n",
    "an activation function this is not a neuron. The AF is not in the figure</p>\n",
    "<img src=\"bplog.png\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "From the Hinton Notation the output of the logistic is y: $y = \\frac{1}{1+exp(-z)}$ and\n",
    "the input to the logistic is z. The input to the logistic is a summation of 2 terms, the bias b and\n",
    "the product of the input with the weight. \n",
    "$z = \\sum\\limits_{i=1}^{N}x_iw_i+b_i$ \n",
    "<p></p>\n",
    "From the Ng Notation the output of the logistic neuron is y and the bias terms are \n",
    "combined with the weight terms in $\\theta$ : $y = \\frac{1}{1+exp(-\\theta_i \\cdot x_i)}$\n",
    "<p></p>\n",
    "We will use the Hinton notation and start the index term from 0 to avoid writing a separate bias term. \n",
    "    \n",
    "For the a single neuron with 1 weight and 1 logistic:\n",
    "We want the derivative of the output y as a function of the weights $w_i$\n",
    "\n",
    "    \n",
    "$\\frac{\\partial y}{\\partial w}=\\frac{\\partial y}{\\partial z} \\cdot \\frac{\\partial z}{\\partial w}$    \n",
    "\n",
    "    \n",
    "The derivative of the logistic WRT z. \n",
    "$\\frac{dy}{dz} = y(1-y)$\n",
    "\n",
    "$\\frac{dz}{dw}=x$\n",
    "<p></p>\n",
    "$\\frac{\\partial y}{\\partial w}=x_i \\cdot y(1-y)$\n",
    "\n",
    "<p></p>\n",
    "To derive the delta rule, add a multiply by the learning rate and add it to the existing weights\n",
    "\n",
    "$\\delta w=w_i + \\epsilon \\cdot x_i \\cdot y(1-y)$\n",
    "\n",
    "<p></p>\n",
    "\n",
    "\n",
    "<p></p>\n",
    "Now to add the subscripts for multiple neurons/nodes in a 1 hidden layer feed foward network: \n",
    "    \n",
    "$z = b + \\sum\\limits_{i=1}^N x_iw_i = \\sum\\limits_{i=0}^N x_iw_i$\n",
    "\n",
    "Most use the notation start the index from 0 and do not bother to write out the b term. This is consistent with \n",
    "tensor notation used in statistical physics. \n",
    "\n",
    "$\\frac{\\partial y_i}{\\partial w_i}$ \n",
    "\n",
    "Using the chain rule, we expand the above to: \n",
    "    \n",
    "$\\frac{\\partial y_i}{\\partial w_i}=\\frac{\\partial y_i}{\\partial z_i} \\frac{\\partial z_i}{\\partial w_i}$    \n",
    "\n",
    "\n",
    "$\\frac{\\partial z}{\\partial w_i}= x_i$\n",
    "<p></p>\n",
    "$\\frac{\\partial y}{\\partial z_i} = y(1-y)$ for a logistic neuron\n",
    "<p></p>\n",
    "\n",
    "\n",
    "$\\frac{\\partial y_i}{\\partial w_i}=x_i \\cdot y_i(1-y_i) $\n",
    "\n",
    "To compute a set of weights we need a set of inputs and expected values the inputs should generate. Define\n",
    "an error function $E = \\frac{1}{2}(tn-yn)^2$\n",
    "\n",
    "Take the derivative of this error function wrt w using the chain rule:\n",
    "    \n",
    "$\\frac{\\partial E}{\\partial w} = \\frac{\\partial E}{\\partial y}  \\frac{\\partial y}{\\partial w}$ where we have already \n",
    "calculated $\\frac{\\partial y}{\\partial w}$ from above. \n",
    "\n",
    "To calcluate $\\frac{\\partial E}{\\partial y} = -(t_n-y_n)$\n",
    "\n",
    "The derivative is used to solve an optimization problem using this value as the amount to change the weights when multiplied\n",
    "by a step size, $\\epsilon$\n",
    "\n",
    "Hinton calls this the iterative process of calculating the weight update\n",
    "by computing the difference between the expected output and multiplying with the input\n",
    "as the delta rule: \n",
    "    \n",
    "$weight update=\\epsilon \\cdot -(t_n-y_n)\\cdot x_i \\cdot y_i(1-y_i) $\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h4>Feedforward network</h4>\n",
    "A feedforward network is a logistic neuron with an additional hidden layer. This neural network  \n",
    "has 2 sets of weights and a hidden layer denoted by $\\sigma(z)=\\frac{1}{1+exp(-z)}$. \n",
    "\n",
    "\n",
    "<img src=\"bphidden.png\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "There are 2 sets of weights, the set of weights from the input to the hidden layer and the set of weights from \n",
    "the hidden layer to the output. \n",
    "\n",
    "The shape of the input weight mstrix is [number of inputs x number of hidden layers]\n",
    "The shape of the output hidden layer weight matrix is [number if hidden layers x number of outputs]\n",
    "If this is a classification problem the number of outputs is equal to the number of classes. \n",
    "The hidden layer activation function is a logistic type. \n",
    "The output layer activateion function is a logistic type. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "To generalize to multiple inputs/hidden layers: \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic loss not good with squared error loss because: \n",
    "1) small gradients near edges, will never converge. \n",
    "2) the gradients zig zag between + and - values. \n",
    "\n",
    "Insteand of logistic loss use Cross Entropy loss or multinomial logistic loss: \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "We also add anotehr modification to support multiple classes. Use the normalizaiton trick, divide output by sum of all inputs to get probability per class. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "![alt text](nn.png \"NN\" height=\"500px\" width=\"500px\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This is a 2 layer network with a logistic in the output of the hidden layer and a LOGISTIC at the outpue node. Usually there is a linear FC layer here. There are K inputs, N hidden neurons and M outputs.  The hidden layer is a function of the output $y_i$. Let's add a change of variable to $u_i$ for the hidden logistic node and a change of variable to v for the output logistic unit $y_i$. \n",
    "\n",
    "${h}_i=f({u}_i)=f(\\sum_{k=1}^{K}w_{ki}x_{k}) = \\frac{1}{1+e^{-u_i}}$\n",
    "\n",
    "where we defined:\n",
    "    \n",
    "${u}_i = \\sum_{k=1}^{K}w_{ki}x_{k}$\n",
    "\n",
    "The output layer:\n",
    "\n",
    "${y}_i=f({v}_i) = f(\\sum_{k=1}^{K}w^{\\prime}_{ij}h_{k})$\n",
    "\n",
    "where $h_i$ is the output of the logistic unit. If there is additipnal processing such as whitening; this is preformed \n",
    "here and output as hk. We define v as:\n",
    "\n",
    "    \n",
    "${v}_i = \\sum_{k=1}^{K}w^{\\prime}_{ij}x_{k}= \\frac{1}{1+e^{-v_i}}$    \n",
    "    \n",
    "    \n",
    "Minimize the squared error between the training set output and actual:\n",
    "\n",
    "$E=\\frac{1}{2}\\sum_{n=1}^{K}({t}_n - {y}_n)^2 $\n",
    "\n",
    "$from \\; y=\\frac{1}{e^{-z}+1} \\;and\\; \\frac{dy}{dz}=y(1-y)$\n",
    "\n",
    "we can calculate $\\frac{dy_i}{dv_i}$\n",
    "\n",
    "$\\frac{dy}{dv} \\;=\\; y(1-y)$\n",
    "\n",
    "\n",
    "At the ouptut node y changes as a function of the weights and input to the hidden layer.\n",
    "There are 2 layers we have to work through; first from the output of the hidden layer with weights wij prime then \n",
    "to the input of the hidden layer with weights wki. \n",
    "\n",
    "\n",
    "$\\frac{\\partial E}{\\partial y} = \\sum_{n=1}^{M}  -({t}_n - {y}_n) $\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>We want to determine the effect of changing weights on the output. There are 2 sets of weights, the weights after\n",
    "the hidden layer and the set of weights before the hidden layer. Changing the value of theseweights should narrow or widen\n",
    "the difference beween the training sample and output summed over all samples. We use the chain rule in 2 steps to first \n",
    "rewrite the dE/dy term in terms of the w prime weights then using the same logic the w set of weights</p>\n",
    "\n",
    "$\\frac{\\partial{E}}{\\partial {w^{\\prime}_{ij}}}=\\frac{\\partial E}{\\partial y_{j}} \\frac{dy}{dv_{j}} \\frac{\\partial v_j}{\\partial {w^{\\prime}_ij}}$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>substituting from above:</p>\n",
    "    \n",
    "$\\frac {\\partial E}{\\partial w^{\\prime}_{ij}} = -(t_n - y_n) \\cdot y_{j}(1-y_{j}) \\cdot h_{i}$    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>From the equation above we can see given an observed output from the NN we have an expression for the weights which\n",
    "depends on the layer before and the observed output value. This process of converting the output values to get the change\n",
    "in weights based on weight values and input of the previous stage is backpropagation and depends on the chain rule. To update\n",
    "the weights we add an additional term epsilon indicating the new weights are a small change to the desired output</p>\n",
    "\n",
    "\n",
    "$w^{\\prime updated}_{ki} = w^{\\prime old}_{ki}  -\\epsilon \\cdot (t_n - y_n) \\cdot y_{j}(1-y_{j}) \\cdot h_{i}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>the update rules for the next set of weights are backpropagated from this hidden layer stage to the input</p>\n",
    "\n",
    "$\\frac{\\partial E}{\\partial w_{ki}} =\\sum_{i=1}^{M}(\\frac{\\partial E}{\\partial y_i} \\cdot \\frac{\\partial y_i}{\\partial v_i} \\cdot \\frac{\\partial v_i}{\\partial h_i} )\\cdot \\frac{\\partial h_i}{\\partial u_i}\\frac{\\partial u_i}{\\partial w_{ki}}  $\n",
    "\n",
    "\n",
    "<p>The first part in parenthesis is what we just computed. The next 2 derivatives:</p>\n",
    "\n",
    "$\\frac{\\partial h_i}{\\partial u_i} = h_i \\cdot (1-h_i)$  where $h_i$ is the logistic\n",
    "\n",
    "$\\frac{\\partial u_i}{\\partial w_{ki} }  = x_i$\n",
    "\n",
    "$\\frac{\\partial E}{\\partial w_{ki}} = \\sum_{i=1}^{M}(t_i - y_i) \\cdot y_{i}(1-y_i) \\cdot$\n",
    "    \n",
    "<p>The weight update becomes:</p> \n",
    "\n",
    "$w^{updated}_ki = w^{old}_ki - -\\epsilon \\cdot (t_n - y_n) \\cdot y_{j}(1-y_{j}) \\cdot h_{i} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h4>BackPropagation Examples with weight sharing</h4>\n",
    "<img src=\"bp1.png\">\n",
    "<img src=\"bp1a.png\">\n",
    "<img src=\"bp2.png\">\n",
    "<img src=\"bp3.png\">\n",
    "<img src=\"bp4.png\">\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Vanishing gradient\n",
    "https://github.com/dmlc/mxnet-notebooks/blob/master/python/tutorials/understanding_vanish_gradient.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backpropatation exercise of unrolled RNN from Hinton:\n",
    "<img src=\"bprnn.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Find $\\frac{\\partial E}{\\partial z_1}$\n",
    "\n",
    "$\\frac{\\partial E}{\\partial z_1} = \\frac{\\partial E_0}{\\partial z_1} + \\frac{\\partial E_1}{\\partial z_1} + \\frac{\\partial E_2}{\\partial z_1}$\n",
    "partial E0 wrt Z1 is 0 bc it is behind in time to z1 so it should not contain any dependent terms on z1. \n",
    "\n",
    "\n",
    "<p></p>\n",
    "$\n",
    "\\frac{\\partial E_1}{\\partial z_1}=\n",
    "\\frac{\\partial E_1}{\\partial y_1}\\frac{\\partial y_1}{\\partial z_1}=\n",
    "\\frac{\\partial E_1}{\\partial y_1}\\frac{\\partial y_1}{\\partial h_1}\\frac{\\partial h_1}{\\partial z_1}\n",
    "$\n",
    "<p></p>\n",
    "$\\frac{\\partial E_1}{\\partial y_1}=-(t_1-y_1)=-(-0.1-.1)=-0.2$\n",
    "$\\frac{\\partial y_1}{\\partial h_1}=W_{hy}=0.25$\n",
    "$\\frac{\\partial h_1}{\\partial z_1}=h_1(1-h_1)=.4(1-0.4)=-.24$\n",
    "<p></p>\n",
    "$\\frac{\\partial E}{\\partial z_1}=-.2*.25*-.24=.012$                                      \n",
    "\n",
    "<p></p>\n",
    "$\n",
    "\\frac{\\partial E_2}{\\partial z_1}=\n",
    "\\frac{\\partial E_2}{\\partial y_2}\n",
    "\\frac{\\partial y_2}{\\partial h_2}\n",
    "\\frac{\\partial h_2}{\\partial z_2}\n",
    "\\frac{\\partial z_2}{\\partial h_1}\n",
    "\\frac{\\partial h_1}{\\partial z_1}\n",
    "$\n",
    "<p></p>\n",
    "$\\frac{\\partial E_2}{\\partial y_2}=-(t_2-y_2)=-(-0.2-.2)=.4$\n",
    "$\\frac{\\partial y_2}{\\partial h_2}=W_{hy}=.25$\n",
    "$\\frac{\\partial h_2}{\\partial z_2} = h_2(1-h_2)=.8*(1-.8)=-.16$\n",
    "$\\frac{\\partial z_2}{\\partial h_1}=W_{hh}=0.5$\n",
    "$\\frac{\\partial h_1}{\\partial z_1}=h_1(1-h_1)=.4(1-.4)=-.24$\n",
    "\n",
    "$\\frac{\\partial E_2}{\\partial z_1}=.4*.25*-.16*.5*-.24=.00192$\n",
    "<p></p>\n",
    "$.012+.00192=.01392$ \n",
    "<p></p>\n",
    "Given \n",
    "<p></p>\n",
    "$t_0,t_1,t_2$=[0.1,-0.1,-0.2]\n",
    "$x_0,x_1,x_2$=[18,9,-8]\n",
    "$h_0,h_1,h_2$=[.2,.4,.8]\n",
    "$y_0,y_1,y_2$=[.05,.1,.2]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#use to calcluate -.355 value. Too many mistakes by hand. \n",
    "def calculate(x0,x1,x2,Wxh,Xhh,Why,hbias,ybias):\n",
    "    z0 = Wxh*x0+hbias\n",
    "    h0 = 1/(1+np.exp(-z0))\n",
    "    y0=Why*h0+ybias\n",
    "    z1=Wxh*x1+Whh*h0+hbias\n",
    "    h1=1/(1+np.exp(-z1))\n",
    "    y1 = Why*h1+ybias\n",
    "    z2=Wxh*x2+Whh*h1+hbias\n",
    "    h2=1/(1+np.exp(-z2))\n",
    "    y2 = Why*h2+ybias\n",
    "    print(z0,z1,z2,h0,h1,h2,y0,y1,y2)\n",
    "    \n",
    "#calculate(18,9,-8,) \n",
    "calculate()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.0493585136504298e-162"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#vanishing gradient RNN example. \n",
    "import numpy as np\n",
    "def cal(x0,x1,x2,x3,Wxh,Whh,Why,hbias,ybias):\n",
    "    z0 = Wxh*x0+hbias\n",
    "    h0 = 1/(1+np.exp(-z0))\n",
    "    y0=Why*h0+ybias\n",
    "    z1=Wxh*x1+Whh*h0+hbias\n",
    "    h1=1/(1+np.exp(-z1))\n",
    "    y1 = Why*h1+ybias\n",
    "    z2=Wxh*x2+Whh*h1+hbias\n",
    "    h2=1/(1+np.exp(-z2))\n",
    "    y2 = Why*h2+ybias\n",
    "    z3=Wxh*x3+Whh*h2+hbias\n",
    "    h3=1/(1+np.exp(-z3))\n",
    "    y3 = Why*h3+ybias\n",
    "    return (z0,z1,z2,z3,h0,h1,h2,h3,y0,y1,y2,y3) \n",
    "\n",
    "results = cal(1,0,0,0,1,-2,1,0,0)\n",
    "\n",
    "prod=1\n",
    "for x in results:\n",
    "    prod *=x*prod \n",
    "prod\n",
    "\n",
    "#can see this goes to 0 real fast. even before adding the other Whh, Wxh constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "In the following network find $\\frac{\\partial E}{\\partial Wxh} by backpropating to t=0$\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
