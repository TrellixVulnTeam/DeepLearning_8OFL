{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "<h6>Corrections to Udacity Course</h6>\n",
    "The DL Udacity class was originally created by Vincent Vanhoucke, a research scientist and tech lead for Google Brain. \n",
    "This was a free class. Udacity removed some of Vincent's content, cut it up into 5 modules and added their own content\n",
    "from presented by youtube stars.\n",
    "<h6>Corrections to Uddacity Section1 Introduction</6>\n",
    "Welcome, Anaconda, Jupyter, Applying DL, Regression Videos. I did not view any of these. \n",
    "<h6>Corrections to Udacity Section2 Neural Networks</h6>\n",
    "<p>\n",
    "Matrix Math and NumPy Refresher, Intro to NN,your First Neural Network assignment, Model Evaluation and Validation, \n",
    "Sentiment Analysis with Andrew Trask, MiniFlow. \n",
    "I viewed the Intro to NN, First NN assignment, Model Evaluation and Validation videos\n",
    "</p>\n",
    "<h6>Corrections for Section2 NN - Intro to NN</h6>\n",
    "<p>This section is added by Udacity. There are no presentations by Vincent here. </p>\n",
    "</p>In this section Udacity presents examples for AND/OR/NOT/XOR perceptrons. A better idea would have\n",
    "been to write a MLP for a XOR gate and then to the IRIS example. This gives you a way to see numerical\n",
    "values and to use them to build intuition and debug tools. Difficult if not impossible to do this w/image data. \n",
    "They do not include code for a working XOR gate.</p>\n",
    "<p> Gradient descent; explain and code Gradient Descent. This is useless and misleading. DL does not use GD, \n",
    "it uses SGD which is very different. \n",
    "</p>\n",
    "<p>Model Evaluation and Validation : This presentation isn't clear in separating linear regression from classification. Linear\n",
    "    regression CAN be used for classification. \n",
    "    They present an exercise in filling out Confusion matrix for regression. Confusion matrices are usually reserved\n",
    "    for classifiers. You can use linear regression for linear classification. Logistic regression more popular here\n",
    "    with a probability interpretation. Logistic Regression is important to develop to explain softmax and to understand\n",
    "    the FC layer at the end of CNNs. \n",
    "    They are missing AUC/ROC score for 2 class classification. Make sure basics correct here, accuracy vs. errors. \n",
    "    They present over and underfitting and k-fold cross validation as a way to fix.  </p>\n",
    "    \n",
    "<p>Miniflow. Did not look at this. Code up a python version of tf. If the strategy is to train someone to become an open \n",
    "source contributor we should focus on gradients and how to calculate them especially in a distributed\n",
    "environement. </p>\n",
    "\n",
    "<h6>Corrections for Section2 CNN </h6>\n",
    "There are 10 modules here, Intro to TF, Cloud Computing, Deep Neural Networks, Convolutional Networks, Siraj's Image Classification, \n",
    "Weight Initialization, Image Classification, Siraj's Image Generation, AutoEncoders, Transfer Learning in tf.\n",
    "I did not comment on all of the sections above. Only selected ones. \n",
    "<p>Module 3: Deep Neural Networks</p>\n",
    "<p>Module 3 is a mix between Udacity doing an intro lecture and Vincent</p> \n",
    "<p>Module 3.2-3.3 cover a 2 layer network and RELUs. They don't cover why this solves the vanishing\n",
    "gradient problem with logistic/sigmoids. The goal of this module seems to be programming NNs in TF. \n",
    "</p>\n",
    "<p>Module 3.4  no lecture, covers TF code to classify MNIST digits. My preference is to start with a simpler example\n",
    "using XOR gates or IRIS b/c it allows for debugging and separates the formatting of convolutional layers\n",
    "away from NNs. The key is to focus first on a simple numerical example both with regression and classification. These\n",
    "are common interview questions. </p>\n",
    "<p>Vincent Module 3.5. Training a DNN and how adding layers for CNNS increase the capacity and is more efficient\n",
    "in terms of parameter efficiency and the deeper layers capture hierarchial/neighboring image effects. Initial layers\n",
    "in a CNN capture lines, higher layers capture partial shapes and next layers capture representations of  objects.</p>\n",
    "<img src=\"vincent35.png\"> Note to self: add resnets and skip layers and show deep vanishing gradients,\n",
    "<p>Module 3.6 no lecture covers model saving and restoring. Add details on graph/saving restoring. The simplifiction\n",
    "is they only cover saving and restoring variables, specifically initial weights and biases. They cover saving\n",
    "the final model and then restoring the final model and running an accuracy operation on test data. Saving variables is \n",
    "never used. They fail to explain the difference between graphs/models/variables and when/why you would do this.</p>\n",
    "<p>>Module 3.7 Finetuning, unclear if this is relevant or usable. Goal is to finetune a model. They present to do this \n",
    "you have to name the weights and biases with names and not let TF create variable names. If tf saves and restores \n",
    "a session there is no way for tf to know Variable0 is a saved variable so it fails to load a valid value. Udacity added\n",
    "content. </p>\n",
    "<p>Vincent Module 3.8 Regularization Intro. Make model bigger than necessary and take steps to not overfit. Hard\n",
    "to design exactly size of NN for data. Skinny Jeans problem, hard to get network to exactly fit like jeans. Try bigger\n",
    "pants and then prevent overfitting. </p>\n",
    "<p>Vincent Module 3.9 - Regularization. 1) Early termination still best way, 2) regularization, adding \n",
    "artificial constraints. L2 regulartization,add another term to prevent large weights. Some of the value of the\n",
    "weights go into the regularization term. </p>\n",
    "<p>Vincent Module 3.10 derivative of regularizatoin term beta*1/2*norm weight^2 = w. </p>\n",
    "<p>Vincent Module 3.11 - dropout. Dropout some of the activations. Set 1/2 to 0. Random. Hintons idea. The network\n",
    "can not rely on any given activation so it has to add some redundancies. IN practice this makes things more robust\n",
    "makes the network seem like ensemble. If dropout not working then you should probably using a bigger network. </p>\n",
    "<p>Vincent Module 3.12 - Dropout trick. Not only do you zero out 1/2 the activatoins but you scale the remaining\n",
    "ones by a factor of 2. 2x the ones not zeroed out. </p><img src=\"vincentdropout2.png\">\n",
    "\n",
    "\n",
    "\n",
    "<li>The gradient descent programming lessons from L2-10 to L2-13 do not use momentum or \n",
    "any of the tricks Vincent talks about in Lesson1 to get SGD to work. Adding implementing SGD from Hinton Coursera</li>\n",
    "<li>Add Tensorboard</li>\n",
    "<li>Add global_step</li>\n",
    "<li>Add compute_gradient, apply_gradient</li>\n",
    "<li>Add OO models</li>\n",
    "<li>Add Estimator</li>\n",
    "<p>Vincent covers convnets and how the shapes vary with the stride/patch size/border settings. The hw assignment\n",
    "requires you to set these manually. Keras does this automaticall. Mistake to not include Keras programming</p>\n",
    "\n",
    "\n",
    "\n",
    "<h6>Corrections to Udacity Lesson11</h6>\n",
    "\n",
    "\n",
    "The png below lists links listing chatbots and machine traslation. Those are 2 fundamentally different implementations\n",
    "A chatbot closely resembles a QA system while a ML application is a different architecture, similiar to performing\n",
    "text alignment across language models. \n",
    "The links also include a dynamic memory network and attention which they do not cover in the lectures.\n",
    "<img src=\"UdacityCorrectionDeepLearningLesson12.png\">\n",
    "\n",
    "<h6>Corrections to Udacity Section4 Recurrent Neural Networks</h6>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
