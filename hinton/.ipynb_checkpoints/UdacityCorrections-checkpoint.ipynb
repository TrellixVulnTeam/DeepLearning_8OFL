{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "<h6>Corrections to Udacity Course</h6>\n",
    "The DL Udacity class was originally created by Vincent Vanhoucke, a research scientist and tech lead for Google Brain. \n",
    "This was a free class. Udacity removed some of Vincent's content, cut it up into 5 modules and added their own content\n",
    "from presented by youtube stars.\n",
    "<h6>Corrections to Uddacity Section1 Introduction</6>\n",
    "Welcome, Anaconda, Jupyter, Applying DL, Regression Videos. I did not view any of these. \n",
    "<h6>Corrections to Udacity Section2 Neural Networks</h6>\n",
    "<p>\n",
    "Matrix Math and NumPy Refresher, Intro to NN,your First Neural Network assignment, Model Evaluation and Validation, \n",
    "Sentiment Analysis with Andrew Trask, MiniFlow. \n",
    "I viewed the Intro to NN, First NN assignment, Model Evaluation and Validation videos\n",
    "</p>\n",
    "<h6>Corrections for Section2 NN - Intro to NN</h6>\n",
    "<p>This section is added by Udacity. There are no presentations by Vincent here. </p>\n",
    "</p>In this section Udacity presents examples for AND/OR/NOT/XOR perceptrons. A better idea would have\n",
    "been to write a MLP for a XOR gate and then to the IRIS example. This gives you a way to see numerical\n",
    "values and to use them to build intuition and debug tools. Difficult if not impossible to do this w/image data. \n",
    "They do not include code for a working XOR gate.</p>\n",
    "<p> Gradient descent; explain and code Gradient Descent. This is useless and misleading. DL does not use GD, \n",
    "it uses SGD which is very different. \n",
    "</p>\n",
    "<p>Model Evaluation and Validation : This presentation isn't clear in separating linear regression from classification. Linear\n",
    "    regression CAN be used for classification. \n",
    "    They present an exercise in filling out Confusion matrix for regression. Confusion matrices are usually reserved\n",
    "    for classifiers. You can use linear regression for linear classification. Logistic regression more popular here\n",
    "    with a probability interpretation. Logistic Regression is important to develop to explain softmax and to understand\n",
    "    the FC layer at the end of CNNs. \n",
    "    They are missing AUC/ROC score for 2 class classification. Make sure basics correct here, accuracy vs. errors. \n",
    "    They present over and underfitting and k-fold cross validation as a way to fix.  </p>\n",
    "    \n",
    "<p>Miniflow. Did not look at this. Code up a python version of tf. If the strategy is to train someone to become an open \n",
    "source contributor we should focus on gradients and how to calculate them especially in a distributed\n",
    "environement. </p>\n",
    "\n",
    "<h6>Corrections for Section2 CNN </h6>\n",
    "There are 10 modules here, Intro to TF, Cloud Computing, Deep Neural Networks, Convolutional Networks, Siraj's Image Classification, \n",
    "Weight Initialization, Image Classification, Siraj's Image Generation, AutoEncoders, Transfer Learning in tf.\n",
    "I did not comment on all of the sections above. Only selected ones. \n",
    "<p>Deep Neural Networks</p>\n",
    "<li>The gradient descent programming lessons from L2-10 to L2-13 do not use momentum or \n",
    "any of the tricks Vincent talks about in Lesson1 to get SGD to work. They implement GD not SGD which is of no value\n",
    "as a learning exercise</li>\n",
    "<li>They implement a Forward pass on a MLP then calculate the Back propagation. They do not cover hand calculating\n",
    "gradients. </li>\n",
    "<li>Add matching gradient to calculations</li>\n",
    "<h6>Corrections to Udacity Section 3 Convolutional Neural Networks</h6>\n",
    "<p>Vincent describes DNN with intro to a 2 layer NN, ReLUs, \n",
    "Training, Save/Restore, Findtuning, Regularizations, Dropout. </p>\n",
    "The missing piece is writing tensorflow code. Udacity is supposed to provide this \n",
    "but is missing pieces which conform to the TF tutorials and source code. \n",
    "<li>Add Tensorboard</li>\n",
    "<li>Add global_step</li>\n",
    "<li>Add compute_gradient, apply_gradient</li>\n",
    "<li>Add OO models</li>\n",
    "<li>Add Estimator</li>\n",
    "<p>Vincent covers convnets and how the shapes vary with the stride/patch size/border settings. The hw assignment\n",
    "requires you to set these manually. Keras does this automaticall. Mistake to not include Keras programming</p>\n",
    "\n",
    "\n",
    "\n",
    "<h6>Corrections to Udacity Lesson11</h6>\n",
    "\n",
    "\n",
    "The png below lists links listing chatbots and machine traslation. Those are 2 fundamentally different implementations\n",
    "A chatbot closely resembles a QA system while a ML application is a different architecture, similiar to performing\n",
    "text alignment across language models. \n",
    "The links also include a dynamic memory network and attention which they do not cover in the lectures.\n",
    "<img src=\"UdacityCorrectionDeepLearningLesson12.png\">\n",
    "\n",
    "<h6>Corrections to Udacity Section4 Recurrent Neural Networks</h6>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
