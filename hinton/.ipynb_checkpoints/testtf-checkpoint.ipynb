{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np shape: (4, 2)\n",
      "Tensor(\"X1a:0\", shape=(?, 4), dtype=float32)\n",
      "Tensor(\"X2a:0\", shape=(4, 2), dtype=int64)\n",
      "X1a_static_shape: [None, 4] X1a_dynamic_shape: [<tf.Tensor 'unstack:0' shape=() dtype=int32>, <tf.Tensor 'unstack:1' shape=() dtype=int32>]\n",
      "X2a_static_shape: [None, 4] X2a_dynamic_shape: [<tf.Tensor 'unstack_1:0' shape=() dtype=int32>, <tf.Tensor 'unstack_1:1' shape=() dtype=int32>]\n",
      "weight: [10, 1] weight_dynamic_shape: [<tf.Tensor 'unstack_2:0' shape=() dtype=int32>, <tf.Tensor 'unstack_2:1' shape=() dtype=int32>]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# to reshape (5,1) back to (5,) you can use ...\n",
    "#shape tests\n",
    "#\n",
    "X_XOR=np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "Y_XOR = np.array([0,1,1,0])\n",
    "\n",
    "tf.reset_default_graph()\n",
    "#why do the above? get memory leak, keeps on adding variables and allocating space. \n",
    "#unstack:1 will get incremented to unstack:2, etc...everytime you run this you get new allocations\n",
    "#user program can cause OOM. \n",
    "\n",
    "print(\"np shape:\",X_XOR.shape)\n",
    "\n",
    "X1a = tf.placeholder(tf.float32, shape=([None,len(X_XOR)]), name=\"X1a\")\n",
    "X2a = tf.constant(X_XOR,name=\"X2a\")\n",
    "normal_dist = tf.truncated_normal(shape=[10,1],mean=0.0, stddev=.01,dtype=tf.float32, name='normaldist')\n",
    "weight = tf.Variable(normal_dist, name='weight')\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    print(X1a)\n",
    "    print(X2a)\n",
    "\n",
    "X1a_static_shape = X1a.shape.as_list()\n",
    "X1a_dynamic_shape = tf.unstack(tf.shape(X1a))\n",
    "\n",
    "X2a_static_shape = X1a.shape.as_list()\n",
    "X2a_dynamic_shape = tf.unstack(tf.shape(X1a))\n",
    "\n",
    "weight_static_shape = weight.shape.as_list()\n",
    "weight_dynamic_shape = tf.unstack(tf.shape(weight))\n",
    "\n",
    "print(\"X1a_static_shape:\",X1a_static_shape, \"X1a_dynamic_shape:\",X1a_dynamic_shape)\n",
    "print(\"X2a_static_shape:\",X2a_static_shape, \"X2a_dynamic_shape:\",X2a_dynamic_shape)\n",
    "\n",
    "print(\"weight:\",weight_static_shape, \"weight_dynamic_shape:\",weight_dynamic_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var0: [ 0.69999999  1.70000005]\n",
      "var1: [ 2.97000003  3.97000003]\n",
      "grads0: [ 0.1  0.1]\n",
      "grads1: [ 0.01  0.01]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "var0 = tf.Variable([1.0, 2.0], dtype=tf.float32)\n",
    "var1 = tf.Variable([3.0, 4.0], dtype=tf.float32)\n",
    "grads0 = tf.constant([0.1, 0.1], dtype=tf.float32)\n",
    "grads1 = tf.constant([0.01, 0.01], dtype=tf.float32)\n",
    "with tf.Session() as sess:\n",
    "    sgd_op = tf.train.GradientDescentOptimizer(3.0).apply_gradients(\n",
    "        zip([grads0, grads1], [var0, var1]))\n",
    "    tf.global_variables_initializer().run()\n",
    "    # Fetch params to validate initial values\n",
    "    #self.assertAllCloseAccordingToType([3.0, 4.0], var1.eval())\n",
    "    # Run 1 step of sgd\n",
    "    sgd_op.run()\n",
    "    print(\"var0:\",sess.run(var0))\n",
    "    print(\"var1:\",sess.run(var1))\n",
    "    print(\"grads0:\",sess.run(grads0))\n",
    "    print(\"grads1:\",sess.run(grads1))\n",
    "    \n",
    "#this is funny. The learning rate is 3.0 so 3.0*0.1 is .3. But when you do the multiply\n",
    "#you dont get .7. Is a little off bc rounding error. \n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# updating a variable; can't update a tensor b/c it is immutable. \n",
    "#a scatter updates only the first dimension in an array. \n",
    "\n",
    "# this a better style, define the graph first\n",
    "#\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    a = tf.Variable(initial_value=[[0, 0, 0, 0],[0, 0, 0, 0]])\n",
    "    b = tf.scatter_update(a, [0, 1], [[1, 0, 0, 0], [1, 0, 0, 0]])\n",
    "\n",
    "with tf.Session(graph=g) as sess:\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    print sess.run(a)\n",
    "    print sess.run(b)\n",
    "\n",
    "#scatter used to distribute cells to larger/smaller matrices\n",
    "#this for distributed copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tf practice; from the Udacity website\n",
    "# do not do this\n",
    "#learning rate\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "learning_rate = .001\n",
    "training_epocs = 20\n",
    "batch_size = 20\n",
    "display_step = 1\n",
    "n_input = 784\n",
    "n_classes = 10\n",
    "n_hidden_layer = 256\n",
    "\n",
    "#save_file = './model.ckpt'\n",
    "\n",
    "#Weights and biases\n",
    "weights = {\n",
    "    'hidden_layer' : tf.Variable(tf.random_normal([n_input, n_hidden_layer])),\n",
    "    'out': tf.Variable(tf.random_normal[n_classes])    \n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'hidden_layer' : tf.Variable(tf.random_normal([n_hidden_layer])),\n",
    "    'out': tf.Variable(tf.random_normal[n_classes])\n",
    "}\n",
    "\n",
    "\n",
    "#Input\n",
    "x = tf.placeholder(\"float\", [None,28,28,1])\n",
    "y = tf.placeholder(\"float\", [None,n_classes])\n",
    "\n",
    "x_flat = tf.reshape(x,[-1,n_input]) #this is different from numpy reshape, -1 vs. none reshape\n",
    "\n",
    "layer_1 = tf.add(tf.matmul(x_flat,weights['hidden_layer']), biases['hidden_layer'])\n",
    "layer_1 = tf.nn.relu('layer_1')\n",
    "logits = tf.add(tf.matmul(layer_1, weights['out']), biases['out'])\n",
    "\n",
    "#optimizer\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "#session\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(training_epochs):\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            sess.run(optimizer,feed_dict={x:batch_x, y:batch_y})\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
