{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np shape: (4, 2)\n",
      "Tensor(\"X1a:0\", shape=(?, 4), dtype=float32)\n",
      "Tensor(\"X2a:0\", shape=(4, 2), dtype=int64)\n",
      "X1a_static_shape: [None, 4] X1a_dynamic_shape: [<tf.Tensor 'unstack:0' shape=() dtype=int32>, <tf.Tensor 'unstack:1' shape=() dtype=int32>]\n",
      "X2a_static_shape: [None, 4] X2a_dynamic_shape: [<tf.Tensor 'unstack_1:0' shape=() dtype=int32>, <tf.Tensor 'unstack_1:1' shape=() dtype=int32>]\n",
      "weight: [10, 1] weight_dynamic_shape: [<tf.Tensor 'unstack_2:0' shape=() dtype=int32>, <tf.Tensor 'unstack_2:1' shape=() dtype=int32>]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# to reshape (5,1) back to (5,) you can use ...\n",
    "#shape tests\n",
    "#\n",
    "X_XOR=np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "Y_XOR = np.array([0,1,1,0])\n",
    "\n",
    "tf.reset_default_graph()\n",
    "#why do the above? get memory leak, keeps on adding variables and allocating space. \n",
    "#unstack:1 will get incremented to unstack:2, etc...everytime you run this you get new allocations\n",
    "#user program can cause OOM. \n",
    "\n",
    "print(\"np shape:\",X_XOR.shape)\n",
    "\n",
    "X1a = tf.placeholder(tf.float32, shape=([None,len(X_XOR)]), name=\"X1a\")\n",
    "X2a = tf.constant(X_XOR,name=\"X2a\")\n",
    "normal_dist = tf.truncated_normal(shape=[10,1],mean=0.0, stddev=.01,dtype=tf.float32, name='normaldist')\n",
    "weight = tf.Variable(normal_dist, name='weight')\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    print(X1a)\n",
    "    print(X2a)\n",
    "\n",
    "X1a_static_shape = X1a.shape.as_list()\n",
    "X1a_dynamic_shape = tf.unstack(tf.shape(X1a))\n",
    "\n",
    "X2a_static_shape = X1a.shape.as_list()\n",
    "X2a_dynamic_shape = tf.unstack(tf.shape(X1a))\n",
    "\n",
    "weight_static_shape = weight.shape.as_list()\n",
    "weight_dynamic_shape = tf.unstack(tf.shape(weight))\n",
    "\n",
    "print(\"X1a_static_shape:\",X1a_static_shape, \"X1a_dynamic_shape:\",X1a_dynamic_shape)\n",
    "print(\"X2a_static_shape:\",X2a_static_shape, \"X2a_dynamic_shape:\",X2a_dynamic_shape)\n",
    "\n",
    "print(\"weight:\",weight_static_shape, \"weight_dynamic_shape:\",weight_dynamic_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var0: [ 0.69999999  1.70000005]\n",
      "var1: [ 2.97000003  3.97000003]\n",
      "grads0: [ 0.1  0.1]\n",
      "grads1: [ 0.01  0.01]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "var0 = tf.Variable([1.0, 2.0], dtype=tf.float32)\n",
    "var1 = tf.Variable([3.0, 4.0], dtype=tf.float32)\n",
    "grads0 = tf.constant([0.1, 0.1], dtype=tf.float32)\n",
    "grads1 = tf.constant([0.01, 0.01], dtype=tf.float32)\n",
    "with tf.Session() as sess:\n",
    "    sgd_op = tf.train.GradientDescentOptimizer(3.0).apply_gradients(\n",
    "        zip([grads0, grads1], [var0, var1]))\n",
    "    tf.global_variables_initializer().run()\n",
    "    # Fetch params to validate initial values\n",
    "    #self.assertAllCloseAccordingToType([3.0, 4.0], var1.eval())\n",
    "    # Run 1 step of sgd\n",
    "    sgd_op.run()\n",
    "    print(\"var0:\",sess.run(var0))\n",
    "    print(\"var1:\",sess.run(var1))\n",
    "    print(\"grads0:\",sess.run(grads0))\n",
    "    print(\"grads1:\",sess.run(grads1))\n",
    "    \n",
    "#this is funny. The learning rate is 3.0 so 3.0*0.1 is .3. But when you do the multiply\n",
    "#you dont get .7. Is a little off bc rounding error. \n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# updating a variable; can't update a tensor b/c it is immutable. \n",
    "#a scatter updates only the first dimension in an array. \n",
    "\n",
    "# this a better style, define the graph first\n",
    "#\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    a = tf.Variable(initial_value=[[0, 0, 0, 0],[0, 0, 0, 0]])\n",
    "    b = tf.scatter_update(a, [0, 1], [[1, 0, 0, 0], [1, 0, 0, 0]])\n",
    "\n",
    "with tf.Session(graph=g) as sess:\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    print sess.run(a)\n",
    "    print sess.run(b)\n",
    "\n",
    "#scatter used to distribute cells to larger/smaller matrices\n",
    "#this for distributed copy\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
