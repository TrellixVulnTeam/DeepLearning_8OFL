{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training matrix shape (60000, 784)\n",
      "Testing matrix shape (10000, 784)\n",
      "(60000, 784) (60000,) (10000, 784) (10000,) (60000, 10) (10000, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dc/anaconda/envs/tf35/lib/python3.5/site-packages/keras/models.py:844: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 6s - loss: 0.2478 - val_loss: 0.1069\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 5s - loss: 0.0997 - val_loss: 0.0791\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 6s - loss: 0.0729 - val_loss: 0.0708\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 6s - loss: 0.0552 - val_loss: 0.0669\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 6s - loss: 0.0469 - val_loss: 0.0657\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 6s - loss: 0.0369 - val_loss: 0.0690\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 6s - loss: 0.0339 - val_loss: 0.0723\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 6s - loss: 0.0328 - val_loss: 0.0703\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 6s - loss: 0.0262 - val_loss: 0.0705\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 6s - loss: 0.0262 - val_loss: 0.0707\n",
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Activation,Dense,Dropout\n",
    "from keras.models import Sequential\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train = X_train.reshape(60000, 784)\n",
    "X_test = X_test.reshape(10000, 784)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(\"Training matrix shape\", X_train.shape)\n",
    "print(\"Testing matrix shape\", X_test.shape)\n",
    " \n",
    "# 6. Preprocess class labels\n",
    "Y_train = np_utils.to_categorical(y_train, 10)\n",
    "Y_test = np_utils.to_categorical(y_test, 10)\n",
    " \n",
    "print(X_train.shape, y_train.shape, X_test.shape,y_test.shape,Y_train.shape,Y_test.shape)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(784,)))\n",
    "model.add(Activation('relu')) # An \"activation\" is just a non-linear function applied to the output\n",
    "                              # of the layer above. Here, with a \"rectified linear unit\",\n",
    "                              # we clamp all values below 0 to 0.\n",
    "                           \n",
    "model.add(Dropout(0.2))   # Dropout helps protect the model from memorizing or \"overfitting\" the training data\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax')) # This special \"softmax\" activation among other things,\n",
    "                                 # ensures the output is a valid probaility distribution, that is\n",
    "                                 # that its values are all non-negative and sum to 1.\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.fit(X_train, Y_train,\n",
    "          batch_size=128, nb_epoch=10, verbose=1,\n",
    "          validation_data=(X_test, Y_test))\n",
    "\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n",
      "acc: 99.64%\n"
     ]
    }
   ],
   "source": [
    "from keras.models import model_from_json\n",
    "\n",
    "\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    " \n",
    "# evaluate loaded model on test data\n",
    "loaded_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "score = loaded_model.evaluate(X_test, Y_test, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MNIST data...\n",
      "train_x shape: (60000, 1, 28, 28) train_y shape: (60000, 10)\n",
      "validation_x shape: (10000, 1, 28, 28) validation_y shape (10000, 10)\n",
      "Training matrix shape (60000, 1, 28, 28) (60000, 10) (10000, 1, 28, 28) (10000, 10)\n",
      "(60000, 1, 28, 28) (60000, 10) (10000, 1, 28, 28) (10000, 10) (600000, 10) (100000, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dc/anaconda/envs/tf35/lib/python3.5/site-packages/keras/models.py:844: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 10s - loss: 0.6538 - acc: 0.8034 - val_loss: 0.2173 - val_acc: 0.9343\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 10s - loss: 0.1638 - acc: 0.9514 - val_loss: 0.1148 - val_acc: 0.9654\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 10s - loss: 0.1096 - acc: 0.9666 - val_loss: 0.0835 - val_acc: 0.9739\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 10s - loss: 0.0874 - acc: 0.9732 - val_loss: 0.0690 - val_acc: 0.9782\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 10s - loss: 0.0734 - acc: 0.9777 - val_loss: 0.0590 - val_acc: 0.9802\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 11s - loss: 0.0639 - acc: 0.9802 - val_loss: 0.0541 - val_acc: 0.9820\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 10s - loss: 0.0579 - acc: 0.9818 - val_loss: 0.0508 - val_acc: 0.9833\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 10s - loss: 0.0522 - acc: 0.9837 - val_loss: 0.0527 - val_acc: 0.9839\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 10s - loss: 0.0475 - acc: 0.9852 - val_loss: 0.0553 - val_acc: 0.9826\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 11s - loss: 0.0440 - acc: 0.9869 - val_loss: 0.0506 - val_acc: 0.9850\n",
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "#CNN KERAS\n",
    "import keras\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Activation,Dense,Dropout,Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.models import Sequential\n",
    "\n",
    "num_class = 10  # number of class\n",
    "\n",
    "input_shape = (1, 28, 28)\n",
    "def preprocess_input(x):\n",
    "    return x.reshape((-1, ) + input_shape) / 255.\n",
    "\n",
    "\n",
    "def preprocess_output(y):\n",
    "    return np_utils.to_categorical(y)\n",
    "\n",
    "(train_x, train_y), (validation_x, validation_y) = mnist.load_data()\n",
    "train_x, validation_x = map(preprocess_input, [train_x, validation_x])\n",
    "train_y, validation_y = map(preprocess_output, [train_y, validation_y])\n",
    "print('Loading MNIST data...')\n",
    "print('train_x shape:', train_x.shape, 'train_y shape:', train_y.shape)\n",
    "print('validation_x shape:', validation_x.shape,\n",
    "      'validation_y shape', validation_y.shape)\n",
    "\n",
    "print(\"Training matrix shape\", train_x.shape,train_y.shape, validation_x.shape, validation_y.shape)\n",
    "\n",
    "Y_train = np_utils.to_categorical(train_y, 10)\n",
    "Y_test = np_utils.to_categorical(validation_y, 10)\n",
    " \n",
    "print(train_x.shape, train_y.shape,validation_x.shape,validation_y.shape,Y_train.shape, Y_test.shape)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32,3, input_shape= (1,28,28),padding='same', name='conv1'))\n",
    "model.add(MaxPooling2D(2, name='pool1', data_format=\"channels_first\"))\n",
    "model.add(Conv2D(64, 3, padding='same', name='conv2'))\n",
    "model.add(MaxPooling2D(2, name='pool2', data_format=\"channels_first\"))\n",
    "model.add(Flatten(name='flatten'))\n",
    "model.add(Dense(64, activation='relu', name='fc1'))\n",
    "model.add(Dense(num_class, activation='softmax', name='fc2'))\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "            optimizer=SGD(lr=0.01, momentum=0.9),\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_x, train_y,\n",
    "          batch_size=128, nb_epoch=10,verbose=1,\n",
    "          validation_data=(validation_x, validation_y))\n",
    "\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#keras CNN cifar\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
