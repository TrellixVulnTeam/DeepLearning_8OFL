{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Linear separability Demo using Hinton's code as example. Converted from octave\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_init shape: (3,)\n",
      "self.neg[i]: [[-0.80857  0.83721  1.     ]] type: <class 'numpy.ndarray'>\n",
      "x.shape: (1, 3)\n",
      "x.T shape: (3, 1)\n",
      "self.w shape: (3,)\n",
      "activation: [[ 0.]\n",
      " [ 0.]\n",
      " [ 0.]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-03c042489ca1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0mperc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetExamples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneg_examples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpos_examples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0mperc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetWeight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_init\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m \u001b[0mperc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-03c042489ca1>\u001b[0m in \u001b[0;36miterate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mactivation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw_init\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'activation:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mactivation\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'activation > 0 updating weights'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0mdeltaw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#simple linear neuron. If less than activation, 0 if more 1. Binary class training. \n",
    "#the definiton of a perceptron is a linear neuron with a binary activation function\n",
    "#\n",
    "class perceptron:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.x=0\n",
    "        self.w_init=0\n",
    "        self.w_neg=0\n",
    "        self.w_pos=0\n",
    "        self.neg = 0\n",
    "        self.pos = 0\n",
    "        \n",
    "    def setExamples(self,n,p):\n",
    "        self.neg=n\n",
    "        self.pos=p\n",
    "        \n",
    "    def setX(self,input):\n",
    "        self.x = input\n",
    "    \n",
    "    def setWeight(self,weight):\n",
    "        self.w = weight\n",
    "    \n",
    "    def iterate(self):\n",
    "        \n",
    "        for i in range(len(self.neg)):\n",
    "            print ('self.neg[i]:',self.neg[i],'type:',type(self.neg[i]))\n",
    "            x = self.neg[i]\n",
    "            print ('x.shape:',x.shape)\n",
    "            print ('x.T shape:',x.T.shape)\n",
    "            print ('self.w shape:', self.w.shape)\n",
    "            activation = np.dot(x.T, self.w_init)\n",
    "            print ('activation:', activation)\n",
    "            if activation >= 0:\n",
    "                print('activation > 0 updating weights')\n",
    "                deltaw = x*(0-activation);\n",
    "                self.w_neg = self.w_neg + deltaw;\n",
    "                print('neg_examples deltaw:\\t', deltaw, '\\n');\n",
    "                print('neg_examples after update w:\\t', self.w, '\\n');\n",
    "        \n",
    "        for i in range(len(self.pos)):\n",
    "            print (self.pos[i])\n",
    "            x = np.array(self.pos[i])\n",
    "            print ('x.shape:',x.shape)\n",
    "            print ('x.T shape:',x.T.shape)\n",
    "            print ('self.w shape:', self.w.shape)\n",
    "            activation = np.dot(x.T, self.w)\n",
    "            print ('neg activation:', activation)\n",
    "            if activation > 0:\n",
    "                print('activation > 0 updating weights')\n",
    "                deltaw = x*(1-activation);\n",
    "                self.w = self.w + deltaw;\n",
    "                print('pos_examples deltaw:\\t', deltaw, '\\n');\n",
    "                print('pos_examples after update w:\\t', self.w, '\\n');\n",
    "            \n",
    "       \n",
    "            \n",
    "    \n",
    "   \n",
    "  \n",
    "\n",
    "#tehre are 4 datasets here from hinton assignment1 coursera\n",
    "            \n",
    "pos_examples =[ np.array([[0.871429,0.624585,1]]),np.array([[ -0.020000,-0.923588,1]]),\n",
    "                         np.array([[0.362857,-0.318937,1]]),np.array([[0.888571,-0.870432,1]])]\n",
    "\n",
    "neg_examples = [np.array([[-0.80857,0.83721,1]]),\n",
    "                np.array([[0.35714,0.85050,1]]),\n",
    "                np.array([[-0.75143,-0.73090,1]]),\n",
    "                np.array([[-0.30000,0.12625,1]])]\n",
    "\n",
    "#dataset2\n",
    "#pos_examples_nobias =\n",
    "#\n",
    "#   0.871429   0.624585\n",
    "#  -0.020000  -0.923588\n",
    "#   0.362857  -0.318937\n",
    "#   0.888571  -0.870432\n",
    "#  -0.528571   0.511628\n",
    "#neg_examples_nobias =\n",
    "#\n",
    "#  -0.80857   0.83721\n",
    "#   0.35714   0.85050\n",
    "#  -0.75143  -0.73090\n",
    "#  -0.30000   0.12625\n",
    "#   0.64286  -0.54485\n",
    "\n",
    "\n",
    "#pos_examples_nobias =\n",
    "#\n",
    "#  -0.76286  -0.19269\n",
    "#  -0.60286  -0.48505\n",
    "#  -0.38571  -0.69767\n",
    "#  -0.19143  -0.35880\n",
    "#   0.28286  -0.43189\n",
    "#   0.40857  -0.69767\n",
    "#   0.75143  -0.23256\n",
    "#\n",
    "#>> neg_examples_nobias\n",
    "#neg_examples_nobias =\n",
    "#\n",
    "#  -0.791429   0.079734\n",
    "#  -0.557143   0.411960\n",
    "#  -0.225714   0.697674\n",
    "#   0.162857   0.830565\n",
    "#   0.460000   0.657807\n",
    "#   0.734286   0.338870\n",
    "#   0.825714  -0.013289\n",
    "\n",
    "#pos_examples_nobias =\n",
    "#\n",
    "#  -0.0885714   0.2325581\n",
    "#   0.0085714  -0.0664452\n",
    "#   0.1285714  -0.3255814\n",
    "#   0.2885714  -0.5249169\n",
    "#   0.5971429  -0.4651163\n",
    "#   0.7171429  -0.1461794\n",
    "#  0.8600000   0.1594684\n",
    "#   0.9457143   0.4451827\n",
    "\n",
    "#>> neg_examples_nobias\n",
    "#neg_examples_nobias =\n",
    "#\n",
    "#  -0.865714  -0.392027\n",
    "#  -0.785714  -0.172757\n",
    "#  -0.557143   0.245847\n",
    "#  -0.288571   0.524917\n",
    "#  -0.128571   0.524917\n",
    "#   0.082857   0.332226\n",
    "#  0.180000   0.106312\n",
    "#   0.254286  -0.166113\n",
    "#   0.328571  -0.352159\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "w_init = np.array([-0.62170,0.76092,0.77187])\n",
    "print ('w_init shape:',w_init.shape)\n",
    "perc  = perceptron()\n",
    "perc.setExamples(neg_examples,pos_examples)\n",
    "perc.setWeight(w_init)\n",
    "perc.iterate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h4>Definition of a Neuron</h4>\n",
    "A neuron in contrast to fitting weights or coefficients to the equation of a line requires the addition of \n",
    "an activation function at the output of the equation of a line. \n",
    "\n",
    "\n",
    "<img src=\"neuron1.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Activation function</h4>\n",
    "An example of an activation function is a binary threshold operator. \n",
    "Output 1 if input > threshold\n",
    "Output 0 if input < threshold\n",
    "\n",
    "<img src=\"activation1.png\">\n",
    "<img src=\"activation1a.png\">\n",
    "\n",
    "The subscripts either start from 0 or 1 if you include the b term\n",
    "Explained in more detail in backprop workbook\n",
    "\n",
    "There are different activations, logistic/sigmoid, relu, selu, tanh, etc... The requirement is we can write a program\n",
    "to approximate a gradient for the activaiton functions. \n",
    "\n",
    "Figure from Hinton Coursera"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Examples of threshold activations with boolean logic </h4>\n",
    "<img src=\"neuron2.png\">\n",
    "<img src=\"neuron2a.png\">\n",
    "<img src=\"neuron3.png\">\n",
    "<img src=\"neuron3a.png\">\n",
    "<img src=\"neuron4.png\">\n",
    "<img src=\"neuron5.png\">\n",
    "<img src=\"neuron5a.png\">\n",
    "The XOR gate can not be implemented with a single neuron but can be implemented using a network of 2 neurons. \n",
    "This was a result of the McCullough Pitts(1954) paper. The multi-neuron networks are referred to as perceptrons\n",
    "Figures from Hinton? Forgot.. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial wt: [[ 1.]\n",
      " [ 1.]\n",
      " [ 1.]]\n",
      "output [[ 0.00574536]\n",
      " [ 0.00382618]\n",
      " [ 0.99745138]\n",
      " [ 0.99617055]]\n",
      "weights: [[ 11.12326143]\n",
      " [ -0.40845247]\n",
      " [ -5.15360652]]\n"
     ]
    }
   ],
   "source": [
    "placeholder for perceptron learning procedure. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h4>Perceptron Learning Procedure</h4>\n",
    "<li>A perceptron like the XOR gate above has only 1 layer. There are no hidden layers.</li> \n",
    "<li>A perceptron is a feedfoward network without hidden layers. </li>\n",
    "<li>A perceptron can only converge if the input is linearly separable; proven in 1962 by Block/Novikoff</li>\n",
    "<li>The perceptron learning procedure requires iteration of the weights using gradient descent. </li>\n",
    "<li>The perceptron learning procedure does not work if there are hidden layers which require the development\n",
    "of backpropagation. </li>\n",
    "<li>Perceptrons can only train on 1 example at a time whereas in feedforward neural networks we see later we train on\n",
    "batches or all of the training data in each iteration. </li>\n",
    "\n",
    "\n",
    "The perceptron learning procedure outputs the weights in a single iteration per example\n",
    "using the following rule:\n",
    "    \n",
    "If the output is not correct, update the weights. \n",
    "    \n",
    "$weight = weight+weight_update$\n",
    "<p></p>\n",
    "$weight_update = learning_rate * error(x_i,w_i,y_expected) * input * derivative_activation(w_i,x_i)$\n",
    "<p></p>\n",
    "\n",
    "\n",
    "\n",
    "Reference: coursera Hinton\n",
    "<p></p>\n",
    "Reference: https://page.mi.fu-berlin.de/rojas/neural/chapter/K4.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
