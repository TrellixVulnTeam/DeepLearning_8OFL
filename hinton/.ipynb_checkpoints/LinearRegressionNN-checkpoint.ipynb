{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Linear Regression </h4>\n",
    "\n",
    "Brief derivation of OLS meethod of Linear Regression. \n",
    "There are many alternate derivations \n",
    "Assume a system of linear equations or assume a set of points with the goal of \n",
    "drawing a line through them. \n",
    "\n",
    "\n",
    "\n",
    "<img src=\"linreg.png\" height=\"300\", width=\"300\">\n",
    "\n",
    "For a set of input points$x_i, y_i$ define a least squares distance measure: \n",
    "    \n",
    "$error = (y_i-x_i)^2$\n",
    "\n",
    "Take the derivative to minimize this error and set it to 0 to calculate the coefficients:\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "pos_examples1 = np.array([[0.871429,0.624585,1],[ -0.020000,-0.923588,1],\n",
    "                         [0.362857,-0.318937,1],[0.888571,-0.870432,1]])\n",
    "\n",
    "neg_examples1 = np.array([[-0.80857,0.83721,1],[0.35714,0.85050,1],\n",
    "                         [-0.75143,-0.73090,1],[-0.30000,0.12625,1]\n",
    "                        ])\n",
    "\n",
    "\n",
    "\n",
    "x=pos_examples1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The simplest neuron is a linear neuron used for performing linear separation. \n",
    "\n",
    "\n",
    "<img src=\"linearnn1.png\">\n",
    "\n",
    "We define the operation of the neuron as multiplying the input with a weight: $w_i$. \n",
    "Define the loas function as the expected output minus the actual squared. $loss = \\frac{1}{2}\\sum\\limits_{i=1}^n(t_n-y_n)^2$. \n",
    "We are going to take the sum of all the errors for all the sample points and update the weights. This is batch training. \n",
    "If we process 1 sample at at time this is online training. To find the values of weights we use gradient descent. \n",
    "\n",
    "\n",
    "We want to minimize loss as a function of the weights. Using the chain rule we can expand the partial deriviative WRT w\n",
    "as follows: \n",
    "\n",
    "$\\frac{\\partial loss}{\\partial w_i} = \\frac{\\partial y_i}{\\partial w_i} \\cdot \\frac{\\partial loss}{\\partial y_i}$\n",
    "\n",
    "$\\frac{\\partial loss}{\\partial w_i} = \\frac{1}{2} \\cdot (2)(t_i-y_i)(-1) = -(t_i-y_i)$\n",
    "\n",
    "$y_i = \\sum\\limits_{i=1}^N w_ix_i$ so $\\frac{\\partial y_i}{\\partial w_i} = \\sum\\limits_{i=1}^N x_i$\n",
    "\n",
    "$w_{i+1} = w_i+\\delta \\cdot w_i$\n",
    "\n",
    "$\\delta w_i = \\epsilon \\cdot \\frac{\\partial loss}{\\partial w_i} $\n",
    "where $\\epsilon$ is the learning rate\n",
    "\n",
    "$\\delta w_i = \\epsilon \\cdot \\sum\\limits_{i=1}^N x_i(t_i-y_i)$\n",
    "\n",
    "Set the initial weights to a random value with variance=1 and compute the delta w for all the training samples and\n",
    "keep iterating until the weight values converge. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 't' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-3da2134baebb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mdw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mdw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 't' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "w=0\n",
    "\n",
    "\n",
    "def loss(y,t):\n",
    "    \"\"\"\n",
    "    input y:output of neuron \n",
    "    input t: training case output of neuron \"\"\"\n",
    "    return .5 * np.sum(np.square(np.subtract(t-y)))\n",
    "\n",
    "\n",
    "\n",
    "def gradient(w,x,t):\n",
    "    #do we need transpose for other dims besides 1? \n",
    "    y = np.dot(x.T,w)\n",
    "    print (\"gradient:\",y)\n",
    "    return -np.subtract(t,y)\n",
    "\n",
    "def update_weight(w,x,t,learning_rate=.001):\n",
    "    deltaw = learning_rate * gradient(w,x,t)\n",
    "    print(\"deltaw\",deltaw)\n",
    "    return dw\n",
    "    \n",
    "\n",
    "for i in range(10):\n",
    "    dw = update_weight(w,x,t)\n",
    "    w = w+dw\n",
    "    print (w)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Logistic Regression for classification. Simplest case for 2 classes. \n",
    "\n",
    "\n",
    "\n",
    "Logistic Neuron\n",
    "$y = \\frac{1}{1+exp(-z)}$\n",
    "\n",
    "\n",
    "$\\frac{dy}{dz}=y(1-y)$\n",
    "    \n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "<h4>Logistic Regression</h4>\n",
    "<p></p>\n",
    "Another way to look at the linear separation problem above is to consider the case of a 2 class\n",
    "classificaiton problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h4>Softmax regression model/classification and cross entropy</h4>\n",
    "\n",
    "\n",
    "For more than 2 classes we can use a combination of logistic regression models or softmax regression which is the\n",
    "generalization of logistic regression to multiple classes. \n",
    "\n",
    "\n",
    "We saw the logistic function used to output probability and \n",
    "$\\{ (x^{(1)}, y^{(1)}), \\ldots, (x^{(m)}, y^{(m)}) \\}$ from a dataset with input X, class Y per input and with m trained \n",
    "samples. Using Ng's notation our hypothesis/logistic is: \n",
    "$\n",
    "\\begin{align}\n",
    "h_\\theta(x) = \\frac{1}{1+\\exp(-\\theta^Tx)},\n",
    "\\end{align}\n",
    "$\n",
    "where $\\theta$ is our weight matrix. Hinton uses notation $y=\\frac{1}{1+exp(-z)}$\n",
    "\n",
    "For multiple classes logistc loss also called the cross entropy loss function is: \n",
    "    \n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "J(\\theta) = -\\frac{1}{m} \\left[ \\sum_{i=1}^m y^{(i)} \\log h_\\theta(x^{(i)}) + (1-y^{(i)}) \\log (1-h_\\theta(x^{(i)})) \\right]\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "which is derived from the law of total probability for multiple classes. Ecah class is reprented by $y_k$ where for 10 classes k would be 1..10\n",
    "\n",
    "The softmax loss is: \n",
    "    \n",
    "$p(y^{(i)} = j | x^{(i)} ; \\theta) = \\frac{e^{\\theta_j^T x^{(i)}}}{\\sum_{l=1}^k e^{ \\theta_l^T x^{(i)}} }$\n",
    "\n",
    "and the derivative of the logistic loss is in Ng notation:\n",
    "    \n",
    "$    \n",
    "\\begin{align}\n",
    "\\nabla_{\\theta_j} J(\\theta) = - \\frac{1}{m} \\sum_{i=1}^{m}{ \\left[ x^{(i)} \\left( 1\\{ y^{(i)} = j\\}  - p(y^{(i)} = j | x^{(i)}; \\theta) \\right) \\right]  }\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "Hinton calls Cross entropy as the right cost function to use for softmax. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "RELU activation function\n",
    "\n",
    "$ f(x)=max(0,x)$\n",
    "\n",
    "The RELUa activation function is used in multilayer deep networks, especially CNNs to compensate for vanishing gradients \n",
    "during backpropagation. The last layer is a FC layer to turn the output into a probability distribution. One of the problems\n",
    "with a RELU is returning a 0 causing a multiply by 0 which destroys coefficients. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    \"\"\"if x>0 1, else 0.\"\"\"\n",
    "    return np.maximum(0,x)\n",
    "\n",
    "def derivative_relu(x):\n",
    "    if x<=0:\n",
    "        return 0\n",
    "    return 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SELU activation function\n",
    "\n",
    "$\n",
    "  selu(x)=\\lambda \\begin{cases}\n",
    "               x \\;for \\;x>0\\\\\n",
    "               \\alpha e^x-a \\;for \\;x<0\\\\\n",
    "            \\end{cases}\n",
    "$\n",
    "\n",
    "\n",
    "\n",
    "$\n",
    "  \\frac {\\partial selu(x)}{\\partial x}=\\lambda \\begin{cases}\n",
    "               1 \\;for \\;x>0 \\\\\n",
    "               \\alpha e^x \\;for\\; x<0\\\\\n",
    "            \\end{cases}\n",
    "$\n",
    "\n",
    "with Î»=1.0507,a=1.6733\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def selu(lambda,x):\n",
    "    \"\"\"if x>0 1, else 0.\"\"\"\n",
    "    return np.maximum(0,x)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
