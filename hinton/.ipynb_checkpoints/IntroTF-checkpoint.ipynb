{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h4>Intro to TF</h4>\n",
    "<p>Document how the input stages using tf.nn.conv2d have to match the dimensions using filters\n",
    "and strides. Google Vincent does a good job. Document this here. </p>\n",
    "\n",
    "<p></p>\n",
    "There are 2 tf APIS, the old/original tf.nn and the newer tf.layers API. Layers was built after the success\n",
    "of keras. \n",
    "<p></p>\n",
    "An input into a TF graph consists of in input stage which specifies the input dimension of the input tensor. \n",
    "If an image you can give it RGB or BGR(openCV compatability) order. The input specifications require \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'interactive'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-9e205e495973>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoc_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minteractive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'interactive'"
     ]
    }
   ],
   "source": [
    "# this is a little different interface than tf.layers. Is it good or bad? \n",
    "# Don't need to add the weights, those are included automatically!!\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import layers\n",
    "import numpy as np\n",
    "\n",
    "def mlp(x):\n",
    "    fc1 = tf.contrib.layers.fully_connected(x,256,activation_fn=tf.nn.relu)\n",
    "    fc2 = tf.contrib.layers.fully_connected(fc1,256,activation_fn = tf.nn.relu)\n",
    "    out = tf.contrib.layers.fully_connected(fc2,10,activation_fn=None)\n",
    "    return out\n",
    "\n",
    "x = tf.placeholder(tf.float32,[None,784])\n",
    "y = tf.placeholder(tf.float32,[None,10])\n",
    "\n",
    "pred = mlp(x)\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred,labels=y))\n",
    "train_op = tf.train.AdamOptimizer(learning_rate=.01).minimize(loss)\n",
    "\n",
    "def train(session):\n",
    "    batch_size=200\n",
    "    session.run(tf.initialize_all_variables())\n",
    "    \n",
    "    for epoch in range(10):\n",
    "        epoch_loss = 0.0\n",
    "        batch_steps = mnist.train.num_examples/batch_size\n",
    "        for step in range(batch_steps):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            _,c = session.run([train_op,loss],{x:batch_x, y:batch_y})\n",
    "            epoch_loss += c/batch_steps\n",
    "        print(epoch,epoc_loss)\n",
    " \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "0.917\n"
     ]
    }
   ],
   "source": [
    "#MNIST TF not CNN\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "y = tf.nn.softmax(tf.matmul(x, W) + b)\n",
    "y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "for i in range(1000):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cnn mnist\n",
    "\n",
    "import time\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "def timing(f):\n",
    "    def wrap(*args):\n",
    "        time1 = time.time()\n",
    "        ret = f(*args)\n",
    "        time2 = time.time()\n",
    "        print ('%s function took %0.3f ms' % (f.func_name, (time2-time1)*1000.0))\n",
    "        return ret\n",
    "    return wrap\n",
    "\n",
    "\n",
    "\n",
    "time1 = time.time()\n",
    "W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "b_conv1 = bias_variable([32])\n",
    "x_image = tf.reshape(x, [-1,28,28,1])\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "W_fc2 = weight_variable([1024, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "\n",
    "y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_,logits=y_conv ))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "#sess = tf.Session() add session=sess\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for i in range(4000):\n",
    "    batch = mnist.train.next_batch(50)\n",
    "    if i%100 == 0:\n",
    "        train_accuracy = accuracy.eval(session=sess,feed_dict={x:batch[0], y_: batch[1], keep_prob: 1.0})\n",
    "        print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "    train_step.run(session=sess,feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n",
    "\n",
    "print(\"test accuracy %g\"%accuracy.eval(session=sess,feed_dict={\n",
    "    x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))\n",
    "\n",
    "time2 = time.time()\n",
    "print ('minutes:' , (time2-time1)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.framework.ops.Graph object at 0x111ffb5f8>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "g = tf.Graph()\n",
    "print (g)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h4> Refactor to a model. This is a basic template. </h4>\n",
    "<code>\n",
    "class Model:\n",
    "    def __init__(self, params):\n",
    "        pass\n",
    "    def _create_placeholders(self):\n",
    "        \"\"\" Step 1: define the placeholders for input and output. Placeholders are initialized\n",
    "        at runtime, variables are initialized when you create them. \"\"\"\n",
    "        pass\n",
    "    def _create_embedding(self):\n",
    "        \"\"\" Step 2: define weights. Weights are supposed to be reused for transfer learning and\n",
    "        for generating incremental updated models as new data arrives in realtime. Embeddings are\n",
    "        the exported weights as in word2vec and entity embeddings\"\"\"\n",
    "        pass\n",
    "    def _create_loss(self):\n",
    "         \"\"\" Step 3 + 4: define the inference + the loss function.  \"\"\"\n",
    "        pass\n",
    "    def _create_optimizer(self):\n",
    "        \"\"\" Step 5: define optimization\"\"\"\n",
    "    def _train(self):\n",
    "        \"\"\"Step 6: train the model\"\"\"\n",
    "    def _test(self):\n",
    "        \"\"\"Step 7: test and display summaries\"\"\"\n",
    "</code>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./train-images-idx3-ubyte.gz\n",
      "Extracting ./train-labels-idx1-ubyte.gz\n",
      "Extracting ./t10k-images-idx3-ubyte.gz\n",
      "Extracting ./t10k-labels-idx1-ubyte.gz\n",
      "0.8596\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, mnist):\n",
    "        self.mnist = mnist\n",
    "        self.learning_rate = .1\n",
    "    def create_placeholders(self):\n",
    "        self.x = tf.placeholder(tf.float32,shape = [None,784],name=\"x\")\n",
    "        self.y = tf.placeholder(tf.float32,shape = [None,10],name=\"y\")\n",
    "    def create_embeddings(self):\n",
    "        self.W = tf.Variable(tf.zeros([784,10]),name=\"W\")\n",
    "        self.b = tf.Variable(tf.zeros([10]),name=\"b\")\n",
    "        self.output = tf.nn.softmax(tf.add(tf.matmul(self.x,self.W), self.b),name=\"output\")\n",
    "    def create_loss(self):\n",
    "        self.loss = tf.reduce_mean(-tf.reduce_sum(self.y * tf.log(self.output), reduction_indices=[1]),name=\"loss\")\n",
    "    def create_optimizer(self):\n",
    "        self.optimizer = tf.train.GradientDescentOptimizer(self.learning_rate,name=\"optimizer\").minimize(self.loss)\n",
    "    def train(self):\n",
    "        global_step = tf.Variable(0)\n",
    "        self.sess = tf.Session()\n",
    "        init = tf.global_variables_initializer()\n",
    "        self.sess.run(init)\n",
    "        for x in range(100):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "            self.sess.run(self.optimizer, feed_dict={self.x: batch_xs, self.y: batch_ys})\n",
    "    def test(self):\n",
    "        correct_prediction = tf.equal(tf.argmax(self.output,1), tf.argmax(self.y,1),name=\"equal\")\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32),name=\"accuracy\")\n",
    "        print(self.sess.run(accuracy, feed_dict={self.x: self.mnist.test.images, self.y: self.mnist.test.labels}))\n",
    "\n",
    "        \n",
    "mnist = input_data.read_data_sets(\".\", one_hot=True)\n",
    "model = Model(mnist)\n",
    "model.create_placeholders()\n",
    "model.create_embeddings()\n",
    "model.create_loss()\n",
    "model.create_optimizer()\n",
    "model.train()\n",
    "model.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
