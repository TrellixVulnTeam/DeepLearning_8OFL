{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "From the stats framework. There are several derivations of linear regression. We just present one to for reference. \n",
    "\n",
    "\n",
    "pos_examples1 = np.array([[0.871429,0.624585,1],[ -0.020000,-0.923588,1],\n",
    "                         [0.362857,-0.318937,1],[0.888571,-0.870432,1]])\n",
    "\n",
    "neg_examples1 = np.array([[-0.80857,0.83721,1],[0.35714,0.85050,1],\n",
    "                         [-0.75143,-0.73090,1],[-0.30000,0.12625,1]\n",
    "                        ])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The simplest neuron is a linear neuron used for performing linear separation. \n",
    "\n",
    "\n",
    "<img src=\"linearnn1.png\">\n",
    "\n",
    "We define the operation of the neuron as multiplying the input with a weight: $w_i$. \n",
    "Define the loas function as the expected output minus the actual squared. $loss = \\frac{1}{2}\\sum\\limits_{i=1}^n(t_n-y_n)^2$. \n",
    "We are going to take the sum of all the errors for all the sample points and update the weights. This is batch training. \n",
    "If we process 1 sample at at time this is online training. To find the values of weights we use gradient descent. \n",
    "\n",
    "\n",
    "We want to minimize loss as a function of the weights. Using the chain rule we can expand the partial deriviative WRT w\n",
    "as follows: \n",
    "\n",
    "$\\frac{\\partial loss}{\\partial w_i} = \\frac{\\partial y_i}{\\partial w_i} \\cdot \\frac{\\partial loss}{\\partial y_i}$\n",
    "\n",
    "$\\frac{\\partial loss}{\\partial w_i} = \\frac{1}{2} \\cdot (2)(t_i-y_i)(-1) = -(t_i-y_i)$\n",
    "\n",
    "$y_i = \\sum\\limits_{i=1}^N w_ix_i$ so $\\frac{\\partial y_i}{\\partial w_i} = \\sum\\limits_{i=1}^N x_i$\n",
    "\n",
    "$w_{i+1} = w_i+\\delta \\cdot w_i$\n",
    "\n",
    "$\\delta w_i = \\epsilon \\cdot \\frac{\\partial loss}{\\partial w_i} $\n",
    "where $\\epsilon$ is the learning rate\n",
    "\n",
    "$\\delta w_i = \\epsilon \\cdot \\sum\\limits_{i=1}^N x_i(t_i-y_i)$\n",
    "\n",
    "Set the initial weights to a random value with variance=1 and compute the delta w for all the training samples and\n",
    "keep iterating until the weight values converge. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "w=0\n",
    "\n",
    "\n",
    "def loss(y,t):\n",
    "    \"\"\"\n",
    "    input y:output of neuron \n",
    "    input t: training case output of neuron \"\"\"\n",
    "    return .5 * np.sum(np.square(np.subtract(t-y)))\n",
    "\n",
    "\n",
    "\n",
    "def gradient(w,x,t):\n",
    "    #do we need transpose for other dims besides 1? \n",
    "    y = np.dot(x,w)\n",
    "    return -np.subtract(t,y)\n",
    "\n",
    "def update_weight(w,x,t,learning_rate=.001):\n",
    "    deltaw = learning_rate * gradient(w,x,t)\n",
    "    print(\"deltaw\",deltaw)\n",
    "    return dw\n",
    "    \n",
    "\n",
    "for i in range(10):\n",
    "    dw = update_weight(w,x,t)\n",
    "    w = w+dw\n",
    "    print (w)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Logistic Regression for classification. Simplest case for 2 classes. \n",
    "\n",
    "\n",
    "\n",
    "Logistic Neuron\n",
    "$y = \\frac{1}{1+exp(-z)}$\n",
    "\n",
    "\n",
    "$\\frac{dy}{dz}=y(1-y)$\n",
    "    \n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "softmax classification and cross entropy\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RELU activation function\n",
    "\n",
    "$ f(x)=max(0,x)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    \"\"\"if x>0 1, else 0.\"\"\"\n",
    "    return np.maximum(0,x)\n",
    "\n",
    "def derivative_relu(x):\n",
    "    if x<=0:\n",
    "        return 0\n",
    "    return 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SELU activation function\n",
    "\n",
    "$\n",
    "  selu(x)=\\lambda \\begin{cases}\n",
    "               x \\;for \\;x>0\\\\\n",
    "               \\alpha e^x-a \\;for \\;x<0\\\\\n",
    "            \\end{cases}\n",
    "$\n",
    "\n",
    "\n",
    "\n",
    "$\n",
    "  \\frac {\\partial selu(x)}{\\partial x}=\\lambda \\begin{cases}\n",
    "               1 \\;for \\;x>0 \\\\\n",
    "               \\alpha e^x \\;for\\; x<0\\\\\n",
    "            \\end{cases}\n",
    "$\n",
    "\n",
    "with Î»=1.0507,a=1.6733\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def selu(lambda,x):\n",
    "    \"\"\"if x>0 1, else 0.\"\"\"\n",
    "    return np.maximum(0,x)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tf35]",
   "language": "python",
   "name": "conda-env-tf35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
