{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<h6>Embeddings</h6>\n",
    "<p>Source: udacity lecture on embedings</p>\n",
    "The insipiraion for embeddings comes from word vocabularies which can be 50k words. If\n",
    "we represent each word as a one hot encoded vector then this is not practical. The multiplication of\n",
    "the input word which is a one hot vector with 49999 0s and one 1 with a weight matrix is not efficient. \n",
    "The smarter implementation is to not do the multiplication and to return the row of the weight matrix corresponding\n",
    "to the one in the one hot vector. \n",
    "<img src=\"embedding1.png\">\n",
    "A second improvement is to replace the weight matrix with a lookup table and to assign each word a row in the lookup\n",
    "table which contains the row of the weight matrix. \n",
    "<img src=\"embedding2.png\">\n",
    "We can also\n",
    "apply this scenario to feature vectors or tables with a large number of columns such as the Rossman example. \n",
    "\n",
    "<p>Word2Vec example for embeddings. </p>\n",
    "<><>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
