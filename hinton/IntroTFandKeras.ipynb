{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Intro to TF</h4>\n",
    "<p>Document how the input stages using tf.nn.conv2d have to match the dimensions using filters\n",
    "and strides. Google Vincent does a good job. Document this here. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MNIST TF not CNN\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "y = tf.nn.softmax(tf.matmul(x, W) + b)\n",
    "y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "for i in range(1000):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dnn mnist\n",
    "\n",
    "import time\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "def timing(f):\n",
    "    def wrap(*args):\n",
    "        time1 = time.time()\n",
    "        ret = f(*args)\n",
    "        time2 = time.time()\n",
    "        print ('%s function took %0.3f ms' % (f.func_name, (time2-time1)*1000.0))\n",
    "        return ret\n",
    "    return wrap\n",
    "\n",
    "\n",
    "\n",
    "time1 = time.time()\n",
    "W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "b_conv1 = bias_variable([32])\n",
    "x_image = tf.reshape(x, [-1,28,28,1])\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "W_fc2 = weight_variable([1024, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "\n",
    "y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_,logits=y_conv ))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "#sess = tf.Session() add session=sess\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for i in range(4000):\n",
    "    batch = mnist.train.next_batch(50)\n",
    "    if i%100 == 0:\n",
    "        train_accuracy = accuracy.eval(session=sess,feed_dict={x:batch[0], y_: batch[1], keep_prob: 1.0})\n",
    "        print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "    train_step.run(session=sess,feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n",
    "\n",
    "print(\"test accuracy %g\"%accuracy.eval(session=sess,feed_dict={\n",
    "    x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))\n",
    "\n",
    "time2 = time.time()\n",
    "print ('minutes:' , (time2-time1)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Activation,Dense,Dropout\n",
    "from keras.models import Sequential\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train = X_train.reshape(60000, 784)\n",
    "X_test = X_test.reshape(10000, 784)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(\"Training matrix shape\", X_train.shape)\n",
    "print(\"Testing matrix shape\", X_test.shape)\n",
    " \n",
    "# 6. Preprocess class labels\n",
    "Y_train = np_utils.to_categorical(y_train, 10)\n",
    "Y_test = np_utils.to_categorical(y_test, 10)\n",
    " \n",
    "print(X_train.shape, y_train.shape, X_test.shape,y_test.shape,Y_train.shape,Y_test.shape)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(784,)))\n",
    "model.add(Activation('relu')) # An \"activation\" is just a non-linear function applied to the output\n",
    "                              # of the layer above. Here, with a \"rectified linear unit\",\n",
    "                              # we clamp all values below 0 to 0.\n",
    "                           \n",
    "model.add(Dropout(0.2))   # Dropout helps protect the model from memorizing or \"overfitting\" the training data\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax')) # This special \"softmax\" activation among other things,\n",
    "                                 # ensures the output is a valid probaility distribution, that is\n",
    "                                 # that its values are all non-negative and sum to 1.\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.fit(X_train, Y_train,\n",
    "          batch_size=128, nb_epoch=10,\n",
    "          show_accuracy=True, verbose=1,\n",
    "          validation_data=(X_test, Y_test))\n",
    "\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "\n",
    "\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    " \n",
    "# evaluate loaded model on test data\n",
    "loaded_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "score = loaded_model.evaluate(X_test, Y_test, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MNIST data...\n",
      "train_x shape: (60000, 1, 28, 28) train_y shape: (60000, 10)\n",
      "validation_x shape: (10000, 1, 28, 28) validation_y shape (10000, 10)\n",
      "Training matrix shape (60000, 1, 28, 28) (60000, 10) (10000, 1, 28, 28) (10000, 10)\n",
      "(60000, 1, 28, 28) (60000, 10) (10000, 1, 28, 28) (10000, 10) (600000, 10) (100000, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dc/anaconda/envs/tf35/lib/python3.5/site-packages/keras/models.py:844: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 10s - loss: 0.5819 - acc: 0.8262 - val_loss: 0.1897 - val_acc: 0.9440\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 10s - loss: 0.1581 - acc: 0.9539 - val_loss: 0.1155 - val_acc: 0.9645\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 10s - loss: 0.1148 - acc: 0.9653 - val_loss: 0.1039 - val_acc: 0.9674\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 10s - loss: 0.0919 - acc: 0.9722 - val_loss: 0.0789 - val_acc: 0.9746\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 11s - loss: 0.0779 - acc: 0.9759 - val_loss: 0.0741 - val_acc: 0.9765\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 11s - loss: 0.0684 - acc: 0.9788 - val_loss: 0.0667 - val_acc: 0.9775\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 11s - loss: 0.0605 - acc: 0.9810 - val_loss: 0.0567 - val_acc: 0.9818\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 11s - loss: 0.0556 - acc: 0.9827 - val_loss: 0.0568 - val_acc: 0.9814\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 10s - loss: 0.0509 - acc: 0.9840 - val_loss: 0.0561 - val_acc: 0.9813\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 11s - loss: 0.0463 - acc: 0.9860 - val_loss: 0.0548 - val_acc: 0.9824\n",
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "#CNN KERAS\n",
    "import keras\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Activation,Dense,Dropout,Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.models import Sequential\n",
    "\n",
    "num_class = 10  # number of class\n",
    "\n",
    "input_shape = (1, 28, 28)\n",
    "def preprocess_input(x):\n",
    "    return x.reshape((-1, ) + input_shape) / 255.\n",
    "\n",
    "\n",
    "def preprocess_output(y):\n",
    "    return np_utils.to_categorical(y)\n",
    "\n",
    "(train_x, train_y), (validation_x, validation_y) = mnist.load_data()\n",
    "train_x, validation_x = map(preprocess_input, [train_x, validation_x])\n",
    "train_y, validation_y = map(preprocess_output, [train_y, validation_y])\n",
    "print('Loading MNIST data...')\n",
    "print('train_x shape:', train_x.shape, 'train_y shape:', train_y.shape)\n",
    "print('validation_x shape:', validation_x.shape,\n",
    "      'validation_y shape', validation_y.shape)\n",
    "\n",
    "print(\"Training matrix shape\", train_x.shape,train_y.shape, validation_x.shape, validation_y.shape)\n",
    "\n",
    "Y_train = np_utils.to_categorical(train_y, 10)\n",
    "Y_test = np_utils.to_categorical(validation_y, 10)\n",
    " \n",
    "print(train_x.shape, train_y.shape,validation_x.shape,validation_y.shape,Y_train.shape, Y_test.shape)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32,3, input_shape= (1,28,28),padding='same', name='conv1'))\n",
    "model.add(MaxPooling2D(2, name='pool1', data_format=\"channels_first\"))\n",
    "model.add(Conv2D(64, 3, padding='same', name='conv2'))\n",
    "model.add(MaxPooling2D(2, name='pool2', data_format=\"channels_first\"))\n",
    "model.add(Flatten(name='flatten'))\n",
    "model.add(Dense(64, activation='relu', name='fc1'))\n",
    "model.add(Dense(num_class, activation='softmax', name='fc2'))\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "            optimizer=SGD(lr=0.01, momentum=0.9),\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_x, train_y,\n",
    "          batch_size=128, nb_epoch=10,verbose=1,\n",
    "          validation_data=(validation_x, validation_y))\n",
    "\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.6\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
