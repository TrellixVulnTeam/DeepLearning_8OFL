{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<li>Run in tf35.</li> \n",
    "<li>If you run in python 27 you will see artifacts like vertical lines at the end of Latek equations</li>\n",
    "<li>The contribution of this work is</li>\n",
    "<li>1) to put into 1 place the derivation of backpropagation for linear and logistic neurons with 0 and 1 hidden layer</li>\n",
    "<li>2) add torch verificaiton of backprop which is not documented on internet. </li>\n",
    "<li>3) </li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Backpropagation in supervised neural nets is used to set the weights given a set of training data. The most common use\n",
    "case for NN are in using feedforward networks with hidden layers to model multiple functions. Hidden layers can have a \n",
    "width or number of neurons and depth. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"bp0.png\">\n",
    "\n",
    "\n",
    "Backpropagation is the chain rule applied to the above graph. \n",
    "$y=g(f(x))$\n",
    "\n",
    "$\\frac{dy}{dx} = \\frac{dg}{dx} = \\frac{dg}{df} \\; \\frac{df}{dx}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h4>Linear Weight Backpropagation </h4>\n",
    "<p>Note: the definition of a neuron requires a set of weights and an activation function. To keep things\n",
    "    as simple as possible there is no activation\n",
    "    function in this example. This is backpropagation applied to a linear multiplicatoin </p>\n",
    "\n",
    "\n",
    "<img src=\"bplin.png\">\n",
    "\n",
    "The linear operation $w_ix_i+b$ converts the b in $z=\\sum\\limits_{i=n}^Nw_ix_i+b$ into a weight term by adding a 1 into the \n",
    "input of the neuron. This converts b into $w_0$\n",
    "\n",
    "The notation for a linear neuron can have\n",
    "subscripts starting from 1 or 0 and may/maynot explicitly\n",
    "mention the existence of a b term: $z=\\sum\\limits_{i=1}^N w_ix_i + b = \\sum\\limits_{i=0}^N w_ix_i$\n",
    "\n",
    "To find the weights of the linear neuron given a set of training/test data we apply the delta rule(Hinton Coursera)\n",
    "which is a 2 step process of 1) defining a square error loss and 2) taking a derivative and using the delta rule to \n",
    "determine an iterative procedure as a substitute for an exact analytical solution. The delta rule shows how much\n",
    "to change the weights as we apply each training sample. We simplify this and call the operation without an \n",
    "activation function a linear operation. \n",
    "\n",
    "\n",
    "<li>define the square error loss</li>\n",
    "Note: we do this derivation for squared error loss but many times we use a different loss function. \n",
    "Different loss functions would require modification in this step. \n",
    "$E = \\frac{1}{2}\\sum\\limits_{n in training}(t_n-y_n)^2$\n",
    "<p>verbatim Hinton Courser lecture3.pdf</p>\n",
    "or the total error over the entire training set of size N is:\n",
    "$E_{total} = \\frac{1}{2}\\sum\\limits_{i=0}^N (t_i -y_i)^2$\n",
    "<p></p>\n",
    "We can also define the error for a single training sample:\n",
    "$E_i = \\frac{1}{2} (t_i-y_i)^2$\n",
    "<p></p>\n",
    "<li>Take the derivative of the total sum error loss over the entire training set</li>\n",
    "We can take the derivative of either the total loss or each step. It does not matter as long\n",
    "as we are consistent with the notation and clear which option we take. \n",
    "\n",
    "\n",
    "<li>Chain rule (copied verbatim from Hinton Coursera lecture3.pdf)</li>\n",
    "<p><div> </div></p>\n",
    "$\\frac{\\partial E}{\\partial w_i}=\\sum\\limits_{n} \\frac{\\partial E}{\\partial y^n} \\cdot \\frac{\\partial y^n}{\\partial w_i}$\n",
    "<p><div> </div></p>\n",
    "\n",
    "<p></p>\n",
    "We can calculate the 2 above parts separately \n",
    "<p></p>\n",
    "\n",
    "$\\frac{\\partial E}{\\partial y_n}=\\frac{1}{2}(2)(t_n-y_n)(-1) = -(t_n-y_n)$\n",
    "<p></p>\n",
    "<p></p>\n",
    "The above is Hinton notation. Many use the opposite with the training samples $t_n$ reversed from the\n",
    "training set labels $y_n$. \n",
    "Will not matter can adjust when defining delta rule\n",
    "<p></p>\n",
    "$\\frac{\\partial E}{\\partial y_n} = -(t_n-y_n)$\n",
    "<p></p>\n",
    "$\\frac{\\partial y}{\\partial w_i} = x_i$\n",
    "<p></p>\n",
    "\n",
    "$\\frac{\\partial E}{\\partial w_i}= -x_i(t_n-y_n)$\n",
    "<p></p>\n",
    "\n",
    "The delta rule takes the above and multiplies a learning rate, epsilon to the expression:\n",
    "    \n",
    "$\\delta w_i = w_i + \\epsilon x_i(t_n-y_n)$    \n",
    "\n",
    "<p></p>   \n",
    "    \n",
    "    \n",
    "The above example was shown without an activation function. To be a neuron we need\n",
    "a binary threshold activation function or something which mimics a binary threshold function like a \n",
    "logistic. The Udactiy hw assignment adds a linear activation function $y=f(x)$ to do regression. Technically\n",
    "this is not a neuron because a neuron is modeled after the brain neuron which \n",
    "requires a binary threshold operator. A linear function $y=f(x)$ is not a threshold operator. \n",
    "Adding a linear activation function for a single weight and input look like this: \n",
    " \n",
    "<img src=\"bplin1a.png\" >    \n",
    "\n",
    "<p>This is not the Udacity hw1 neuron. This is a simplified example with multiple inputs and input weights, \n",
    "1 hidden node, oue hidden node output and one output node y. This reduces the summations to \n",
    "only the ones at the input node to keep things simple. </p>\n",
    "\n",
    "This adds an additional set of weights $w_h$\n",
    "<p></p>\n",
    "$E = \\frac{1}{2}(t_n-y_n)^2$\n",
    "\n",
    "<p></p>\n",
    "$y=f(x)$\n",
    "<p></p>\n",
    "This is crappy Notation practice from Udacity. They should leave the y=f(x) equation out as it is confusing and \n",
    "obvious. It does not need to be written down because it is confusing why it is a constructive statement \n",
    "and is not needed. It is not part of the equations used in the chain rule. \n",
    "<p></p>\n",
    "$y=x$\n",
    "<p></p>\n",
    "$x=h(z) \\cdot w_{h}$\n",
    "<p></p>\n",
    "$z=\\sum\\limits_{i=0}^N w_{i} x_{i}$\n",
    "<p></p>\n",
    "$\\frac{\\partial E}{\\partial w_h} = \\frac{\\partial E}{\\partial y} \\cdot \\frac{\\partial y}{\\partial x} \\cdot \\frac{\\partial x}{\\partial w_h}=-(t_n-y_n)(1)h(z) $\n",
    "<p></p>\n",
    "$\\frac{\\partial E}{\\partial w_i} = \\frac{\\partial E}{\\partial y} \\cdot \\frac{\\partial y}{\\partial x} \\cdot \\frac{\\partial x}{\\partial h} \\cdot \\frac{\\partial h}{\\partial w_i}$\n",
    "<p></p>\n",
    "$\\frac{\\partial y}{\\partial x } = 1 $\n",
    "<p></p>\n",
    "$\\frac{\\partial x}{\\partial h} = w_h $\n",
    "<p></p>\n",
    "$\\frac{\\partial h}{\\partial w_i}= x_i$\n",
    "<p></p>\n",
    "$\\frac{\\partial E}{\\partial w_{i}} = -(t_n-y_n)(1) \\cdot w_h \\cdot x_i$\n",
    "<p></p>\n",
    "\n",
    "<h4>Feedforward Network with 1 hidden layer and linear activation function</h4>\n",
    "Adding multiple neurons together adds a summation operation in the path between the hidden layers to \n",
    "the activation function. This expands on the above example by adding a new set of summation operators. \n",
    "\n",
    "\n",
    "\n",
    "<img src=\"bplinb.png\">\n",
    "<p></p>\n",
    "\n",
    "To generalize for the case where there are n inputs, m hidden nodes and p outputs, we add summations to include \n",
    "the general case for a fully connected hidden layer with 2 linear neurons. \n",
    "<p></p>\n",
    "$h(z) = \\sum\\limits_{m=0}^M w_{im} x_{im}$\n",
    "<p></p>\n",
    "$y = x$\n",
    "<p></p>\n",
    "$x = \\sum\\limits_{n=0}^N w_{on} h_{on}$\n",
    "<p></p>\n",
    "\n",
    "$E = \\frac{1}{2}(t_n-y_n)^2$\n",
    "<p></p>\n",
    "$\\frac{\\partial E}{\\partial w_{oh}} = \\frac{\\partial E}{\\partial y} \\frac{\\partial y}{\\partial x} \\frac{\\partial x}{\\partial w_{on}}$\n",
    "<p></p>\n",
    "$\\frac{\\partial E}{\\partial y} = -\\sum\\limits_{m=0}^M(t_m-y_m) $\n",
    "<p></p>\n",
    "$\\frac{\\partial y}{\\partial x} =  1$\n",
    "<p></p>\n",
    "$\\frac{\\partial x}{\\partial w_{on}} =\\sum\\limits_{n=0}^N h_{on} $\n",
    "<p></p>\n",
    "<p></p>\n",
    "$\\frac{\\partial E}{\\partial w_{im}} = \\frac{\\partial E}{\\partial y} \\frac{\\partial y}{\\partial x} \\frac{\\partial x}{\\partial h} \\frac{\\partial h}{\\partial w_{im}}$\n",
    "<p></p>\n",
    "\n",
    "$\\frac{dx}{dh}=w_{on}$\n",
    "<p></p>\n",
    "$\\frac{\\partial h}{\\partial w_{im}} = x_{im}$\n",
    "<p></p>\n",
    "$\\frac{\\partial E}{\\partial w_{im}} = -(t_n-y_n) \\cdot 1 \\cdot w_{on} \\cdot x_{im}$\n",
    "<p></p>\n",
    "Here is a numerical example of backprop: \n",
    "<p></p>    \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "Here is an example of the delta rule in action on a XOR gate:\n",
    "<li>add link</li>\n",
    "    \n",
    "    \n",
    "Instead of summing over all the training samples we could have just used one. Here is an example of the XOR\n",
    "case. \n",
    "<li>add link</il>\n",
    "\n",
    "Some links similar to the udacity hw1:\n",
    "    http://machinelearningmastery.com/implement-backpropagation-algorithm-scratch-python/\n",
    "\n",
    "This process of using the chain rule to calculate the $\\delta w_i$ generalizes to more layers. \n",
    "There are 2 steps used to calculate the weights by observing onlyt the output of a NN. \n",
    "1) Use the chain rule/backpropagation for each layer.  \n",
    "2) Iterating to a approximate solution using some form of SGD. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Logistic function</h4>\n",
    "<p>A logistic neuron requires a set of weights and an Activation function at the output of y. Without\n",
    "an activation function this is not a neuron. The AF is not in the figure hence this is not a neuron\n",
    "and we first do back propagation with the logistic function first. Backpropagation refers to the \n",
    "method of determining the weights for a given training sample using gradient descent to compute the \n",
    "least loss or error with a set of weights per training sample. </p>\n",
    "<img src=\"bplog.png\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "From the Hinton Notation the output of the logistic is y: $\\sigma(z) = \\frac{1}{1+exp(-z)}$ and\n",
    "the input to the logistic is z. For the diagram above we use $y = \\frac{1}{1+exp(-z)}$ The input to the logistic is a summation of 2 terms, the bias b and\n",
    "the product of the input with the weight. \n",
    "$z = \\sum\\limits_{i=1}^{N}x_iw_i+b_i$ \n",
    "<p></p>\n",
    "From the Ng Notation the output of the logistic neuron is y and the bias terms are \n",
    "combined with the weight terms in $\\theta$ : $y = \\frac{1}{1+exp(-\\theta_i \\cdot x_i)}$\n",
    "<p></p>\n",
    "We will use the Hinton notation and start the index term from 0 to avoid writing a separate bias term. \n",
    "    \n",
    "For the a single logistic function with 1 weight, multiple inputs and 1 output:\n",
    "We want to calculate the derivative of the output y as a function of the weights $w_i.$ We use the derivative when \n",
    "set to 0 to determine the amount to change the state of the weights as we iterate using the training sets towards a \n",
    "converged value. \n",
    "\n",
    "<p></p>    \n",
    "$\\frac{\\partial y}{\\partial w}=\\frac{\\partial y}{\\partial z} \\cdot \\frac{\\partial z}{\\partial w}$    \n",
    "<p></p>\n",
    "    \n",
    "The derivative of the logistic WRT z. \n",
    "$\\frac{dy}{dz} = y(1-y)$\n",
    "<p></p>\n",
    "$\\frac{dz}{dw}=x_i$\n",
    "<p></p>\n",
    "$\\frac{\\partial y}{\\partial w}=x_i \\cdot y(1-y)$\n",
    "\n",
    "<p></p>\n",
    "To derive the delta rule, add a multiply by the learning rate and add it to the existing weights\n",
    "\n",
    "$\\delta w=w_i + \\epsilon \\cdot x_i \\cdot y(1-y)$\n",
    "\n",
    "<p></p>\n",
    "\n",
    "To mirror the linear derivation we add an activation function to the output. In this case we add a logistic\n",
    "activation function as seen below. \n",
    "\n",
    "<img src=\"log2.png\">\n",
    "<p></p>\n",
    "Add a hidden layer h(z) where $h(z) = \\frac{1}{1+exp(-z)}$ There are 2 logistic functions, the other one is at \n",
    "the output of the activation function, $y(h(z)) = \\frac{1}{1+exp(-h(z)}$\n",
    "<p></p>    \n",
    "$z = b + \\sum\\limits_{i=1}^N x_iw_i = \\sum\\limits_{i=0}^N x_iw_i$\n",
    "<p></p>\n",
    "\n",
    "Most use the notation start the index from 0 and do not bother to write out the b term. This is consistent with \n",
    "tensor notation used in statistical physics. \n",
    "There are 2 weights we need the update rule for, $w_h$ and $w_i$\n",
    "$\\frac{\\partial E}{\\partial w_h}$ \n",
    "<p></p>\n",
    "$\\frac{\\partial E}{\\partial w_i}$ \n",
    "<p></p>                                                               \n",
    "Assign a temporary variable to the output of the hidden layer which is also teh input to the activation function, variable $u$\n",
    "<p></p>\n",
    "$u = b + \\sum\\limits_{i=1}^M h_iw_h = \\sum\\limits_{i=0}^M x_iw_h$                                                                 \n",
    "where M is the number of hidden nodes.                                                                \n",
    "<p></p>                                                                 \n",
    "Using the chain rule, we expand the above to: \n",
    "<p></p>\n",
    "$\\frac{\\partial E}{\\partial w_h}=\\frac{\\partial y_i}{\\partial u_i} \\frac{\\partial u_i}{\\partial w_h}$    \n",
    "<p></p>\n",
    "\n",
    "$\\frac{\\partial u}{\\partial w_h}= \\sum\\limits_{i=0}^M h_i$\n",
    "<p></p>\n",
    "$\\frac{\\partial E}{\\partial y_n} = -(t_n-y_n)$ where n is over the training set data. \n",
    "<p></p>\n",
    "$\\frac{\\partial E}{\\partial w_h}= -(t_n-y_n) \\cdot h_i $\n",
    "<p></p>\n",
    "$\\frac{\\partial E}{\\partial w_i}= \\frac{\\partial E}{\\partial y} \n",
    "\\frac{\\partial y}{\\partial u} \\frac{\\partial u}{\\partial h} \n",
    "\\frac{\\partial h}{\\partial z} \\frac{\\partial z}{\\partial w_i}$\n",
    "<p></p>\n",
    "\n",
    "<p></p>\n",
    "$\\frac{\\partial E}{\\partial y}=-(t_n-y_n)$\n",
    "<p></p>\n",
    "$\\frac{\\partial y}{\\partial u}= y(1-y)$\n",
    "<p></p>\n",
    "\n",
    "$\\frac{\\partial u}{\\partial h}=w_h$\n",
    "<p></p>\n",
    "Add a temporary variable z, $z=\\sum\\limits_{i=0}^I w_i x_i$ \n",
    "<p></p>$h(z) = \\frac{1}{1+exp(-z)}$ and \n",
    "<p></p>\n",
    "$\\frac{\\partial h}{\\partial z} = h(1-h)$\n",
    "<p></p>\n",
    "$\\frac{\\partial z}{\\partial w_i}=x_i$\n",
    "<p></p>\n",
    "                                                                 \n",
    "\n",
    "$weight update=\\epsilon \\cdot -(t_n-y_n)\\cdot x_i \\cdot y_i(1-y_i) \\cdot h_i(1-h_i) \\cdot w_h $\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h4>Multiclass classification Cross Entropy and Softmax</h4>\n",
    "\n",
    "Source: https://www.cs.toronto.edu/~urtasun/courses/CSC411/10_nn1.pdf\n",
    "Neither Hinton Coursera or Udacity DL MOOC treats this material that follows in detail. \n",
    "Logistic loss not good with squared error loss because: \n",
    "1) small gradients near edges, will never converge. The S shape of the logistic shows near 1 the slope flattens out\n",
    "dramatically. The gradients are very small when close to 1. \n",
    "2) the gradients zig zag between + and - values. \n",
    "\n",
    "Insteand of squared loss use Cross Entropy loss or multinomial logistic loss and instead of using a logistic\n",
    "as the activation function use a softmax as the activation function. \n",
    "<p></p>\n",
    "For a binary classification we can see from logistic regression binary CE works better than squared error:    \n",
    "$ E = binary CE = -\\frac{1}{n} \\sum\\limits_{i=0}^N ylog(a) + (1-y)log(1-a) $\n",
    "where \n",
    "<p></p>\n",
    "$a = \\sigma(z)$\n",
    "<p></p>\n",
    "$\\sigma(z) = \\frac{1}{(1+exp(-z))}$\n",
    "<p></p>\n",
    "and \n",
    "<p></p>\n",
    "$z = \\sum\\limits_{i=0}^N w_i x_i + b$\n",
    "<p></p>\n",
    "$\\frac{\\partial CE}{\\partial w_i} = \\frac{-1}{n} \\sum\\limits_{i=1}^{N} \\frac{\\partial(yln(a))}{\\partial a} + \\frac{\\partial(1-y)ln(1-a)}{\\partial a} $\n",
    "<p></p>\n",
    "The derivative of a log $\\frac{d(log_b(x))}{dx} = \\frac{1}{xlog(b)}$\n",
    "\n",
    "<p></p>\n",
    "$\\frac{\\partial yln(a)}{\\partial s} = y \\cdot \\frac{1}{aln(e)}=\\frac{y}{a} = \\frac{y}{\\sigma(z)}$\n",
    "<p></p>\n",
    "$\\frac{\\partial((1-y)ln(1-a))}{\\partial a}=(1-y) \\frac{1}{(1-a)ln(e)} = \\frac{1-y}{1-a} = \\frac{1-y}{1-\\sigma(z)}$\n",
    "<p></p>\n",
    "$\\frac{\\partial CE}{\\partial w_i} = \\frac{-1}{n} \\sum\\limits_{n=1}^N (\\frac{y}{\\sigma(z)} - \\frac{1-y}{1-\\sigma(z)})\\frac{\\partial a}{\\partial w_{ij}}$\n",
    "<p></p>\n",
    "$\\frac{\\partial a}{\\partial w_i} = \\frac{\\partial \\sigma(z)}{\\partial z} \\cdot \\frac{\\partial z}{\\partial w_i}$\n",
    "<p></p>\n",
    "$\\frac{\\partial z}{\\partial w_i} = x_i$\n",
    "<p></p>\n",
    "$\\frac{\\partial a}{\\partial z} = \\frac{ \\partial \\sigma(z)}{\\partial z} = \\sigma(z)(1-\\sigma(z)) $\n",
    "<p></p>\n",
    "$\\frac{\\partial CE}{\\partial w_i} =\\frac{-1}{n} \\sum\\limits_{n=1}^N (\\frac{y}{\\sigma(z)} - \\frac{1-y}{1-\\sigma(z)}) \\sigma(z)(1-\\sigma(z))x_i$\n",
    "<p></p>\n",
    "$\\frac{\\partial CE}{\\partial w_i} =\\frac{y(1-\\sigma(z))-\\sigma(z)(1-y)}{\\sigma(z)(1-\\sigma(z))} \\cdot \\sigma(z) (1-\\sigma(z))x_i$\n",
    "<p></p>\n",
    "$\\frac{\\partial CE}{\\partial w_i} = (y-y\\sigma(z) - \\sigma(z) +y\\sigma(z))x_i$\n",
    "\n",
    "<p></p>\n",
    "$\\frac{\\partial CE}{\\partial w_i} = \\sum \\limits_{i=1}^N (y-\\sigma(z))x_i$\n",
    "\n",
    "<p></p>\n",
    "\n",
    "For multiclass the CE becomes\n",
    "$CE = \\sum\\limits_{n=1}^{training} \\sum\\limits_{k=1}^{classes} t_k^{(n)} log(o_k^{(n)})$\n",
    "<p></p>\n",
    "\n",
    "<p></p>\n",
    "$CE$\n",
    "<p></p>\n",
    "$softmax = \\frac{}{}$\n",
    "<p></p>\n",
    "\n",
    "\n",
    "We also add anotehr modification to support multiple classes. Use the normalizaiton trick, divide output by sum of all inputs to get probability per class. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "<h4>Feedforward network with 1 linear hidden layer and Cross Entropy activation function</h4>\n",
    "A cross entropy loss is used when the data is mutually exclusive, the data can only belong to one of n possible\n",
    "classes. The UFDL tutorial from Ng is the only source which mentions when is it better to use multiple logistic\n",
    "classifiers vs. one with a CE loss. \n",
    "\n",
    "NOTE: ADD for CIFAR10 \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"bphidden.png\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "There are 2 sets of weights, the set of weights from the input to the hidden layer and the set of weights from \n",
    "the hidden layer to the output. \n",
    "\n",
    "The shape of the input weight mstrix is [number of inputs x number of hidden layers]\n",
    "The shape of the output hidden layer weight matrix is [number if hidden layers x number of outputs]\n",
    "If this is a classification problem the number of outputs is equal to the number of classes. \n",
    "\n",
    "The hidden layer activation function is a logistic type.\n",
    "\n",
    "The output layer activateion function is a Cross Entropy Activation function. \n",
    "\n",
    "\n",
    "CE loss: $$\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h4>BackPropagation Examples with weight sharing</h4>\n",
    "<img src=\"bp1.png\">\n",
    "<img src=\"bp1a.png\">\n",
    "<img src=\"bp2.png\">\n",
    "<img src=\"bp3.png\">\n",
    "<img src=\"bp4.png\">\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Vanishing gradient\n",
    "https://github.com/dmlc/mxnet-notebooks/blob/master/python/tutorials/understanding_vanish_gradient.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backpropatation exercise of unrolled RNN from Hinton:\n",
    "<img src=\"bprnn.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Find $\\frac{\\partial E}{\\partial z_1}$\n",
    "\n",
    "$\\frac{\\partial E}{\\partial z_1} = \\frac{\\partial E_0}{\\partial z_1} + \\frac{\\partial E_1}{\\partial z_1} + \\frac{\\partial E_2}{\\partial z_1}$\n",
    "partial E0 wrt Z1 is 0 bc it is behind in time to z1 so it should not contain any dependent terms on z1. \n",
    "\n",
    "\n",
    "<p></p>\n",
    "$\n",
    "\\frac{\\partial E_1}{\\partial z_1}=\n",
    "\\frac{\\partial E_1}{\\partial y_1}\\frac{\\partial y_1}{\\partial z_1}=\n",
    "\\frac{\\partial E_1}{\\partial y_1}\\frac{\\partial y_1}{\\partial h_1}\\frac{\\partial h_1}{\\partial z_1}\n",
    "$\n",
    "<p></p>\n",
    "$\\frac{\\partial E_1}{\\partial y_1}=-(t_1-y_1)=-(-0.1-.1)=-0.2$\n",
    "$\\frac{\\partial y_1}{\\partial h_1}=W_{hy}=0.25$\n",
    "$\\frac{\\partial h_1}{\\partial z_1}=h_1(1-h_1)=.4(1-0.4)=-.24$\n",
    "<p></p>\n",
    "$\\frac{\\partial E}{\\partial z_1}=-.2*.25*-.24=.012$                                      \n",
    "\n",
    "<p></p>\n",
    "$\n",
    "\\frac{\\partial E_2}{\\partial z_1}=\n",
    "\\frac{\\partial E_2}{\\partial y_2}\n",
    "\\frac{\\partial y_2}{\\partial h_2}\n",
    "\\frac{\\partial h_2}{\\partial z_2}\n",
    "\\frac{\\partial z_2}{\\partial h_1}\n",
    "\\frac{\\partial h_1}{\\partial z_1}\n",
    "$\n",
    "<p></p>\n",
    "$\\frac{\\partial E_2}{\\partial y_2}=-(t_2-y_2)=-(-0.2-.2)=.4$\n",
    "$\\frac{\\partial y_2}{\\partial h_2}=W_{hy}=.25$\n",
    "$\\frac{\\partial h_2}{\\partial z_2} = h_2(1-h_2)=.8*(1-.8)=-.16$\n",
    "$\\frac{\\partial z_2}{\\partial h_1}=W_{hh}=0.5$\n",
    "$\\frac{\\partial h_1}{\\partial z_1}=h_1(1-h_1)=.4(1-.4)=-.24$\n",
    "\n",
    "$\\frac{\\partial E_2}{\\partial z_1}=.4*.25*-.16*.5*-.24=.00192$\n",
    "<p></p>\n",
    "$.012+.00192=.01392$ \n",
    "<p></p>\n",
    "Given \n",
    "<p></p>\n",
    "$t_0,t_1,t_2$=[0.1,-0.1,-0.2]\n",
    "$x_0,x_1,x_2$=[18,9,-8]\n",
    "$h_0,h_1,h_2$=[.2,.4,.8]\n",
    "$y_0,y_1,y_2$=[.05,.1,.2]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#use to calcluate -.355 value. Too many mistakes by hand. \n",
    "def calculate(x0,x1,x2,Wxh,Xhh,Why,hbias,ybias):\n",
    "    z0 = Wxh*x0+hbias\n",
    "    h0 = 1/(1+np.exp(-z0))\n",
    "    y0=Why*h0+ybias\n",
    "    z1=Wxh*x1+Whh*h0+hbias\n",
    "    h1=1/(1+np.exp(-z1))\n",
    "    y1 = Why*h1+ybias\n",
    "    z2=Wxh*x2+Whh*h1+hbias\n",
    "    h2=1/(1+np.exp(-z2))\n",
    "    y2 = Why*h2+ybias\n",
    "    print(z0,z1,z2,h0,h1,h2,y0,y1,y2)\n",
    "    \n",
    "#calculate(18,9,-8,) \n",
    "calculate()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.0493585136504298e-162"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#vanishing gradient RNN example. \n",
    "import numpy as np\n",
    "def cal(x0,x1,x2,x3,Wxh,Whh,Why,hbias,ybias):\n",
    "    z0 = Wxh*x0+hbias\n",
    "    h0 = 1/(1+np.exp(-z0))\n",
    "    y0=Why*h0+ybias\n",
    "    z1=Wxh*x1+Whh*h0+hbias\n",
    "    h1=1/(1+np.exp(-z1))\n",
    "    y1 = Why*h1+ybias\n",
    "    z2=Wxh*x2+Whh*h1+hbias\n",
    "    h2=1/(1+np.exp(-z2))\n",
    "    y2 = Why*h2+ybias\n",
    "    z3=Wxh*x3+Whh*h2+hbias\n",
    "    h3=1/(1+np.exp(-z3))\n",
    "    y3 = Why*h3+ybias\n",
    "    return (z0,z1,z2,z3,h0,h1,h2,h3,y0,y1,y2,y3) \n",
    "\n",
    "results = cal(1,0,0,0,1,-2,1,0,0)\n",
    "\n",
    "prod=1\n",
    "for x in results:\n",
    "    prod *=x*prod \n",
    "prod\n",
    "\n",
    "#can see this goes to 0 real fast. even before adding the other Whh, Wxh constants"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
