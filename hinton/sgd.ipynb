{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "For linear neuron we have a quadratic bowl. For a hidden layer neuron we have multiple bowls. \n",
    "add multi surface from vincent pascal\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"ngsgd1.png\">\n",
    "<img src=\"ngsgd2.png\">\n",
    "\n",
    "Source Andrew Ng lecture notes\n",
    "Gradient descent use all the training samples\n",
    "SGD/Online: use one training sample\n",
    "MiniBatch: use batch, typically 100, 10 or some number user picks. \n",
    "    Most common method, \n",
    "    1) the process of averaging the partial derivatives removes effect of bad data samples. \n",
    "    2) can parallelize using GPU by computing gradiesnts in parallel. Each matrix multiple/training case\n",
    "    can be parallelized both at training case level and row/column level. \n",
    "    Hinton: minibatches need to be balanced across classes. Example was if you have 10 classes, you want 100 each from each\n",
    "        of the 10 classses. One way to approximate this is to grab randomly from training set. If the weights are not random\n",
    "        you get oscillation in the weights which is not efficient/can get stuck\n",
    "\n",
    "One problem is the direction of the gradient or epsilon we are moving towards as we update teh weights\n",
    "1) are we converging or just bouncing around b/c the learning rate is too big\n",
    "<img src =\"sgd1.png\" >\n",
    "\n",
    "\n",
    "\n",
    "2) are we not converging bc the learning rate is too small and/or we are going in the wrong direction\n",
    "<img src =\"sgd2.png\" >\n",
    "Images from Hinton/Coursera lecture slides\n",
    "\n",
    "3) are we stuck in a local minima? Annealing, where we wait for a stable point, ie no change in error rate then add a perturbation\n",
    "or change the learning rate to make it leave the local minimia\n",
    "\n",
    "4) there are multiple answers. Which one is best? May not be able to replicate solution. \n",
    "<img src=\"multipleanswers.png\">\n",
    "\n",
    "5) stuck on plateau;the gradient becomes very small. This may look like a\n",
    "local minima but is not. Look at the weights for the Hidden units, are they all positive or all negative? \n",
    "All pegged in one area? \n",
    "\n",
    "common in multilayer perceptrons w/hidden units(pascal graph)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Data preprocessing for SGD: \n",
    "1) input mean at 0\n",
    "\n",
    "\n",
    "2) unit variance\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Nonlinear Conjugate Gradient Descent\n",
    "Momentum Optimizer\n",
    "AdaGradOptimizer\n",
    "AdaDeltaOptimizer\n",
    "AdamOptimizer\n",
    "FtriOptimizer v \n",
    "RMSPropOptimizer\n",
    "\n",
    "Image optimizer\n",
    "CTR prediction optimizer\n",
    "\n",
    "\n",
    "<img src=\"mom1.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tf35]",
   "language": "python",
   "name": "conda-env-tf35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
