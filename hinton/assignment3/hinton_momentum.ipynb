{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h6>Momentum vs. SGD for optimization</h6>\n",
    "<img src=\"mom1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h6>Numerical Stability for calculating exponent divided by sum of exponents</h6>\n",
    "https://stackoverflow.com/questions/10278004/how-to-order-this-computation-for-numerical-stability\n",
    "\n",
    "<p>Do not calculate np.exp(x) directly, subtract x_max first</p>\n",
    "<p>tmp = np.array([-10**10, 10**10])</p>\n",
    "<p>tmp_max = tmp.max()</p>\n",
    "<p>logD = np.log(np.sum(np.exp(tmp-tmp_max))) + tmp_max</p>\n",
    "<p>log_v = tmp - log_D</p>\n",
    "<p>v = np.exp(log_v)</p>\n",
    "<p>>>> v\n",
    "array([ 0.,  1.])</p>\n",
    "<p></p>\n",
    "\n",
    "$log\\sum_{k=1}^{training set}exp(tmp-tmp_{max}) + tmp_{max}$\n",
    "\n",
    "$=log\\sum_{k=1}^{training set} \\frac{exp(tmp)}{exp(tmp_{max})} + tmp_{max}$\n",
    "\n",
    "$= log\\frac{1}{exp(tmp_{max})} \\sum_{k=1}^{training set}exp(tmp) + tmp_{max}$ \n",
    "\n",
    "$= log[exp(-tmp_{max}) \\sum_{k=1}^{training set} exp(tmp)] + tmp_{max}$\n",
    "\n",
    "$= logexp(-tmp_{max}) + log\\sum_{k=1}^{training set}exp(tmp) + tmp_{max}$\n",
    "\n",
    "$= -tmp_{max} + log\\sum_{k=1}^{training set} exp(tmp) + tmp_{max}$\n",
    "\n",
    "<p>The above sould reduce to summation exp(tmp)</p>\n",
    "<p>Below for fraction of exp(tmp)/summation(exp(tmp))</p>\n",
    "<p>Combine log_v = tmp=log_D and v=exp(log_v)</p>\n",
    "$v=exp(tmp-logD) = \\frac{exp(tmp)}{log_D}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "Reference: http://cs231n.github.io/linear-classify/#softmax\n",
    "\n",
    "$\\frac{exp(x)}{\\sum_{k=1}^{K} exp(x)}$\n",
    "<p></p>\n",
    "\n",
    "$\\frac{C * exp(x)}{C*\\sum_{k=1}^{K}exp(x)}$\n",
    "\n",
    "$\\frac{exp(x + logC)}{\\sum_{k=1}^{K}exp(x+logC)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "tmp=[1,2,3,4,5,6,7,8]\n",
    "#tmp = np.array([-10**10, 10**10])\n",
    "   \n",
    "def foo(tmp):\n",
    "    tmp_max=np.max(tmp)\n",
    "    logD = np.log(np.sum(np.exp(tmp-tmp_max))) + tmp_max\n",
    "    log_v = tmp - logD\n",
    "    v = np.exp(log_v)\n",
    "    return v\n",
    "\n",
    "print(foo(tmp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "\n",
    "scores = [30000000000, 50000000000, 70000000000]\n",
    "print(softmax(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "print (softmax(tmp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "from scipy.io import loadmat\n",
    "x = loadmat('data.mat')\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(x['data']['training'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#print(x['data']['training'])\n",
    "train = x['data']['training']\n",
    "print(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(train.shape)                                                                                                                       \n",
    "print(train[0,0].shape)\n",
    "print(train[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(x['data']['validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(x['data']['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(x['data']['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "x['data']['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 9000)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.io import loadmat\n",
    "dt_train_target = loadmat('dt_train_target.mat')\n",
    "dt_train_input = loadmat('dt_train_input.mat')\n",
    "dt_test_input = loadmat('dt_test_input.mat')\n",
    "dt_test_target = loadmat('dt_test_target.mat')\n",
    "dt_valid_input = loadmat('dt_valid_input.mat')\n",
    "dt_valid_target = loadmat('dt_valid_target.mat')\n",
    "\n",
    "\n",
    "#methodology: first .keys() to print keys. Then pick one that is not header, globals, meta info, etc..\n",
    "#find right key name and find shape to verify with octave\n",
    "\n",
    "#dt_train_target.keys()\n",
    "#dt_train_target['dt_train_target'].shape #10x1000\n",
    "\n",
    "#dt_train_input.keys()\n",
    "#dt_train_input['dt_train_input'].shape #256x1000\n",
    "\n",
    "#dt_test_input.keys()\n",
    "#dt_test_input['dt_test_input'].shape #256x9000\n",
    "\n",
    "dt_test_target.keys()\n",
    "dt_test_target['dt_test_target'].shape #10x9000\n",
    "\n",
    "#dt_valid_input.keys()\n",
    "#dt_valid_input['dt_valid_input'].shape #256x1000\n",
    "\n",
    "#dt_valid_target.keys()\n",
    "#dt_valid_target['dt_valid_target'].shape #10x1000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='data.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "the dimensions of the names ,atcj training_inputs in octave and python = 256x1000\n",
    "training_targets=10x1000\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 2.0]\n",
      "[1, 2, 3]\n",
      "<__main__.data object at 0x10bca1780>\n",
      "[4, 5, 6]\n",
      "[1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class model:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.input_to_hid=None\n",
    "        self.hid_to_class=None\n",
    "    \n",
    "    def set_input_to_hid(self,a):\n",
    "        self.input_to_hid = a\n",
    "    def set_hid_to_class(self,a):\n",
    "        self.hid_to_class = a\n",
    "\n",
    "class data:\n",
    "    def __init__(self):\n",
    "        self.targets=None\n",
    "        self.inputs=None\n",
    "    def set_target(self,targets):\n",
    "        self.targets=targets\n",
    "    def set_inputs(self,inputs):\n",
    "        self.inputs = inputs\n",
    "\n",
    "class datas:\n",
    "    def __init__(self):\n",
    "        self.training=None\n",
    "        self.testing=None\n",
    "        self.validation=None\n",
    "    def set_training(self,data):\n",
    "        self.training=data\n",
    "    def set_testing(self,data):\n",
    "        self.testing=data\n",
    "    def set_validation(self,data):\n",
    "        self.validation=data\n",
    "        \n",
    "m = model()\n",
    "m.set_input_to_hid([1.0,2.0])\n",
    "print(m.input_to_hid)\n",
    "\n",
    "d = data()\n",
    "d.set_target([1,2,3])\n",
    "d.set_inputs([4,5,6])\n",
    "print(d.targets)\n",
    "\n",
    "datas_test = datas()\n",
    "datas_test.set_training(d)\n",
    "print(datas_test.training)\n",
    "print(datas_test.training.inputs)\n",
    "print(datas_test.training.targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def a3(wd_coefficient, n_hid,n_iters,learning_rate,momentum_multiplier,do_early_stopping, mini_batch_size):\n",
    "    model = initial_model(n_hid)\n",
    "    from_data_file = load('data.mat')\n",
    "    datas = from_data_file.data\n",
    "    # this is funny, variable outside \n",
    "    n_training_cases = dt_train_input['dt_train_input'].shape[1]\n",
    "    print('n_training_cases:{}',n_training_cases)\n",
    "    assert(n_training_cases==1000)\n",
    "    if n_iters !=0:\n",
    "        test_gradient(model,datas.training,wd_coefficient)\n",
    "    theta = model_to_theta(model)\n",
    "    momentum_speed = theta * 0\n",
    "    training_data_losses = []\n",
    "    validataion_data_losses = []\n",
    "    if do_early_stopping:\n",
    "        print('do_early_stopping true')\n",
    "        best_so_far_theta = -1\n",
    "        best_so_far_validation_loss = 10000000 #replace inf with a large number\n",
    "        best_so_far_after_n_iters = -1\n",
    "    for optimization_iteration in range(1,n_iters):\n",
    "        print('optimization_iteration:{} n_iters:{}',optimization_iteration,n_iters)\n",
    "        model = theta_to_model(theta)\n",
    "        momentum_speed = theta * 0;\n",
    "        training_batch_start = (((optimization_iteration_i-1) * mini_batch_size) % n_training_cases)+1;\n",
    "        print('training_batch_start:',training_batch_start)\n",
    "        #training_batch_inputs = dt_train_input['dt_train_input'](:, training_batch_start : training_batch_start + mini_batch_size - 1);\n",
    "        #training_batch_targets = dt_train_target['dt_train_target'](:, training_batch_start : training_batch_start + mini_batch_size - 1);\n",
    "        #print('training_batch_inputs shape:{} training_batch_targets shape:',training_batch_inputs.shape,training_batch_targets.shape)\n",
    "        \n",
    "        gradient = model_to_theta(d_loss_by_d_model(model, training_batch, wd_coefficient));\n",
    "        momentum_speed = momentum_speed * momentum_multiplier - gradient;\n",
    "        theta = theta + momentum_speed * learning_rate;\n",
    "        model = theta_to_model(theta);\n",
    "        training_data_losses = [training_data_losses, loss(model, datas.training, wd_coefficient)];\n",
    "        validation_data_losses = [validation_data_losses, loss(model, datas.validation, wd_coefficient)];\n",
    "        if do_early_stopping and validation_data_losses(end) < best_so_far.validation_loss:\n",
    "            best_so_far.theta = theta; # this will be overwritten soon\n",
    "            best_so_far.validation_loss = validation_data_losses(end);\n",
    "            best_so_far.after_n_iters = optimization_iteration_i;\n",
    "        if optimization_iteration_i % round(n_iters/10) == 0:\n",
    "            print('After %d optimization iterations, training data loss is %f, and validation data loss is %f\\n', \n",
    "                    optimization_iteration_i, training_data_losses(end), validation_data_losses(end));\n",
    "        if n_iters != 0: \n",
    "            test_gradient(model, datas.training, wd_coefficient) \n",
    "            #end % check again, this time with more typical parameters\n",
    "    if do_early_stopping:\n",
    "        print('Early stopping: validation loss was lowest after %d iterations. We chose the model that we had then.\\n', \n",
    "              best_so_far.after_n_iters);\n",
    "        #theta = best_so_far.theta;\n",
    "\n",
    "def initial_model(n_hid):\n",
    "    \"\"\"\n",
    "    initial_model(0) creates model.input_to_hid={0x256} and model.hid_to_class=(10x0). A numpy 0 dimension\n",
    "    is an empty matrix with allocated shape. Just like in octave!\n",
    "    \n",
    "    octave starts array indexing from 1, numpy from 0. octave range = n_params-1 and numpy range 0,n_params\n",
    "    which is same as range(n_params)\n",
    "    the octave code creates a vector from 0 to n_params-1 and does a cos on the range\n",
    "    \"\"\"\n",
    "    if n_hid==0:\n",
    "        m = model()\n",
    "        m.set_input_to_hid(np.zeros(shape=(0,256)))\n",
    "        m.set_hid_to_class(np.zeros(shape=(10,0)))\n",
    "        return m\n",
    "    \n",
    "    n_params = (256+10)*n_hid\n",
    "    print(\"n_params:\",n_params)\n",
    "    as_row_vector = np.cos(np.array(range(0,n_params)))\n",
    "    print('as_row_vector shape:',as_row_vector.shape)\n",
    "    transpose = np.transpose(np.array(as_row_vector))\n",
    "    print (\"as_row_vector transpose shape:\",transpose.shape)\n",
    "    \n",
    "    m_theta =  theta_to_model(transpose * 0.1 )\n",
    "    print('initial_model model input_to_hid shape:', m_theta.input_to_hid.shape)\n",
    "    return m_theta\n",
    "\n",
    "#divide into 2 parts, the first 256 and the second 10; the 2 sets of weights are concatenated together\n",
    "#theta is some vector.. which can be split into 1 parts. Becareful of octave start 0 numpy from 1\n",
    "def theta_to_model(theta):\n",
    "    #print('theta:',theta)\n",
    "    print('theta shape:', theta.shape)\n",
    "    n_hid = (theta.shape[0]/(256+10))\n",
    "    n_hid = int(n_hid)\n",
    "    print('theta_to_model n_hid:%f',n_hid)\n",
    "    if(n_hid==0):\n",
    "        print (\"error n_hid!=0\")\n",
    "    #reshape changes the order...\n",
    "    input_to_hid = (theta[0:(256*n_hid)]).reshape(256,n_hid)\n",
    "    print('input_to_hid shape:',input_to_hid.shape)\n",
    "    print('input_to_hid:', input_to_hid)\n",
    "    print('hid to class before reshape:', theta[256*n_hid:])\n",
    "    if(n_hid==1):\n",
    "        hid_to_class = theta[256*n_hid:].reshape(1,-1)\n",
    "    else:\n",
    "        hid_to_class = theta[256*n_hid:].reshape(n_hid,n_hid)\n",
    "    print('before transpose input_to_hid shape:', input_to_hid.shape)\n",
    "    print('before transpose hid_to_class shape:', hid_to_class.shape)\n",
    "    \n",
    "    m = model()\n",
    "    m.set_input_to_hid(np.array(np.transpose(input_to_hid)))\n",
    "    m.set_hid_to_class(np.transpose(hid_to_class))\n",
    "        \n",
    "    return m\n",
    "\n",
    "#\n",
    "def model_to_theta(model):\n",
    "    input_to_hid = model.input_to_hid\n",
    "    hid_to_class = model.hid_to_class\n",
    "    return np.stack((input_to_hid, hid_to_class),axis=1)\n",
    "\n",
    "def logistic(x):\n",
    "    return 1./(1+np.exp(np.negative(x)))\n",
    "\n",
    "#equivalent to log(sum(exp(A)))\n",
    "def log_sum_exp_over_rows(x):\n",
    "    #max row similar to octave max(x,[],1) returns max row\n",
    "    max_row =  np.max(x,axis=0)\n",
    "    return np.log(np.sum(np.exp(x),axis=0))\n",
    "\n",
    "#mapping from octave model.input_to_hid->input_to_hid, model.hid_to_class->hid_to_class\n",
    "#data.inputs->data_input, data.targets->data_targets, data_input,data_targets for test, validation, target\n",
    "#data sets. \n",
    "def loss(model,data, data_input,data_targets,wd_coefficient):\n",
    "    print('model.input_to_hid shape:', model.input_to_hid.shape)\n",
    "    hid_input = np.dot(model.input_to_hid, data.inputs)\n",
    "    print('loss model.input_to_hid shape: {} data_input.shape {} hid_input shape: ()', \n",
    "          model.input_to_hid.shape,(data.inputs).shape, hid_input.shape )\n",
    "    hid_output = logistic(hid_input)\n",
    "    class_input = np.dot(model.hid_to_class,hid_output)\n",
    "    print('class_input shape:', class_input.shape)\n",
    "    #\n",
    "    class_normalizer = log_sum_exp_over_rows(class_input)\n",
    "    print('class_normalizer shape:',class_normalizer.shape)\n",
    "    \n",
    "    log_class_prob = class_input - np.matlib.repmat(class_normalizer, class_input.shape[0],1)\n",
    "    print('log_class_prob shape:{} data_targets shape {}',log_class_prob.shape,data_targets.shape)\n",
    "\n",
    "    test_multiply = np.multiply(log_class_prob, data.targets)\n",
    "    print('test_mutiply shape:', test_multiply.shape)\n",
    "   \n",
    "    test_sum = np.sum(test_multiply,0)\n",
    "    print('test_sum shape:',test_sum.shape)\n",
    "    class_loss = -np.mean(test_sum)\n",
    "    print('classification_loss:', class_loss)\n",
    "    #wd_loss = \n",
    "    return class_loss\n",
    "    \n",
    "def classification_loss():\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "def test_gradient(model,data,wd_coefficient):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    base_theta = model_to_theta(model)\n",
    "    h = 1e-2\n",
    "    correctness_threshold = 1e-5\n",
    "    analytic_gradient = model_to_theta(d_loss_by_d_model(model,data,wd_coefficient))\n",
    "    for i in range(100):\n",
    "        test_index = (i * 1299721) % base_theta.shape[0] + 1\n",
    "        print(\"i:{} test_index:{}\", i, test_index)\n",
    "        analytic_here = analytic_gradient(test_index)\n",
    "        theta_step = base_theta * 0\n",
    "        theta_step[test_index] = h\n",
    "        #contribution_distances = [-4:1, 1:4]\n",
    "        contribution_weights = [1/280,-4/105,1/5,4/5,-1/5,4/105,-1/280]\n",
    "        temp = 0\n",
    "        #for contribution_index in range(0,8):\n",
    "        #temp = temp + loss(theta_to_model(base_theta + theta_step * contribution_distances(contribution_index)),\n",
    "        #                  data,wd_coefficient) * contribution_weights(contribution_index)\n",
    "        #fd_here = temp/h\n",
    "        #diff = np.abs(analytic_here - fd_here)\n",
    "        # print('diff:{}',diff)\n",
    "        #print('correctness_threshold:{}',correctness_threshold)\n",
    "\n",
    "def d_loss_by_d_model(model,data,wd_coefficient):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    m = data.inputs.shape[1]\n",
    "    hid_input = model.input_to_hid * data.inputs\n",
    "    hid_output = logistic(hid_input)\n",
    "    class_input = np.dot(model.hid_to_class, hid_output)\n",
    "    class_normalizer = log_sum_exp_over_rows(class_input)\n",
    "    log_class_prob = class_input - np.matlib.repmat(class_normalizer, class_input.shape[0],1)\n",
    "    class_output = np.exp(log_class_prob)\n",
    "    \n",
    "    delta_3 = class_output = data.targets\n",
    "    \n",
    "    delta_2 = np.dot(np.transpose(model.hid_to_class),delta_3) * np.dot(logistic(hid_input), 1-logistic(hid_input))\n",
    "    \n",
    "    ret_input_to_hid = 1/m * np.dot(delta_2, np.transpose(data.inputs)) + np.dot(wd_coefficient,model.input_to_hid)\n",
    "    \n",
    "    ret_hid_to_class = 1/m * np.dot(delta_3,np.transpose(hid_output)) + np.dot(wd_coefficient, model.hid_to_class)\n",
    "    print('')\n",
    "    m = model()\n",
    "    m.set_input_to_hid(ret_input_to_hid)\n",
    "    m.set_hid_to_class(ret_hid_to_class)\n",
    "    return m\n",
    "\n",
    "def classification_performance(model,data):\n",
    "    hid_input = np.dot(model.input_to_hid,data.inputs)\n",
    "    hid_output = logistic(hid_input)\n",
    "    class_input  = np.dot(model.hid_to_class, hid_output)\n",
    "    print('is this a tuple? max class_input:{}', np.max(class_input))\n",
    "    print('is this a tuple? max data.targets:{}', np.max(data.targets))\n",
    "    choices = np.max(class_input)\n",
    "    targets = max(data.targets)\n",
    "    #this is fucked up\n",
    "    test = mean((double)(choices = choices != targets))\n",
    "    \n",
    "#foo = np.array([ 4.04858735,  5.04858735,  6.04858735])\n",
    "#print(foo)\n",
    "#np.testing.assert_array_equal(np.array([1]),np.array([1]))\n",
    "#np.testing.assert_array_equal(test_logsumexp,np.array([ 4.04858735,  5.04858735,  6.04858735]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "test_logistic shape: (2, 3)\n",
      "test_logistic: [[ 0.73105858  0.88079708  0.95257413]\n",
      " [ 0.98201379  0.99330715  0.99752738]]\n"
     ]
    }
   ],
   "source": [
    "A=[[1,2,3],[4,5,6]]\n",
    "test_logistic = logistic(A)\n",
    "print(type(test_logistic))\n",
    "print(\"test_logistic shape:\",test_logistic.shape)\n",
    "print (\"test_logistic:\",test_logistic)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>verify logistic</h6>\n",
    "<img src=\"verify2.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_params: 266\n",
      "as_row_vector shape: (266,)\n",
      "transpose shape: (266,)\n",
      "theta shape: (266,)\n",
      "theta_to_model n_hid:%f 1\n",
      "input_to_hid shape: (256, 1)\n",
      "input_to_hid: [[ 0.1       ]\n",
      " [ 0.05403023]\n",
      " [-0.04161468]\n",
      " [-0.09899925]\n",
      " [-0.06536436]\n",
      " [ 0.02836622]\n",
      " [ 0.09601703]\n",
      " [ 0.07539023]\n",
      " [-0.01455   ]\n",
      " [-0.09111303]\n",
      " [-0.08390715]\n",
      " [ 0.00044257]\n",
      " [ 0.0843854 ]\n",
      " [ 0.09074468]\n",
      " [ 0.01367372]\n",
      " [-0.07596879]\n",
      " [-0.09576595]\n",
      " [-0.02751633]\n",
      " [ 0.06603167]\n",
      " [ 0.09887046]\n",
      " [ 0.04080821]\n",
      " [-0.05477293]\n",
      " [-0.09999608]\n",
      " [-0.0532833 ]\n",
      " [ 0.0424179 ]\n",
      " [ 0.09912028]\n",
      " [ 0.06469193]\n",
      " [-0.02921388]\n",
      " [-0.09626059]\n",
      " [-0.07480575]\n",
      " [ 0.01542514]\n",
      " [ 0.09147424]\n",
      " [ 0.08342234]\n",
      " [-0.00132767]\n",
      " [-0.08485703]\n",
      " [-0.09036922]\n",
      " [-0.01279637]\n",
      " [ 0.07654141]\n",
      " [ 0.09550736]\n",
      " [ 0.02666429]\n",
      " [-0.06669381]\n",
      " [-0.09873393]\n",
      " [-0.03999853]\n",
      " [ 0.05551133]\n",
      " [ 0.09998433]\n",
      " [ 0.0525322 ]\n",
      " [-0.04321779]\n",
      " [-0.09923355]\n",
      " [-0.06401443]\n",
      " [ 0.03005925]\n",
      " [ 0.0964966 ]\n",
      " [ 0.07421542]\n",
      " [-0.01629908]\n",
      " [-0.09182828]\n",
      " [-0.08293098]\n",
      " [ 0.00221268]\n",
      " [ 0.08532201]\n",
      " [ 0.08998668]\n",
      " [ 0.01191801]\n",
      " [-0.07710802]\n",
      " [-0.0952413 ]\n",
      " [-0.02581016]\n",
      " [ 0.06735072]\n",
      " [ 0.09858966]\n",
      " [ 0.03918572]\n",
      " [-0.05624539]\n",
      " [-0.09996475]\n",
      " [-0.05177698]\n",
      " [ 0.0440143 ]\n",
      " [ 0.09933904]\n",
      " [ 0.06333192]\n",
      " [-0.03090227]\n",
      " [-0.09672506]\n",
      " [-0.07361927]\n",
      " [ 0.01717173]\n",
      " [ 0.09217513]\n",
      " [ 0.08243313]\n",
      " [-0.0030975 ]\n",
      " [-0.08578031]\n",
      " [-0.08959709]\n",
      " [-0.01103872]\n",
      " [ 0.0776686 ]\n",
      " [ 0.09496777]\n",
      " [ 0.02495401]\n",
      " [-0.06800235]\n",
      " [-0.09843766]\n",
      " [-0.03836984]\n",
      " [ 0.05697503]\n",
      " [ 0.09993733]\n",
      " [ 0.0510177 ]\n",
      " [-0.04480736]\n",
      " [-0.09943675]\n",
      " [-0.06264444]\n",
      " [ 0.03174287]\n",
      " [ 0.09694594]\n",
      " [ 0.07301736]\n",
      " [-0.01804304]\n",
      " [-0.09251475]\n",
      " [-0.08192882]\n",
      " [ 0.00398209]\n",
      " [ 0.08623189]\n",
      " [ 0.08920049]\n",
      " [ 0.01015857]\n",
      " [-0.07822309]\n",
      " [-0.0946868 ]\n",
      " [-0.0240959 ]\n",
      " [ 0.06864866]\n",
      " [ 0.09827796]\n",
      " [ 0.03755096]\n",
      " [-0.05770022]\n",
      " [-0.09990208]\n",
      " [-0.05025443]\n",
      " [ 0.04559691]\n",
      " [ 0.09952666]\n",
      " [ 0.06195206]\n",
      " [-0.03258098]\n",
      " [-0.09715922]\n",
      " [-0.07240972]\n",
      " [ 0.01891294]\n",
      " [ 0.09284713]\n",
      " [ 0.0814181 ]\n",
      " [-0.00486636]\n",
      " [-0.08667671]\n",
      " [-0.08879689]\n",
      " [-0.00927762]\n",
      " [ 0.07877145]\n",
      " [ 0.09439841]\n",
      " [ 0.02323591]\n",
      " [-0.06928958]\n",
      " [-0.09811055]\n",
      " [-0.03672913]\n",
      " [ 0.05842088]\n",
      " [ 0.09985901]\n",
      " [ 0.04948722]\n",
      " [-0.04638289]\n",
      " [-0.09960878]\n",
      " [-0.06125482]\n",
      " [ 0.03341654]\n",
      " [ 0.09736489]\n",
      " [ 0.07179641]\n",
      " [-0.01978136]\n",
      " [-0.09317224]\n",
      " [-0.08090099]\n",
      " [ 0.00575025]\n",
      " [ 0.08711474]\n",
      " [ 0.08838634]\n",
      " [ 0.00839594]\n",
      " [-0.07931364]\n",
      " [-0.09410263]\n",
      " [-0.0223741 ]\n",
      " [ 0.06992508]\n",
      " [ 0.09793546]\n",
      " [ 0.03590443]\n",
      " [-0.05913697]\n",
      " [-0.09980811]\n",
      " [-0.04871613]\n",
      " [ 0.04716523]\n",
      " [ 0.0996831 ]\n",
      " [ 0.06055279]\n",
      " [-0.03424948]\n",
      " [-0.09756293]\n",
      " [-0.07117748]\n",
      " [ 0.02064822]\n",
      " [ 0.09349004]\n",
      " [ 0.08037755]\n",
      " [-0.00663369]\n",
      " [-0.08754595]\n",
      " [-0.08796886]\n",
      " [-0.00751361]\n",
      " [ 0.07984962]\n",
      " [ 0.09379948]\n",
      " [ 0.02151053]\n",
      " [-0.0705551 ]\n",
      " [-0.09775269]\n",
      " [-0.03507691]\n",
      " [ 0.05984842]\n",
      " [ 0.09974939]\n",
      " [ 0.04794123]\n",
      " [-0.04794388]\n",
      " [-0.09974961]\n",
      " [-0.05984601]\n",
      " [ 0.03507973]\n",
      " [ 0.09775333]\n",
      " [ 0.07055296]\n",
      " [-0.02151347]\n",
      " [-0.09380052]\n",
      " [-0.0798478 ]\n",
      " [ 0.00751662]\n",
      " [ 0.08797029]\n",
      " [ 0.08754449]\n",
      " [ 0.00663069]\n",
      " [-0.08037934]\n",
      " [-0.09348897]\n",
      " [-0.02064527]\n",
      " [ 0.07117959]\n",
      " [ 0.09756227]\n",
      " [ 0.03424665]\n",
      " [-0.06055519]\n",
      " [-0.09968286]\n",
      " [-0.04716257]\n",
      " [ 0.04871877]\n",
      " [ 0.0998083 ]\n",
      " [ 0.05913454]\n",
      " [-0.03590724]\n",
      " [-0.09793607]\n",
      " [-0.06992293]\n",
      " [ 0.02237703]\n",
      " [ 0.09410365]\n",
      " [ 0.07931181]\n",
      " [-0.00839895]\n",
      " [-0.08838775]\n",
      " [-0.08711326]\n",
      " [-0.00574724]\n",
      " [ 0.08090276]\n",
      " [ 0.09317114]\n",
      " [ 0.0197784 ]\n",
      " [-0.07179851]\n",
      " [-0.0973642 ]\n",
      " [-0.0334137 ]\n",
      " [ 0.06125721]\n",
      " [ 0.09960852]\n",
      " [ 0.04638022]\n",
      " [-0.04948984]\n",
      " [-0.09985917]\n",
      " [-0.05841844]\n",
      " [ 0.03673194]\n",
      " [ 0.09811114]\n",
      " [ 0.06928741]\n",
      " [-0.02323884]\n",
      " [-0.09439941]\n",
      " [-0.07876959]\n",
      " [ 0.00928062]\n",
      " [ 0.08879828]\n",
      " [ 0.08667521]\n",
      " [ 0.00486335]\n",
      " [-0.08141985]\n",
      " [-0.09284601]\n",
      " [-0.01890998]\n",
      " [ 0.0724118 ]\n",
      " [ 0.09715851]\n",
      " [ 0.03257813]\n",
      " [-0.06195443]\n",
      " [-0.09952637]\n",
      " [-0.04559423]\n",
      " [ 0.05025704]\n",
      " [ 0.09990221]\n",
      " [ 0.05769776]\n",
      " [-0.03755375]\n",
      " [-0.09827852]\n",
      " [-0.06864646]\n",
      " [ 0.02409883]\n",
      " [ 0.09468777]\n",
      " [ 0.07822121]\n",
      " [-0.01016157]\n",
      " [-0.08920185]\n",
      " [-0.08623036]]\n",
      "hid to class before reshape: [-0.00397908  0.08193055  0.09251361  0.01804008 -0.07301942 -0.0969452\n",
      " -0.03174001  0.06264679  0.09943643  0.04480467]\n",
      "before transpose input_to_hid shape: (256, 1)\n",
      "before transpose hid_to_class shape: (1, 10)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'model' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-191-94fc47efb55d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitial_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_to_hid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhid_to_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_to_hid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhid_to_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-183-40e067fbadb2>\u001b[0m in \u001b[0;36minitial_model\u001b[0;34m(n_hid)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"transpose shape:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0mm_theta\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mtheta_to_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranspose\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.1\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'initial_model model input_to_hid shape:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm_theta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_to_hid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mm_theta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-183-40e067fbadb2>\u001b[0m in \u001b[0;36mtheta_to_model\u001b[0;34m(theta)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'before transpose hid_to_class shape:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhid_to_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m     \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m     \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_input_to_hid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_to_hid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_hid_to_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhid_to_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'model' object is not callable"
     ]
    }
   ],
   "source": [
    "a = initial_model(1)\n",
    "print(a.input_to_hid.shape)\n",
    "print(a.hid_to_class.shape)\n",
    "print(a.input_to_hid)\n",
    "print(a.hid_to_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Initial Model(1)</h6>\n",
    "<img src=\"im1a.png\">\n",
    "<img src=\"im1b.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img arc=\"verify3.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "test_logsumexp: (3,)\n",
      "test_logsumexp: [ 4.04858735  5.04858735  6.04858735]\n"
     ]
    }
   ],
   "source": [
    "A=[[1,2,3],[4,5,6]]\n",
    "test_logsumexp = log_sum_exp_over_rows(A)\n",
    "print(type(test_logsumexp))\n",
    "print(\"test_logsumexp:\",test_logsumexp.shape)\n",
    "print (\"test_logsumexp:\",test_logsumexp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>verify log_sum_exp_over_rows</h6>\n",
    "<img src=\"verify1.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_params: 2660\n",
      "as_row_vector shape: (2660,)\n",
      "transpose shape: (2660,)\n",
      "theta shape: (2660,)\n",
      "theta_to_model n_hid:%f 10\n",
      "input_to_hid shape: (256, 10)\n",
      "input_to_hid: [[ 0.1         0.05403023 -0.04161468 ...,  0.07539023 -0.01455    -0.09111303]\n",
      " [-0.08390715  0.00044257  0.0843854  ..., -0.02751633  0.06603167\n",
      "   0.09887046]\n",
      " [ 0.04080821 -0.05477293 -0.09999608 ..., -0.02921388 -0.09626059\n",
      "  -0.07480575]\n",
      " ..., \n",
      " [-0.05251424  0.04323682  0.09923615 ...,  0.0163199   0.09183663\n",
      "   0.08291919]\n",
      " [-0.00223377 -0.08533301 -0.08997748 ..., -0.06736631 -0.09858612\n",
      "  -0.03916631]\n",
      " [ 0.05626283  0.09996418  0.05175893 ...,  0.09673041  0.07360499\n",
      "  -0.01719252]]\n",
      "hid to class before reshape: [-0.09218331 -0.08242119  0.00311859  0.08579115  0.08958772  0.01101775\n",
      " -0.07768189 -0.09496116 -0.02493358  0.06801782  0.09843395  0.03835036\n",
      " -0.05699237 -0.09993658 -0.05099955  0.04482622  0.09943898  0.062628\n",
      " -0.03176288 -0.09695111 -0.07300294  0.0180638   0.09252276  0.08191672\n",
      " -0.00400317 -0.08624257 -0.08919095 -0.01013758  0.07823623  0.09468001\n",
      "  0.02407543 -0.068664   -0.09827406 -0.0375314   0.05771745  0.09990115\n",
      "  0.05023619 -0.04561569 -0.09952871 -0.0619355   0.03260093  0.09716421\n",
      "  0.07239516 -0.01893366 -0.09285497 -0.08140584  0.00488744  0.08668723\n",
      "  0.08878718  0.00925661 -0.07878445 -0.09439145 -0.02321539  0.0693048\n",
      "  0.09810647  0.03670951 -0.05843801 -0.09985788 -0.04946888  0.04640158\n",
      "  0.09961065  0.06123814 -0.03343643 -0.0973697  -0.07178172  0.01980204\n",
      "  0.0931799   0.08088859 -0.00577132 -0.0871251  -0.08837647 -0.00837492\n",
      "  0.07932649  0.09409549  0.02235353 -0.06994016 -0.09793119 -0.03588473\n",
      "  0.05915398  0.0998068   0.04869771 -0.04718383 -0.09968478 -0.06053599\n",
      "  0.0342693   0.09756756  0.07116265 -0.02066887 -0.09349753 -0.08036499\n",
      "  0.00665475  0.08755614  0.08795882  0.00749257 -0.07986232 -0.09379216\n",
      " -0.02148992  0.07057005  0.09774824  0.03505715]\n",
      "before transpose input_to_hid shape: (256, 10)\n",
      "before transpose hid_to_class shape: (10, 10)\n",
      "initial_model model input_to_hid shape: (10, 256)\n",
      "0.131565327668\n",
      "0.0216791270802\n"
     ]
    }
   ],
   "source": [
    "a = initial_model(10)\n",
    "print(np.sum(a.input_to_hid))\n",
    "print(np.sum(a.hid_to_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
       "        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
       "        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
       "        39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
       "        52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
       "        65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
       "        78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
       "        91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
       "       104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
       "       117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n",
       "       130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,\n",
       "       143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
       "       156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168,\n",
       "       169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
       "       182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194,\n",
       "       195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207,\n",
       "       208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220,\n",
       "       221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233,\n",
       "       234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246,\n",
       "       247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259,\n",
       "       260, 261, 262, 263, 264, 265])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "#careful numpy is 0->265 octave is 1->266\n",
    "a = np.arange(0,266)\n",
    "a.shape\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
      " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
      " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253 254 255 256 257 258 259 260 261 262 263 264 265]\n",
      "theta shape: (266,)\n",
      "theta_to_model n_hid:%f 1\n",
      "input_to_hid shape: (256, 1)\n",
      "input_to_hid: [[  0]\n",
      " [  1]\n",
      " [  2]\n",
      " [  3]\n",
      " [  4]\n",
      " [  5]\n",
      " [  6]\n",
      " [  7]\n",
      " [  8]\n",
      " [  9]\n",
      " [ 10]\n",
      " [ 11]\n",
      " [ 12]\n",
      " [ 13]\n",
      " [ 14]\n",
      " [ 15]\n",
      " [ 16]\n",
      " [ 17]\n",
      " [ 18]\n",
      " [ 19]\n",
      " [ 20]\n",
      " [ 21]\n",
      " [ 22]\n",
      " [ 23]\n",
      " [ 24]\n",
      " [ 25]\n",
      " [ 26]\n",
      " [ 27]\n",
      " [ 28]\n",
      " [ 29]\n",
      " [ 30]\n",
      " [ 31]\n",
      " [ 32]\n",
      " [ 33]\n",
      " [ 34]\n",
      " [ 35]\n",
      " [ 36]\n",
      " [ 37]\n",
      " [ 38]\n",
      " [ 39]\n",
      " [ 40]\n",
      " [ 41]\n",
      " [ 42]\n",
      " [ 43]\n",
      " [ 44]\n",
      " [ 45]\n",
      " [ 46]\n",
      " [ 47]\n",
      " [ 48]\n",
      " [ 49]\n",
      " [ 50]\n",
      " [ 51]\n",
      " [ 52]\n",
      " [ 53]\n",
      " [ 54]\n",
      " [ 55]\n",
      " [ 56]\n",
      " [ 57]\n",
      " [ 58]\n",
      " [ 59]\n",
      " [ 60]\n",
      " [ 61]\n",
      " [ 62]\n",
      " [ 63]\n",
      " [ 64]\n",
      " [ 65]\n",
      " [ 66]\n",
      " [ 67]\n",
      " [ 68]\n",
      " [ 69]\n",
      " [ 70]\n",
      " [ 71]\n",
      " [ 72]\n",
      " [ 73]\n",
      " [ 74]\n",
      " [ 75]\n",
      " [ 76]\n",
      " [ 77]\n",
      " [ 78]\n",
      " [ 79]\n",
      " [ 80]\n",
      " [ 81]\n",
      " [ 82]\n",
      " [ 83]\n",
      " [ 84]\n",
      " [ 85]\n",
      " [ 86]\n",
      " [ 87]\n",
      " [ 88]\n",
      " [ 89]\n",
      " [ 90]\n",
      " [ 91]\n",
      " [ 92]\n",
      " [ 93]\n",
      " [ 94]\n",
      " [ 95]\n",
      " [ 96]\n",
      " [ 97]\n",
      " [ 98]\n",
      " [ 99]\n",
      " [100]\n",
      " [101]\n",
      " [102]\n",
      " [103]\n",
      " [104]\n",
      " [105]\n",
      " [106]\n",
      " [107]\n",
      " [108]\n",
      " [109]\n",
      " [110]\n",
      " [111]\n",
      " [112]\n",
      " [113]\n",
      " [114]\n",
      " [115]\n",
      " [116]\n",
      " [117]\n",
      " [118]\n",
      " [119]\n",
      " [120]\n",
      " [121]\n",
      " [122]\n",
      " [123]\n",
      " [124]\n",
      " [125]\n",
      " [126]\n",
      " [127]\n",
      " [128]\n",
      " [129]\n",
      " [130]\n",
      " [131]\n",
      " [132]\n",
      " [133]\n",
      " [134]\n",
      " [135]\n",
      " [136]\n",
      " [137]\n",
      " [138]\n",
      " [139]\n",
      " [140]\n",
      " [141]\n",
      " [142]\n",
      " [143]\n",
      " [144]\n",
      " [145]\n",
      " [146]\n",
      " [147]\n",
      " [148]\n",
      " [149]\n",
      " [150]\n",
      " [151]\n",
      " [152]\n",
      " [153]\n",
      " [154]\n",
      " [155]\n",
      " [156]\n",
      " [157]\n",
      " [158]\n",
      " [159]\n",
      " [160]\n",
      " [161]\n",
      " [162]\n",
      " [163]\n",
      " [164]\n",
      " [165]\n",
      " [166]\n",
      " [167]\n",
      " [168]\n",
      " [169]\n",
      " [170]\n",
      " [171]\n",
      " [172]\n",
      " [173]\n",
      " [174]\n",
      " [175]\n",
      " [176]\n",
      " [177]\n",
      " [178]\n",
      " [179]\n",
      " [180]\n",
      " [181]\n",
      " [182]\n",
      " [183]\n",
      " [184]\n",
      " [185]\n",
      " [186]\n",
      " [187]\n",
      " [188]\n",
      " [189]\n",
      " [190]\n",
      " [191]\n",
      " [192]\n",
      " [193]\n",
      " [194]\n",
      " [195]\n",
      " [196]\n",
      " [197]\n",
      " [198]\n",
      " [199]\n",
      " [200]\n",
      " [201]\n",
      " [202]\n",
      " [203]\n",
      " [204]\n",
      " [205]\n",
      " [206]\n",
      " [207]\n",
      " [208]\n",
      " [209]\n",
      " [210]\n",
      " [211]\n",
      " [212]\n",
      " [213]\n",
      " [214]\n",
      " [215]\n",
      " [216]\n",
      " [217]\n",
      " [218]\n",
      " [219]\n",
      " [220]\n",
      " [221]\n",
      " [222]\n",
      " [223]\n",
      " [224]\n",
      " [225]\n",
      " [226]\n",
      " [227]\n",
      " [228]\n",
      " [229]\n",
      " [230]\n",
      " [231]\n",
      " [232]\n",
      " [233]\n",
      " [234]\n",
      " [235]\n",
      " [236]\n",
      " [237]\n",
      " [238]\n",
      " [239]\n",
      " [240]\n",
      " [241]\n",
      " [242]\n",
      " [243]\n",
      " [244]\n",
      " [245]\n",
      " [246]\n",
      " [247]\n",
      " [248]\n",
      " [249]\n",
      " [250]\n",
      " [251]\n",
      " [252]\n",
      " [253]\n",
      " [254]\n",
      " [255]]\n",
      "hid to class before reshape: [256 257 258 259 260 261 262 263 264 265]\n",
      "before transpose input_to_hid shape: (256, 1)\n",
      "before transpose hid_to_class shape: (1, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10, 1, 1)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = theta_to_model(a)\n",
    "b.input_to_hid\n",
    "b.input_to_hid.shape\n",
    "b.hid_to_class.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 256, 1)\n",
      "(10, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "#this verifies and mataches the octave row format\n",
    "type(b.input_to_hid)\n",
    "print(b.input_to_hid.shape)\n",
    "print(b.hid_to_class.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta: [   0    1    2 ..., 2657 2658 2659]\n",
      "theta shape: (2660,)\n",
      "theta_to_model n_hid:%f 10\n",
      "input_to_hid shape: (256, 10)\n",
      "input_to_hid: [[   0    1    2 ...,    7    8    9]\n",
      " [  10   11   12 ...,   17   18   19]\n",
      " [  20   21   22 ...,   27   28   29]\n",
      " ..., \n",
      " [2530 2531 2532 ..., 2537 2538 2539]\n",
      " [2540 2541 2542 ..., 2547 2548 2549]\n",
      " [2550 2551 2552 ..., 2557 2558 2559]]\n",
      "hid to class before reshape: [2560 2561 2562 2563 2564 2565 2566 2567 2568 2569 2570 2571 2572 2573 2574\n",
      " 2575 2576 2577 2578 2579 2580 2581 2582 2583 2584 2585 2586 2587 2588 2589\n",
      " 2590 2591 2592 2593 2594 2595 2596 2597 2598 2599 2600 2601 2602 2603 2604\n",
      " 2605 2606 2607 2608 2609 2610 2611 2612 2613 2614 2615 2616 2617 2618 2619\n",
      " 2620 2621 2622 2623 2624 2625 2626 2627 2628 2629 2630 2631 2632 2633 2634\n",
      " 2635 2636 2637 2638 2639 2640 2641 2642 2643 2644 2645 2646 2647 2648 2649\n",
      " 2650 2651 2652 2653 2654 2655 2656 2657 2658 2659]\n",
      "before transpose input_to_hid shape: (256, 10)\n",
      "before transpose hid_to_class shape: (10, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10, 10)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test 10x confused octave shows 10x256 and 10x10 for n_hid=10\n",
    "a = np.arange(0,2660)\n",
    "b = theta_to_model(a)\n",
    "b.input_to_hid\n",
    "b.input_to_hid.shape\n",
    "b.hid_to_class.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2560, 2561, 2562, 2563, 2564, 2565, 2566, 2567, 2568, 2569],\n",
       "       [2570, 2571, 2572, 2573, 2574, 2575, 2576, 2577, 2578, 2579],\n",
       "       [2580, 2581, 2582, 2583, 2584, 2585, 2586, 2587, 2588, 2589],\n",
       "       [2590, 2591, 2592, 2593, 2594, 2595, 2596, 2597, 2598, 2599],\n",
       "       [2600, 2601, 2602, 2603, 2604, 2605, 2606, 2607, 2608, 2609],\n",
       "       [2610, 2611, 2612, 2613, 2614, 2615, 2616, 2617, 2618, 2619],\n",
       "       [2620, 2621, 2622, 2623, 2624, 2625, 2626, 2627, 2628, 2629],\n",
       "       [2630, 2631, 2632, 2633, 2634, 2635, 2636, 2637, 2638, 2639],\n",
       "       [2640, 2641, 2642, 2643, 2644, 2645, 2646, 2647, 2648, 2649],\n",
       "       [2650, 2651, 2652, 2653, 2654, 2655, 2656, 2657, 2658, 2659]])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.arange(0,2660)\n",
    "n_hid=10\n",
    "a[0:256*n_hid].shape\n",
    "a[0:(256*n_hid)].reshape(256,n_hid).shape\n",
    "np.transpose(a[0:(256*n_hid)].reshape(256,n_hid)).shape\n",
    "#incrementing across columns is correct, \n",
    "a[256*n_hid:].reshape(n_hid,n_hid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a transpose shape: (2660, 1)\n",
      "theta: [[   0]\n",
      " [   1]\n",
      " [   2]\n",
      " ..., \n",
      " [2657]\n",
      " [2658]\n",
      " [2659]]\n",
      "theta shape: (2660, 1)\n",
      "theta_to_model n_hid:%f 10\n",
      "input_to_hid shape: (256, 10)\n",
      "input_to_hid: [[   0    1    2 ...,    7    8    9]\n",
      " [  10   11   12 ...,   17   18   19]\n",
      " [  20   21   22 ...,   27   28   29]\n",
      " ..., \n",
      " [2530 2531 2532 ..., 2537 2538 2539]\n",
      " [2540 2541 2542 ..., 2547 2548 2549]\n",
      " [2550 2551 2552 ..., 2557 2558 2559]]\n",
      "hid to class before reshape: [[2560]\n",
      " [2561]\n",
      " [2562]\n",
      " [2563]\n",
      " [2564]\n",
      " [2565]\n",
      " [2566]\n",
      " [2567]\n",
      " [2568]\n",
      " [2569]\n",
      " [2570]\n",
      " [2571]\n",
      " [2572]\n",
      " [2573]\n",
      " [2574]\n",
      " [2575]\n",
      " [2576]\n",
      " [2577]\n",
      " [2578]\n",
      " [2579]\n",
      " [2580]\n",
      " [2581]\n",
      " [2582]\n",
      " [2583]\n",
      " [2584]\n",
      " [2585]\n",
      " [2586]\n",
      " [2587]\n",
      " [2588]\n",
      " [2589]\n",
      " [2590]\n",
      " [2591]\n",
      " [2592]\n",
      " [2593]\n",
      " [2594]\n",
      " [2595]\n",
      " [2596]\n",
      " [2597]\n",
      " [2598]\n",
      " [2599]\n",
      " [2600]\n",
      " [2601]\n",
      " [2602]\n",
      " [2603]\n",
      " [2604]\n",
      " [2605]\n",
      " [2606]\n",
      " [2607]\n",
      " [2608]\n",
      " [2609]\n",
      " [2610]\n",
      " [2611]\n",
      " [2612]\n",
      " [2613]\n",
      " [2614]\n",
      " [2615]\n",
      " [2616]\n",
      " [2617]\n",
      " [2618]\n",
      " [2619]\n",
      " [2620]\n",
      " [2621]\n",
      " [2622]\n",
      " [2623]\n",
      " [2624]\n",
      " [2625]\n",
      " [2626]\n",
      " [2627]\n",
      " [2628]\n",
      " [2629]\n",
      " [2630]\n",
      " [2631]\n",
      " [2632]\n",
      " [2633]\n",
      " [2634]\n",
      " [2635]\n",
      " [2636]\n",
      " [2637]\n",
      " [2638]\n",
      " [2639]\n",
      " [2640]\n",
      " [2641]\n",
      " [2642]\n",
      " [2643]\n",
      " [2644]\n",
      " [2645]\n",
      " [2646]\n",
      " [2647]\n",
      " [2648]\n",
      " [2649]\n",
      " [2650]\n",
      " [2651]\n",
      " [2652]\n",
      " [2653]\n",
      " [2654]\n",
      " [2655]\n",
      " [2656]\n",
      " [2657]\n",
      " [2658]\n",
      " [2659]]\n",
      "before transpose input_to_hid shape: (256, 10)\n",
      "before transpose hid_to_class shape: (10, 10)\n",
      "(256, 10)\n",
      "(10, 10)\n",
      "input_to_hid\n",
      "[[   0    1    2 ...,    7    8    9]\n",
      " [  10   11   12 ...,   17   18   19]\n",
      " [  20   21   22 ...,   27   28   29]\n",
      " ..., \n",
      " [2530 2531 2532 ..., 2537 2538 2539]\n",
      " [2540 2541 2542 ..., 2547 2548 2549]\n",
      " [2550 2551 2552 ..., 2557 2558 2559]]\n",
      "hid_to_class:\n",
      "[[2560 2561 2562 2563 2564 2565 2566 2567 2568 2569]\n",
      " [2570 2571 2572 2573 2574 2575 2576 2577 2578 2579]\n",
      " [2580 2581 2582 2583 2584 2585 2586 2587 2588 2589]\n",
      " [2590 2591 2592 2593 2594 2595 2596 2597 2598 2599]\n",
      " [2600 2601 2602 2603 2604 2605 2606 2607 2608 2609]\n",
      " [2610 2611 2612 2613 2614 2615 2616 2617 2618 2619]\n",
      " [2620 2621 2622 2623 2624 2625 2626 2627 2628 2629]\n",
      " [2630 2631 2632 2633 2634 2635 2636 2637 2638 2639]\n",
      " [2640 2641 2642 2643 2644 2645 2646 2647 2648 2649]\n",
      " [2650 2651 2652 2653 2654 2655 2656 2657 2658 2659]]\n"
     ]
    }
   ],
   "source": [
    "# test by taking transpose of a with new axis\n",
    "a= np.arange(0,2660)[np.newaxis]\n",
    "at = np.transpose(a)\n",
    "print('a transpose shape:',at.shape)\n",
    "bt = theta_to_model(at)\n",
    "print(bt.input_to_hid.shape)\n",
    "print(bt.hid_to_class.shape)\n",
    "#wrong bc cant have new axis if n_hid!=1. Only for numpy\n",
    "print('input_to_hid')\n",
    "print(bt.input_to_hid)\n",
    "print('hid_to_class:')\n",
    "print(bt.hid_to_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_params: 2660\n",
      "as_row_vector shape: (2660,)\n",
      "transpose shape: (2660,)\n",
      "theta shape: (2660,)\n",
      "theta_to_model n_hid:%f 10\n",
      "input_to_hid shape: (256, 10)\n",
      "input_to_hid: [[ 0.1         0.05403023 -0.04161468 ...,  0.07539023 -0.01455    -0.09111303]\n",
      " [-0.08390715  0.00044257  0.0843854  ..., -0.02751633  0.06603167\n",
      "   0.09887046]\n",
      " [ 0.04080821 -0.05477293 -0.09999608 ..., -0.02921388 -0.09626059\n",
      "  -0.07480575]\n",
      " ..., \n",
      " [-0.05251424  0.04323682  0.09923615 ...,  0.0163199   0.09183663\n",
      "   0.08291919]\n",
      " [-0.00223377 -0.08533301 -0.08997748 ..., -0.06736631 -0.09858612\n",
      "  -0.03916631]\n",
      " [ 0.05626283  0.09996418  0.05175893 ...,  0.09673041  0.07360499\n",
      "  -0.01719252]]\n",
      "hid to class before reshape: [-0.09218331 -0.08242119  0.00311859  0.08579115  0.08958772  0.01101775\n",
      " -0.07768189 -0.09496116 -0.02493358  0.06801782  0.09843395  0.03835036\n",
      " -0.05699237 -0.09993658 -0.05099955  0.04482622  0.09943898  0.062628\n",
      " -0.03176288 -0.09695111 -0.07300294  0.0180638   0.09252276  0.08191672\n",
      " -0.00400317 -0.08624257 -0.08919095 -0.01013758  0.07823623  0.09468001\n",
      "  0.02407543 -0.068664   -0.09827406 -0.0375314   0.05771745  0.09990115\n",
      "  0.05023619 -0.04561569 -0.09952871 -0.0619355   0.03260093  0.09716421\n",
      "  0.07239516 -0.01893366 -0.09285497 -0.08140584  0.00488744  0.08668723\n",
      "  0.08878718  0.00925661 -0.07878445 -0.09439145 -0.02321539  0.0693048\n",
      "  0.09810647  0.03670951 -0.05843801 -0.09985788 -0.04946888  0.04640158\n",
      "  0.09961065  0.06123814 -0.03343643 -0.0973697  -0.07178172  0.01980204\n",
      "  0.0931799   0.08088859 -0.00577132 -0.0871251  -0.08837647 -0.00837492\n",
      "  0.07932649  0.09409549  0.02235353 -0.06994016 -0.09793119 -0.03588473\n",
      "  0.05915398  0.0998068   0.04869771 -0.04718383 -0.09968478 -0.06053599\n",
      "  0.0342693   0.09756756  0.07116265 -0.02066887 -0.09349753 -0.08036499\n",
      "  0.00665475  0.08755614  0.08795882  0.00749257 -0.07986232 -0.09379216\n",
      " -0.02148992  0.07057005  0.09774824  0.03505715]\n",
      "before transpose input_to_hid shape: (256, 10)\n",
      "before transpose hid_to_class shape: (10, 10)\n",
      "initial_model model input_to_hid shape: (10, 256)\n",
      "(10, 256)\n",
      "(10, 10)\n"
     ]
    }
   ],
   "source": [
    "m = initial_model(10)\n",
    "print (m.input_to_hid.shape)\n",
    "print(m.hid_to_class.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>notes</h6>\n",
    "in octave from_data_file = load('data.mat')\n",
    "<p></p>\n",
    "in octave datas = data_file.data\n",
    "<p></p>\n",
    "in octave, datas2 = {datas.training, datas.validation, datas.test}\n",
    "<p></p>\n",
    "data=datas2{data_i}\n",
    "<p></p>\n",
    "then loss is called with loss(model,data,wd_coefficient) or\n",
    "loss(model,datas.training,wd_coefficient) then loss(model,datas.validation,wd_coefficient) \n",
    "then loss(model,datas.test,wd_coefficient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_target shape: (10, 1000)\n",
      "train_input shape: (256, 1000)\n",
      "model.input_to_hid shape: (10, 256)\n",
      "loss model.input_to_hid shape: {} data_input.shape {} hid_input shape: () (10, 256) (256, 1000) (10, 1000)\n",
      "class_input shape: (10, 1000)\n",
      "class_normalizer shape: (1000,)\n",
      "log_class_prob shape:{} data_targets shape {} (10, 1000) (10, 1000)\n",
      "test_mutiply shape: (10, 1000)\n",
      "test_sum shape: (1000,)\n",
      "classification_loss: 2.30264159691\n",
      "2.30264159691\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_target = dt_train_target['dt_train_target']\n",
    "train_input = dt_train_input['dt_train_input']\n",
    "traindata = data()\n",
    "traindata.set_inputs(train_input)\n",
    "traindata.set_target(train_target)\n",
    "print ('train_target shape:',train_target.shape)\n",
    "print ('train_input shape:',train_input.shape)\n",
    "a = loss(m, traindata,train_input, train_target, 0)\n",
    "print(a)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.input_to_hid shape: (10, 256)\n",
      "loss model.input_to_hid shape: {} data_input.shape {} hid_input shape: () (10, 256) (256, 9000) (10, 9000)\n",
      "class_input shape: (10, 9000)\n",
      "class_normalizer shape: (9000,)\n",
      "log_class_prob shape:{} data_targets shape {} (10, 9000) (10, 9000)\n",
      "test_mutiply shape: (10, 9000)\n",
      "test_sum shape: (9000,)\n",
      "classification_loss: 2.30260276601\n",
      "2.30260276601\n",
      "model.input_to_hid shape: (10, 256)\n",
      "loss model.input_to_hid shape: {} data_input.shape {} hid_input shape: () (10, 256) (256, 1000) (10, 1000)\n",
      "class_input shape: (10, 1000)\n",
      "class_normalizer shape: (1000,)\n",
      "log_class_prob shape:{} data_targets shape {} (10, 1000) (10, 1000)\n",
      "test_mutiply shape: (10, 1000)\n",
      "test_sum shape: (1000,)\n",
      "classification_loss: 2.30257269494\n",
      "2.30257269494\n"
     ]
    }
   ],
   "source": [
    "test_input = dt_test_input['dt_test_input'] \n",
    "test_target = dt_test_target['dt_test_target']\n",
    "test_data = data()\n",
    "test_data.set_inputs(test_input)\n",
    "test_data.set_target(test_target)\n",
    "\n",
    "a = loss(m, test_data, test_input, test_target, 0)\n",
    "print(a)\n",
    "valid_input = dt_valid_input['dt_valid_input']\n",
    "valid_target = dt_valid_target['dt_valid_target']\n",
    "valid_data = data()\n",
    "valid_data.set_inputs(valid_input)\n",
    "valid_data.set_target(valid_target)\n",
    "\n",
    "a = loss(m, valid_data, valid_input, valid_target, 0)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
