data function ret = loss(model, data, wd_coefficient)  % model.input_to_hid is a matrix of size <number of hidden units> by <number of inputs i.e. 256>. It contains the weights from the input units to the hidden units.  % model.hid_to_class is a matrix of size <number of classes i.e. 10> by <number of hidden units>. It contains the weights from the hidden units to the softmax units.  % data.inputs is a matrix of size <number of inputs i.e. 256> by <number of data cases>. Each column describes a different data case.   % data.targets is a matrix of size <number of classes i.e. 10> by <number of data cases>. Each column describes a different data case. It contains a one-of-N encoding of the class, i.e. one element in every column is 1 and the others are 0.	   % Before we can calculate the loss, we need to calculate a variety of intermediate values, like the state of the hidden units.  hid_input = model.input_to_hid * data.inputs; % input to the hidden units, i.e. before the logistic. size: <number of hidden units> by <number of data cases>  hid_output = logistic(hid_input); % output of the hidden units, i.e. after the logistic. size: <number of hidden units> by <number of data cases>  class_input = model.hid_to_class * hid_output; % input to the components of the softmax. size: <number of classes, i.e. 10> by <number of data cases>  #fprintf('model.input_to_hid %dx%d\n', rows(model.input_to_hid),columns(model.input_to_hid));  #fprintf('loss model.input_to_hid sum:%f, sum data.inputs:%f\n', sum(sum(model.input_to_hid)),sum(sum(data.inputs)));  #fprintf('loss hid_input rows:%d cols:%d\n',rows(hid_input),columns(hid_input));   #fprintf('loss hid_output rows:%d cols:%d\n',rows(hid_output),columns(hid_output));   #fprintf('loss class_input rows:%d cols:%d\n',rows(class_input),columns(class_input));   #fprintf('loss data.inputs sum:%f\n', sum(sum(data.inputs)))  #fprintf('hid_input %f, hid_output:%f, class_input:%f\n',sum(sum(hid_input)),sum(sum(hid_output)),sum(sum(class_input)));  % The following three lines of code implement the softmax.  % However, it's written differently from what the lectures say.  % In the lectures, a softmax is described using an exponential divided by a sum of exponentials.  % What we do here is exactly equivalent (you can check the math or just check it in practice), but this is more numerically stable.   % "Numerically stable" means that this way, there will never be really big numbers involved.  % The exponential in the lectures can lead to really big numbers, which are fine in mathematical equations, but can lead to all sorts of problems in Octave.  % Octave isn't well prepared to deal with really large numbers, like the number 10 to the power 1000. Computations with such numbers get unstable, so we avoid them.  class_normalizer = log_sum_exp_over_rows(class_input); % log(sum(exp of class_input)) is what we subtract to get properly normalized log class probabilities. size: <1> by <number of data cases>  #fprintf('loss class_normalizer:%d x %d\n', rows(class_normalizer),columns(class_normalizer));  log_class_prob = class_input - repmat(class_normalizer, [size(class_input, 1), 1]); % log of probability of each class. size: <number of classes, i.e. 10> by <number of data cases>  #fprintf('loss log_class_prob:%dx%d\n', rows(log_class_prob),columns(log_class_prob));  class_prob = exp(log_class_prob); % probability of each class. Each column (i.e. each case) sums to 1. size: <number of classes, i.e. 10> by <number of data cases>  #fprintf('loss class_prob:%dx%d\n',rows(class_prob),columns(class_prob))  #fprintf('class_normalizer:%f, log_class_prob:%f, class_prob:%f\n',sum(class_normalizer),sum(sum(log_class_prob)),sum(sum(class_prob)));  classification_loss = -mean(sum(log_class_prob .* data.targets, 1)); % select the right log class probability using that sum; then take the mean over all data cases.  #fprintf("classification_loss:%f\n",classification_loss)  wd_loss = sum(model_to_theta(model).^2)/2*wd_coefficient; % weight decay loss. very straightforward: E = 1/2 * wd_coeffecient * theta^2  fprintf('loss wd_loss:%f\n', wd_loss);  ret = classification_loss + wd_loss;end