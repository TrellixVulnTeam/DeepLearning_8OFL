{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "<h6>Corrections to Udacity Course</h6>\n",
    "The DL Udacity class was originally created by Vincent Vanhoucke, a research scientist and tech lead for Google Brain. \n",
    "This was a free class. Udacity removed some of Vincent's content, cut it up into 5 modules and added their own content\n",
    "from presented by youtube stars.\n",
    "<h6>Corrections to Uddacity Section1 Introduction</6>\n",
    "Welcome, Anaconda, Jupyter, Applying DL, Regression Videos. \n",
    "<h6>Corrections to Udacity Section2 Neural Networks</h6>\n",
    "<p>\n",
    "There are 6 sections here, 1) Matrix Math and NumPy Refresher, 2) Intro to NN,\n",
    "3) your First Neural Network assignment, 4) Model Evaluation and Validation, \n",
    "5) Sentiment Analysis with Andrew Trask, 6)MiniFlow. \n",
    "I viewed the Intro to NN, First NN assignment, Model Evaluation and Validation videos\n",
    "</p>\n",
    "<h6>Corrections for Section 2.2 NN - Intro to NN</h6>\n",
    "<p>This section is added by Udacity. There are no presentations by Vincent here. </p>\n",
    "</p>In this section Udacity presents examples for AND/OR/NOT/XOR perceptrons. A better idea would have\n",
    "been to write a MLP for a XOR gate and then to the IRIS example. This gives you a way to see numerical\n",
    "values and to use them to build intuition and debug tools. Difficult if not impossible to do this w/image data. \n",
    "They do not include code for a working XOR gate.</p>\n",
    "<p> Gradient descent; explain and code Gradient Descent. This is useless and misleading. DL does not use GD, \n",
    "it uses SGD which is very different. SGD is covered in the CNN-Intro to Tensorflow section. \n",
    "</p>\n",
    "<p>Model Evaluation and Validation : This presentation uses linear regression for both regression and classification. \n",
    "    They present a numerical dataset and not a categorical one. \n",
    "    They present an exercise in filling out Confusion matrix for regression. There is no mention of AUC/ROC.  \n",
    "    Logistic regression more popular here. Doesn't seem apparant the presenter knows the difference. \n",
    "    with a probability interpretation. Logistic Regression is important to develop to explain softmax and to understand\n",
    "    the FC layer at the end of CNNs. \n",
    "    They cover accuracy vs. errors. \n",
    "    They present over and underfitting and k-fold cross validation as a way to fix.  \n",
    "</p>\n",
    "    \n",
    "<p>Miniflow. Did not look at this. Code up a python version of tf. If the strategy is to train someone to become an open \n",
    "source contributor we should focus on gradients and how to calculate them especially in a distributed\n",
    "environement. </p>\n",
    "\n",
    "<h6>Corrections for Section2 CNN </h6>\n",
    "There are 10 modules here, 1) Intro to TF, 2) Cloud Computing, 3) Deep Neural Networks, 4) Convolutional Networks, \n",
    "5) Siraj's Image Classification, \n",
    "6) Weight Initialization, 7) Image Classification, 8) Siraj's Image Generation, \n",
    "9) AutoEncoders, 10) Transfer Learning in tf.\n",
    "I did not comment on all of the sections above. Only selected ones. \n",
    "<h6>Section 1: Intro to TF</h6>\n",
    "<p>There are 29 sections here. </p>\n",
    "<h6>Section 3: Deep Neural Networks</h6>\n",
    "<p>There are 13 sections in this module. All of these lectures are from Vincent. Udacity does the intro lecture\n",
    "and adds in some sample code sections. </p>\n",
    "<p>Module 3 is a mix between Udacity doing an intro lecture and Vincent</p> \n",
    "<p>Module 3.2-3.3 cover a 2 layer network and RELUs. They don't cover why this solves the vanishing\n",
    "gradient problem with logistic/sigmoids. The goal of this module seems to be programming NNs in TF. \n",
    "</p>\n",
    "<p>Module 3.4  no lecture, covers TF code to classify MNIST digits. My preference is to start with a simpler example\n",
    "using XOR gates or IRIS b/c it allows for debugging and separates the formatting of convolutional layers\n",
    "away from NNs. The key is to focus first on a simple numerical example both with regression and classification. These\n",
    "are common interview questions. </p>\n",
    "<p>Vincent Module 3.5. Training a DNN and how adding layers for CNNS increase the capacity and is more efficient\n",
    "in terms of parameter efficiency and the deeper layers capture hierarchial/neighboring image effects. Initial layers\n",
    "in a CNN capture lines, higher layers capture partial shapes and next layers capture representations of  objects.</p>\n",
    "<img src=\"vincent35.png\" /> Note to self: add resnets and skip layers and show deep vanishing gradients,\n",
    "<p>Module 3.6 no lecture covers model saving and restoring. Add details on graph/saving restoring. The simplifiction\n",
    "is they only cover saving and restoring variables, specifically initial weights and biases. They cover saving\n",
    "the final model and then restoring the final model and running an accuracy operation on test data. Saving variables is \n",
    "never used. They fail to explain the difference between graphs/models/variables and when/why you would do this.</p>\n",
    "<p>>Module 3.7 Finetuning, unclear if this is relevant or usable. Goal is to finetune a model. They present to do this \n",
    "you have to name the weights and biases with names and not let TF create variable names. If tf saves and restores \n",
    "a session there is no way for tf to know Variable0 is a saved variable so it fails to load a valid value. Udacity added\n",
    "content. </p>\n",
    "<p>Vincent Module 3.8 Regularization Intro. Make model bigger than necessary and take steps to not overfit. Hard\n",
    "to design exactly size of NN for data. Skinny Jeans problem, hard to get network to exactly fit like jeans. Try bigger\n",
    "pants and then prevent overfitting. </p>\n",
    "<p>Vincent Module 3.9 - Regularization. 1) Early termination still best way, 2) regularization, adding \n",
    "artificial constraints. L2 regulartization,add another term to prevent large weights. Some of the value of the\n",
    "weights go into the regularization term. </p>\n",
    "<p>Vincent Module 3.10 derivative of regularizatoin term beta*1/2*norm weight^2 = w. </p>\n",
    "<p>Vincent Module 3.11 - dropout. Dropout some of the activations. Set 1/2 to 0. Random. Hintons idea. The network\n",
    "can not rely on any given activation so it has to add some redundancies. IN practice this makes things more robust\n",
    "makes the network seem like ensemble. If dropout not working then you should probably using a bigger network. </p>\n",
    "<p>Vincent Module 3.12 - Dropout trick. Not only do you zero out 1/2 the activatoins but you scale the remaining\n",
    "ones by a factor of 2. 2x the ones not zeroed out. </p><img src=\"vincentdropout2.png\">\n",
    "<p>No mention if dropout is as good as early termination although most code samples seem to prefer dropout\n",
    "over early termination. </p>\n",
    "\n",
    "<h6>Section 4: Convolutional Networks</h6>\n",
    "<p>There are 35 sections in this module. </p>\n",
    "Intro to CNNs, Color, Statistical Invariance, Convolutional Networks, Intuition, Filters, Feature Map Sizes, Convolutions\n",
    "continued, Parameteres, Convolution Output Shape(2), Number of parameters(2), parameter sharing, Visualizing CNNs, \n",
    "Tensorflow Convolution Layer, Explore Design Space, TF Max Pooling, Pooling intuition(2), Pooling mechanics(2), Pooling\n",
    "practice(2), Average Pooling(2),1x1 convolutions, Inception module, Convolution Network in TF, TF convolution Layer(2), \n",
    "TF pooling layer(2), additional resources. \n",
    "\n",
    "    \n",
    "<h6>Section 4: Weight Initialization</h6>    \n",
    "<p>There are 6 sections here presented by Udacity. This material is covered in Intro to TF by Vincent. There\n",
    "are gaps which we can fill in from the Hinton Coursera material. Weight histograms and how to debug if weights\n",
    "are not small enough. </p>\n",
    "<h6>Section 9: Siraj's Image Generation</h6>  \n",
    "<p>Siraj covers VAEs. There is no mention of generative models, calls autoencoders unsupervised models. Says AE is \n",
    "used for compression but lossy. Says VAEs are bayesian inference. All points of uncertainity are expressed w/probability. \n",
    "He presents Bayes rule and describes prior, posterior and marginal probabilities. There is no mention of what the \n",
    "variables actually are in his presentation. No mention of a posterior, prior, likelihood in his verbal presentation. \n",
    "At any time there is evidence for and against something. When you learn something\n",
    "new you have to fold this new evidence into what you already know. You create a new probability. Siraj quote for his VAE\n",
    "code which says he uses relu to squash the dimensionality. This is an inaccurate statement. The relu is a substibute\n",
    "for a logistic which creates a probablity distribution between 0-1. Squashing is not a technical term. </p>\n",
    "<img src=\"siraj_imagegen1.png\">\n",
    "<img src=\"siraj_imagegen2.png\">\n",
    "<img src=\"siraj_imagegen3.png\">\n",
    "<h6>Module 4: RNN</h6>\n",
    "<p>There are 12 sections in this module, Intro to RNN, Siraj's Stock predictin, Hyperparameters, Embeddings and\n",
    "Word2Vec, Siraj's Style transfer, Q/A with Floydhub, Sentiment prediction RNN, Siraj's text summaratin, generate\n",
    "TV Scripts, Sequence to Sequence, Siraj;s chatbot, Translation project </p>\n",
    "<p></p>\n",
    "<h6>Corrections to Udacity Lesson11</h6>\n",
    "\n",
    "\n",
    "The png below lists links listing chatbots and machine traslation. Those are 2 fundamentally different implementations\n",
    "A chatbot closely resembles a QA system while a ML application is a different architecture, similiar to performing\n",
    "text alignment across language models. \n",
    "The links also include a dynamic memory network and attention which they do not cover in the lectures.\n",
    "<img src=\"UdacityCorrectionDeepLearningLesson12.png\">\n",
    "\n",
    "<h6>Corrections to Udacity Section4 Recurrent Neural Networks</h6>\n",
    "<p>Missing an overview of sequence to sequence architectures</p>\n",
    "<p>Missing an explanation of how LSTM solves vanishing/exploding gradient problem. </p>\n",
    "<h6>Lesson 1 section 2: vanishing and exploding gradients when values are less than or greater than one. \n",
    "    </h6><p>This is not accurate. These are matricies so you have to talk about singular values of matricies not\n",
    "    numbers like they are scalars. </p>\n",
    "<><>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
