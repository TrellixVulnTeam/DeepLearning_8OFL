{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Hi I am here'\n"
     ]
    }
   ],
   "source": [
    "#Tensorflow tutorial\n",
    "#Tensorflow executes code under a Tensorflow session\n",
    "\n",
    "\n",
    "sess = tf.Session()\n",
    "print(sess.run(tf.constant(\"Hi I am here\")))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 200 0.5\n"
     ]
    }
   ],
   "source": [
    "#Tensorflow is very close to the numpy api. Tensorflow converts everything to tensors which are multidimensional\n",
    "#arrays. Tensors refer to the subscipt below the variable indicating multiple dimensions. Tensors are a notation for\n",
    "#matrices. \n",
    "\n",
    "\n",
    "a = tf.constant(10)\n",
    "b = tf.constant(20)\n",
    "result_add = sess.run(tf.add(a,b))\n",
    "result_multiply = sess.run(tf.multiply(a,b))\n",
    "result_divide = sess.run(tf.divide(a,b))\n",
    "print(result_add,result_multiply,result_divide)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Tensorflow was designed as a graph. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0\n",
      "Hypo:\n",
      "w1:[[-0.79593647  0.93947881]\n",
      " [ 0.68854761 -0.89423609]]\n",
      "b1:[[-0.79593647  0.93947881]\n",
      " [ 0.68854761 -0.89423609]]\n",
      "w2:[[-0.79084051  0.93289936]\n",
      " [ 0.69278169 -0.8986907 ]]\n",
      "b2:[ 0.00394399 -0.00394398]\n",
      "cost (ce):2.87031\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (0,) and (50,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-ebaac5e1aaf9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0minput_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mXOR_X\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mXOR_T\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0manalyze_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mXOR_X\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mXOR_T\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-ebaac5e1aaf9>\u001b[0m in \u001b[0;36manalyze_classifier\u001b[0;34m(sess, i, w1, b1, w2, b2, XOR_X, XOR_Y)\u001b[0m\n\u001b[1;32m     28\u001b[0m                 \u001b[0mxs_p\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                 \u001b[0mys_p\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs_p\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mys_p\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'ro'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxs_n\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxs_p\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'bo'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/tf35/lib/python3.5/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   3315\u001b[0m                       mplDeprecation)\n\u001b[1;32m   3316\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3317\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3318\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3319\u001b[0m         \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwashold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/tf35/lib/python3.5/site-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1896\u001b[0m                     warnings.warn(msg % (label_namer, func.__name__),\n\u001b[1;32m   1897\u001b[0m                                   RuntimeWarning, stacklevel=2)\n\u001b[0;32m-> 1898\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1899\u001b[0m         \u001b[0mpre_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1900\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpre_doc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/tf35/lib/python3.5/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1404\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_alias_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1406\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1407\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1408\u001b[0m             \u001b[0mlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/tf35/lib/python3.5/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_grab_next_args\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    405\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mseg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m                     \u001b[0;32myield\u001b[0m \u001b[0mseg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/tf35/lib/python3.5/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs)\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex_of\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_xy_from_xy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommand\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'plot'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/tf35/lib/python3.5/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_xy_from_xy\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m             raise ValueError(\"x and y must have same first dimension, but \"\n\u001b[0;32m--> 244\u001b[0;31m                              \"have shapes {} and {}\".format(x.shape, y.shape))\n\u001b[0m\u001b[1;32m    245\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m             raise ValueError(\"x and y can be no greater than 2-D, but have \"\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (0,) and (50,)"
     ]
    }
   ],
   "source": [
    "def trans_for_one(labels):\n",
    "    return np.array(labels).reshape(len(labels),-1)\n",
    "\n",
    "def analyze_classifier(sess,i,w1,b1,w2,b2,XOR_X,XOR_Y):\n",
    "    print('Epoch:%i'%i)\n",
    "    print('Hypo:'% sess.run(hypothesis,feed_dict={input_:XOR_X, target:XOR_T}))\n",
    "    print('w1:%s' % sess.run(w1))\n",
    "    print('b1:%s' % sess.run(w1))\n",
    "    print('w2:%s' % sess.run(w2))\n",
    "    print('b2:%s' % sess.run(b2))\n",
    "    print('cost (ce):%s' % sess.run(cross_entropy, feed_dict={input_:XOR_X, target:XOR_T}))\n",
    "    #visualize classification boundary\n",
    "    xs = np.linspace(-5,5)\n",
    "    ys = np.linspace(-5,5)\n",
    "    \n",
    "    pred_classes = []\n",
    "    for x in xs:\n",
    "        for y in ys:\n",
    "            pred_class = sess.run(hypothesis,feed_dict={input_:[[x,y]]})\n",
    "            pred_classes.append((x,y,pred_class.argmax()))\n",
    "        xs_p, ys_p = [],[]\n",
    "        xs_n, ys_n = [],[] #negative and positive WRT classification boundary\n",
    "        for x,y,c in pred_classes:\n",
    "            if c==0:\n",
    "                xs_n.append(x)\n",
    "                ys_n.append(y)\n",
    "            else:\n",
    "                xs_p.append(x)\n",
    "                ys_p.append(y)\n",
    "        plt.plot(xs_p,ys_p,'ro',xs_n,xs_p,'bo')\n",
    "        plt.show()\n",
    "        \n",
    "\n",
    "XOR_X = [[0,0],[0,1],[1,0],[1,1]]\n",
    "XOR_Y = [0,1,1,0]\n",
    "\n",
    "\n",
    "enc = OneHotEncoder() #where is this from, there should be a tf.onehotencoder\n",
    "enc.fit(trans_for_one(XOR_Y))\n",
    "XOR_T = enc.transform(trans_for_one(XOR_Y)).toarray()\n",
    "\n",
    "nb_classes=2\n",
    "input_ = tf.placeholder(tf.float32,shape=[None,len(XOR_X[0])],name=\"input\")\n",
    "target = tf.placeholder(tf.float32, shape=[None,nb_classes],name=\"output\")\n",
    "\n",
    "nb_hidden_nodes = 2\n",
    "\n",
    "\n",
    "\n",
    "w1 = tf.Variable(tf.random_uniform([2,nb_hidden_nodes],-1,1,seed=0),name=\"Weights1\")\n",
    "w2 = tf.Variable(tf.random_uniform([nb_hidden_nodes,nb_classes],-1,1,seed=0),name=\"Weights2\")\n",
    "                 \n",
    "b1 = tf.Variable(tf.zeros([nb_hidden_nodes]),name=\"Biases1\")\n",
    "b2 = tf.Variable(tf.zeros([nb_classes]),name=\"Biases2\")\n",
    "\n",
    "                 \n",
    "activation2 = tf.sigmoid(tf.matmul(input_,w1)+b1)\n",
    "hypothesis = tf.nn.softmax(tf.matmul(activation2,w2)+b2)\n",
    "cross_entropy = -tf.reduce_sum(target*tf.log(hypothesis))\n",
    "train_step = tf.train.GradientDescentOptimizer(0.1).minimize(cross_entropy)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init) \n",
    "    for i in range(20000):\n",
    "        sess.run(train_step,feed_dict={input_:XOR_X,target:XOR_T})\n",
    "        if(i%10000==0):\n",
    "            analyze_classifier(sess,i,w1,b1,w2,b2,XOR_X,XOR_T)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-31-040c137acf31>:80: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "\n",
      "Epoch 0\n",
      "Hypothesis [[ 0.48712057  0.51287943]\n",
      " [ 0.3380821   0.66191792]\n",
      " [ 0.65063184  0.34936813]\n",
      " [ 0.5031724   0.49682763]]\n",
      "w1=[[-0.79593647  0.93947881]\n",
      " [ 0.68854761 -0.89423609]]\n",
      "b1=[-0.00733338  0.00893857]\n",
      "w2=[[-0.79084051  0.93289936]\n",
      " [ 0.69278169 -0.8986907 ]]\n",
      "b2=[ 0.00394399 -0.00394398]\n",
      "cost (ce)=2.87031\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nD6CN84Qcx6DVAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11772ac88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10000\n",
      "Hypothesis [[ 0.99773693  0.00226305]\n",
      " [ 0.00290443  0.99709558]\n",
      " [ 0.00295531  0.99704474]\n",
      " [ 0.99804318  0.00195681]]\n",
      "w1=[[-6.62694883  7.52302551]\n",
      " [ 6.91208267 -7.39292049]]\n",
      "b1=[ 3.32245088  3.76204109]\n",
      "w2=[[ 6.63464451 -6.49259472]\n",
      " [ 6.40471601 -6.61061907]]\n",
      "b2=[-9.65064335  9.65065002]\n",
      "cost (ce)=0.0100926\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAGZtJREFUeJztnX+IZedZx7/Pzu6gQyP9Y1dasjv3VosLoYp1l9ISoSEp\nMW5Do+IfJmlhKXRwayGFlNI0f/gDgpRKqlClJPUP5Q6UohYhWGKDLSSipbsxqSZpS2xmsoktbhC0\n1D9Mdh//mLtkJ3PeO/ecOee8z/t9v1+47O7ZOfdznudM3p185sz7mLtDURRF4cmh3BegKIqi9Bst\n7IqiKGTRwq4oikIWLeyKoihk0cKuKIpCFi3siqIoZNHCriiKQhYt7IqiKGTRwq4oikKWwzmgR48e\n9el0mgOtKIpSbC5cuPCKux/b7+OyLOzT6RTnz5/PgVYURSk2Zra9zMdJxSiKopBFC7uiKApZtLAr\niqKQRQu7oigKWbSwK4qikKWchX1zE5hOgUOHdn7d3Fx8vMs5fR1nZ7PXp96Kvc85fSIGibuP/jp1\n6pS3ymzmvrbmDrz+WltzP3eu+fhs1v6cvo6zs9nrU2/F3uec2bnHe0PMZu2WQgDnl1ljLcdovNOn\nT3ur59inU2C74fHNlRXg8uW9xyeTnV/bnNPXcXY2e33qrdj7nDNduYjty8d7QUwmwNbW3uOpmNkF\ndz+978cVsbAfOrTzj9yyMdv5NUNt9Gz2+nKy2esjYR/CZXgLi70IYQZcubL0Wy29sJfh2NfXm4+v\nrKQ/vu05fR1nZ7PXp96Kfc05m7gTU7yAQ7iMKV7AJu7E+sp/9IZIHT9oyljYz5xpPn7yZPrj257T\n13F2Nnt96q3Y82ye/ANs4GFsYwrHIWxjig08jLeftL4QyeMHTRkqRo49Dpu9PvVW7HnaunQ5djn2\nctns9eVks9dXGLutS++AkGNvTIHOrng2e33qbZXsPly6HHvbyLHHYbPXp95Wx948M+vFpcuxy7GX\ny2avT72tjj3FVrbbKscuxx6DzV5fTjZ7fUHZh3Al222VY29KYGdHy2avT72lZTd5dKyvZ72tcuxN\nCersqNns9am3lOzUM+mbZ2ZZb6scuxx7DDZ7feotJTv1THru2yrHLsceg81eX042e30Z2aln0nPf\nVjn2plTsC7Ox2etTb4tnt3kmPfdtlWNvSqW+UB6YlM1e3wjstvu7jHVbHzjzBNbw413H1/BjPHDm\nieaTDpqDDMzo+mo9aGMy2b1D/dXXykrz8cmk/Tl9HWdns9en3hbNnqxcDFf2ZLKzhs1wp0/wghsu\n+wQv+Ax3zv9y+WDsQRtmtgLgPICX3f32RR8rx14wm72+nGz2+kZg97lXel8xA64gsYa1lOw5HPs9\nAJ7r8f1ejxx7HDZ7feptMeyh90rvs7yxJXsvC7uZHQfwfgBf7OP99kSOPQ6bvT71tgj2GHulty3j\nppMvpz362A+yL+Nr9nsB+CsApwDcBOCR/T5ejr1gNnt96m0R7LYufYyyJysX0x49BYnq2M3sdgBn\n3P2jZnYTgE94g2M3sw0AGwCwvr5+arvpaf1U5NjjsNnry8lmr69H9hh7pbd+L1zBFTT4mB4fZB/T\nsd8I4ANmtgXgSwBuNrPZGz/I3R9y99PufvrYsWPtCHLscdjs9am34di59kpv/V6Ja8rxIPuBF3Z3\nv8/dj7v7FMBvAfgHd//gga/s2sixx2Gz16fehmL35dL7LDvp0k/+ZXtIZMd+9QU5dn42e33qbSh2\nXy69z7KTLr0LJKpj7xI9x14wm72+nGz2+jqw+3LpHdDpc1IuvQsksGMfPnLscdjs9am32dhDuvQ+\ny0669ECbxZSxsMuxx2Gz16feZmEPPXe0S9mtXXoXSAmOfdmXHHvBbPb61Nss7Ihlt3bpcuxy7MWy\n2evLyWavbwF76LmjC9AAmstu7dI7QeTY9yawL6Rls9en3mZhRyy7tUsP5Nj3/ZJ+iFdrFTObua+u\n7v5fmNVV93Pnmo/PZu3P6es4O5u9PvV2UPbsyNndauPIWffZLHvZa6uv7vq7tdVXfXbu8eF7O5sN\nomIOD/PPxQBxb/5z6niXc/o6zs4eg1ErewxGJvYm7sKGvRv/O192tjHFhj0MzP+cq+y7sQn4Y7gf\nv4sXsY51vIgH/PdxN35ynN4OkDIcu4ZZx2Gz16feDsbIOVB6UdlbmBYzzVrDrIFB/0Wsls1eX042\neX05B0qnsnAIRp8QoJmhb542JOg3gqjZ7PWpt70wog2UXlQ24zTrMhZ2/YBSHDZ7fertgd8r4kDp\nTkMwxuitfkAJe19Bf9iCms1en3p74PcKOVC6yxCMUX4KatJqKYR+QAntzukr7Gz2+nKySeoLOVC6\nyxCM3uBy7M2RY4/DZq9PvW31XsUMlO4yBEOOfeDIscdhs9en3i59TlEDpbsMwZBjl2Ovhs1en3q7\n9DlFDZSOel/l2FtcJ4mrDMlmry8nu7D6ihooPQa8C0OOvSFErrIYNnt96m3jOcUPlI56X+XYG0Li\nKotis9en3u4JxUDpqPdVjj2TtCP3pCEZtbKD1kcxUDrqfZVjb3GdhbnKotjs9eVkB62PYqB0n/A+\nGXLsDSnQVRbPZq+v8t7SDpSOel/l2BtSmKukYLPXV3FvqQdKR72vcuzBvFmtbPb6Ku5txNb25tKj\n3lc59hbXGdRVUrDZ68vJ1kDpvX/Xl0vvAh+DIcfekMCukpbNXl8FvW3y6PQDpaPeVzn2hgR1ldRs\n9vrIe5t6Jn3zzCxrax8488SwLj3qfZVjD+bNamWz10fe29Qz6blb65PJsC496n2VY29xnewuNieb\nvb6cbM0dzQMH8t1XOfaG1Opio/pChvqIequ5o0vC5dgzRY49Dpu9PpLe5pw7mnr2fOPkN/LNHY16\nX6M6dgAnAHwdwLMAngFwz37nyLEXzGavj6S3OeeOLnr2PNvc0aj3NapjN7O3Aniruz9pZtcBuADg\n19z92dQ5cuwFs9nry8kmmTsack/0nOwSHbu7/8Ddn5z//kcAngNw/UHfd1fk2OOw2esrsLfR5o6G\n3BM96n0twbGb2RTAOwF8s8/3lWMPxGavr7De5pw7WtSe6FHva1THfvUF4E3Y0TC/kfj7DQDnAZxf\nX19v5ZXk2AOx2esrrLc5544WtSd61Psa1bEDgJkdAfAIgEfd/cH9Pl6OvWA2e3052YXNHS1qT/Sc\n7BIdu5kZgD8H8Nwyi3qnyLHHYbPXF7i30eaOFrUnetT7Gtix3wjgQwBuNrOn5q9+xZEcexw2e31B\ne5tz7ijFnuhB72t4x97mpefYC2az1xe0tznnjlLsiR70voZ27G0jx14wm72+nOygc0cp9kTPyS7R\nsY8SOfY4bPb6AvQ22txRij3RA9zX5N8NkDIWdjn2OGz2+jL3NufcUeo90aP+NyPHHsyb1cpmry9z\nb7UnOud9lWNvihx7HDZ7fTnZmeeOUu+JnpMtx56IHHscNnt9I7Ejzh1l6W04thx7InLscdjs9Y3A\nzjl3NPlMOvue6FH/m5FjD+bNamWz1zcCO+fc0eQz6SS9DcmWY09Ejj0Om72+Edg5544mn0kn6W1I\nthx7InLscdjs9fXMjjZ3NPlMeoG9LYYtx56IHHscNnt9PbIjzh1NPpNeWG+LYsuxJyLHHofNXl+P\n7KhzRxl6WxRbjj0ROfY4bPb6emRr7qjY+zLk2BtSq7OL6gsZ6uvI1tzRAxxnZ8uxJyLHHofNXl8H\ntuaOHvA4O1uOPRE59jhs9vo6sDV3lPO+yrG3jBx7wWz2+jqwNXdU7M4MOfaG1OrsovpChvr2YWvu\n6ADH2dly7InIscdhs9e3gD30XumaO0rKlmNPRI49Dpu9vgXsrCpWc0fLZcuxJyLHHofNXt8C9tB7\npWvuKClbjj0ROfY4bPb6Es+kj7FXuuaOkrLl2BORY4/DJq8v517pmjtKypZjT0SOPQ6bvL6se6VP\nXHNHGdly7InIscdhk9eXda90zR3lZMuxJyLHHodNVF+4vdI1d5STLceeiBx7HDZJfSH3StfcUU62\nHHsicuxx2CT1hdwrnaS3YrdgyLG3uE52Z5eTTVJfyL3SSXordguGHHtDanV2UX1h0PqK2Su9wN6K\nfUBGZMduZreZ2XfN7Hkz+1Qf77krcuxx2IXVV9Re6YX1VuweGFEdO4AVAP8O4GcArAJ4GsANi86R\nYy+YXVh9Re2VXlhvxe6BEdWxm9l7APyeu//K/M/3zf/B+MPUOXLsBbMLq6+ovdIL663YPTACO/br\nAVy85s8vzY/1Fzn2OOzA9RW/V3rg3oo9ECOyY18mZrZhZufN7PylS5fanSzHHocdtL6+XHqXsjV3\nVOzOjMCO/T0AHr3mz/cBuG/ROXLsBbOD1teXS++kSTV3VOzOnzyTVkshRnTshwF8D8AtAF4G8C0A\nd7n7M6lz5NgLZgetry+X3gGtuaNid2dEdezu/hqAjwF4FMBzAL68aFHvFDn2OOwA9Q3p0jtpUs0d\nFbsrI7Jjd/e/c/efc/efdfcH+njPXZFjj8Mmnzu6qOzB90pn/9yplV2iY+/ykmMvmE0+d3RR2YPv\nlc7+uVMru0TH3iVy7AWzyeeOLkAPv1c6++dOrewSHfsokWOPwyafO7qobJbeij0yu1THPnjk2OOw\nyeeOZt0rnf1zp1a2HHsicuxx2OxzR3Pulc7+uVMrW449ETn2OGz2uaM590pn/9yplS3Hnogcexw2\n+9zRnHuls3/u1MqWY09Ejj0Om33uaM690tk/d2ply7EnIsceh80+d5Skt2IHYsuxJyLHHofNPnd0\nDHhOhtjjs+XYE5Fjj8NmnztaYG/FDs6WY09Ejj0Om33uaGG9FbsAthx7InLscdjsc0cL663YBbDl\n2BORY4/DZp872ic8IkPs8dly7InIscdhs88dDdxbsQtly7EnIsceh80+dzRobyk+d2ply7EnIsce\nh80+dzRobyk+d2ply7EnIsceh80+d7QLvCSG2OOz5dgTkWOPw2afOxqgt4MyxB6fLceeiBx7HDb7\n3NGoLpbhc6dWthx7InLscdjsc0ejuliG+mply7EnIsceh80+d3QRHBi8t4MzxB6fLceeiBx7FnaV\nc0ejuliG+mply7EnIsc+OrvauaNRXSxDfbWy5dgTkWMfnV3t3NGoLpahvlrZcuyJyLGPzq527mgq\nuV3s0BE7D0OOvSG1Orue2Zo7usxFybGLPQBDjr0htTq7HtmaO7rk8dwulqG+Wtly7InIsQ/G1tzR\nQPeVvb5a2XLsicixD8bW3NFAbPb6amXLsScix94LW3NHD3A8t4tlqK9WdmmO3cw+a2bfMbNvm9lX\nzOzNfV3YrsixH5ituaMHPJ7bxTLUVyu7NMcO4FYAh+e//wyAzyxznhz7+GzNHS3gvrLXVyu7ZMdu\nZr8O4Dfd/e79PlaOfXy25o4WwGavr1Z24Y79wwC+2uP7vR459lZszR0d4HhuF8tQX63siI7dzB4z\ns39reN1xzcfcD+A1AJsL3mfDzM6b2flLly61u0o59qXZQ++VTj93NKqLZaivVnZpjn2ucc4C+CcA\na8ueI8c+HDurLmSYOxrVxTLUVyu7NMduZrcBeBDAe9196S/D5diHYw+9Vzr93NGcbPb6amUX6Ng/\nD+A6AF8zs6fM7AsHfL/myLHvOZ5rr3T6uaNRXSxDfbWyIzr2RXH3t7v7CXf/xfnrt/u6sF2RY9+V\nnHul088djepiGeqrlV2iY+/ykmM/2PGse6VPnHvuaFQXy1BfrezSHHvXyLEfLFn3SmefO5qTzV5f\nrewCHfs4kWPfjci5Vzr73NGoLpahvlrZGRz7vl/SD/FqrWJmM/fV1d3/C7O66n7uXPPx2az9OX0d\n75k9O3J2t/I4ctZn5x7vFb22+uqu42urryaPz2Yd7kfQ3oZjs9dXK3sRYzYbRMWU8RU7sNOKpj+n\njnc5p6/jPb3XJu7Chr3hm6T2MP4Rv9wb+kY8gYf8I5hgC4YrmGALD/lH8Gf4aOPxu6/+DFrhvQ3L\nHoMh9vjs/f6u55Th2KdTYHt77/GVFeDy5b3HJ5OdX9uc09fxHtnTlYvYvnx8UPRk5SVsXT7R4gSO\n3oZks9dXK3sRYzIBtrb2Hk9kWcdexsJe6TdPxxiCoWEXgdjs9dXK1jdPE6ngm6e5hmBo2EUgNnt9\ntbJL+wGl0UL+A0pjDMHQsIsC2Oz11crWDyglQv4DSmMMwdCwiwLY7PXVytYPKCVC7tjHGIKhYRcF\nsNnrq5Utx54IkWPPNQRDwy4KYLPXVytbjj0REsfel0vPukFX0N5SsNnrq5Utx54IiWPvy6Vn3aAr\naG8p2Oz11cqWY0+ExLH35dIXoQffoCtobynY7PXVypZjT6RAxz6kS1+ElgcumM1eX61sOfZECnPs\nQw+UTj6TfuYJeeCS2ez11cqWY0+kMMc+uLJLPZOe2xcy+NCoLpahvlrZcuyJFObYBx8onXomPbcv\nHDrsbPb6amXLsScS1LFnGyideiY9ty9k8KFRXSxDfbWy5dgTCejYxxgo3Xp/l9y+kMGHRnWxDPXV\nypZjTySgYx9joHTr/V1y+0IGH6reij0mQ469xXWSDJQOuVc6uw/NyWavr1a2HHsimR17m2fS+0SH\n3Cud3Yeqt2KPyZBjb8gI3qzt/i5d0EXtlc7uQ9VbscdkyLHn8WZt93fpgi5qr3R2H6reij0mQ469\nxXWyzx3tEx6RUSubvb5a2XLsiYzk2IuZOxrVFzL4UPVW7DEZcuwN6dGbFTV3NKovZPCh6q3YYzLk\n2If1ZkXNHY3qCxl8qHor9piMyI7dzO4F8EcAjrn7K/t9fETHXtTc0S7wkhi1stnrq5VdomM3sxMA\nbgXw4kHfK5meHXvxc0ej+kIGH6reij0mI7Bj/xyATwIY7p+8Hh07xdzRqL6QwYeqt2KPyYjo2AHc\nAeBP5r/fAnB0mfNyOnaKuaNRfSFDfeqt2GMycjl2M3sMwFsa/up+AJ8GcKu7/7eZbQE47QnHbmYb\nADYAYH19/dT29vby//r06Ngp5o4uggPDstl9aE42e321siM6dnd/n7u/440vAN8H8DYAT88X9eMA\nnjSzpn8E4O4Puftpdz997NixpQsB0Nmx084djeoLGepTb8UekxHNsbv7v7r7T7v71N2nAF4C8Evu\n/sPeru5qOjh26rmjUX0hQ33qrdhjMiI69mtfCObYqeeORvWFDPWpt2KPyYj8HHvbjPEcO/Xc0VRy\n+8Khw85mr69WdkTHHiItn0mnnzsa1Rcy1Kfeij0mI5pjHzUtn0mnnzsa1Rcy1Kfeij0mI7pjb/Pq\ny7FXO3c0qi9kqE+9FXtMhhz73uusdu5oTjZ7fTnZ7PXVypZjT6TlM+n0c0ej+kKG+tRbscdk1OzY\n2z6TTj93NKovZKhPvRV7TEbNjn2MkadFzR2N6gsZ6lNvxR6TUbNjH2PkaVFzR3Oy2evLyWavr1a2\nHHtzxhh5WtTc0ai+kKE+9VbsMRk1O/Y+R55SzB2N6gsZ6lNvxR6TIcfek9JimDsa1Rcy1Kfeij0m\nQ459+fennzuak81eX042e321suXYm9OnY6eYOxrVFzLUp96KPSZDjn1vFikt6rmjUX0hQ33qrdhj\nMuTY2ykt6rmjUX0hQ33qrdhjMuTYl39/+rmjOdns9eVks9dXK1uOvTldHDu1s4vqCxnqU2/FHpMx\nkGPf90v6IV5tVcxs5r62tvv/YNbW3M+daz4+m3U4qa/jsxk3m70+9VbsMRmz2SAqZt8PiLCw+3yd\nnkzczXZ+vdqP1PFOJ/V1nJ3NXp96K/aYjBZZdmEvwrEriqIoZI5dURRFWT5a2BVFUciihV1RFIUs\nWtgVRVHIooVdURSFLFmeijGzSwC2RwcfPEcBvJL7IkZMbfUCqrmWlFrzxN2P7fdBWRb2UmNm55d5\n1IgltdULqOZawl6zVIyiKApZtLAriqKQRQt7uzyU+wJGTm31Aqq5llDXLMeuKIpCFn3FriiKQhYt\n7B1iZveamZvZ0dzXMnTM7LNm9h0z+7aZfcXM3pz7moaKmd1mZt81s+fN7FO5r2fomNkJM/u6mT1r\nZs+Y2T25r2mMmNmKmf2LmT2S+1qGihb2ljGzEwBuBfBi7msZKV8D8A53/wUA3wNwX+brGSRmtgLg\nTwH8KoAbANxpZjfkvarB8xqAe939BgDvBvA7FdQMAPcAeC73RQwZLezt8zkAnwRQxTcn3P3v3f21\n+R//GcDxnNczYN4F4Hl3/767/x+ALwG4I/M1DRp3/4G7Pzn//Y+ws9hdn/eqho2ZHQfwfgBfzH0t\nQ0YLe4uY2R0AXnb3p3NfS6Z8GMBXc1/EQLkewMVr/vwSyBe5a2NmUwDvBPDNvFcyeP4YO1+YLT9o\ntMAczn0B0WJmjwF4S8Nf3Q/g09jRMFRZVLO7/+38Y+7Hzv+6b455bcrwMbM3AfhrAB939//JfT1D\nxcxuB/Cf7n7BzG7KfT1DRgv7G+Lu72s6bmY/D+BtAJ62nanjxwE8aWbvcvcfjniJvSdV89WY2VkA\ntwO4xXmfj30ZwIlr/nx8fow6ZnYEO4v6prv/Te7rGTg3AviAmZ0B8BMAfsrMZu7+wczX1Xv0HHvH\nmNkWgNPuXuJGQkvHzG4D8CCA97r7pdzXM1TM7DB2vjl8C3YW9G8BuMvdn8l6YQPGdr5C+QsA/+Xu\nH899PWNm/hX7J9z99tzXMkTk2JX98nkA1wH4mpk9ZWZfyH1BQ2T+DeKPAXgUO99E/DLzoj7PjQA+\nBODm+b19av7VrFJ49BW7oigKWfQVu6IoClm0sCuKopBFC7uiKApZtLAriqKQRQu7oigKWbSwK4qi\nkEULu6IoClm0sCuKopDl/wHvRmmnLOs5DwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11aae4f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 20000\n",
      "Hypothesis [[  9.98954773e-01   1.04520307e-03]\n",
      " [  1.35455513e-03   9.98645484e-01]\n",
      " [  1.37042650e-03   9.98629570e-01]\n",
      " [  9.99092221e-01   9.07784502e-04]]\n",
      "w1=[[-7.04857349  7.84673071]\n",
      " [ 7.33061361 -7.6883769 ]]\n",
      "b1=[ 3.53246331  3.89587522]\n",
      "w2=[[ 7.35947943 -7.21742964]\n",
      " [ 7.14059544 -7.34649324]]\n",
      "b2=[-10.74944305  10.7494421 ]\n",
      "cost (ce)=0.00468077\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nWtgVRVHI8v9pDGugnzvKEgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11b5d3128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "def trans_for_ohe(labels):\n",
    "    \"\"\"Transform a flat list of labels to what one hot encoder needs.\"\"\"\n",
    "    return np.array(labels).reshape(len(labels), -1)\n",
    "\n",
    "\n",
    "def analyze_classifier(sess, i, w1, b1, w2, b2, XOR_X, XOR_T):\n",
    "    \"\"\"Visualize the classification.\"\"\"\n",
    "    print('\\nEpoch %i' % i)\n",
    "    print('Hypothesis %s' % sess.run(hypothesis,\n",
    "                                     feed_dict={input_: XOR_X,\n",
    "                                                target: XOR_T}))\n",
    "    print('w1=%s' % sess.run(w1))\n",
    "    print('b1=%s' % sess.run(b1))\n",
    "    print('w2=%s' % sess.run(w2))\n",
    "    print('b2=%s' % sess.run(b2))\n",
    "    print('cost (ce)=%s' % sess.run(cross_entropy,\n",
    "                                    feed_dict={input_: XOR_X,\n",
    "                                               target: XOR_T}))\n",
    "    # Visualize classification boundary\n",
    "    xs = np.linspace(-5, 5)\n",
    "    ys = np.linspace(-5, 5)\n",
    "    pred_classes = []\n",
    "    for x in xs:\n",
    "        for y in ys:\n",
    "            pred_class = sess.run(hypothesis,\n",
    "                                  feed_dict={input_: [[x, y]]})\n",
    "            pred_classes.append((x, y, pred_class.argmax()))\n",
    "    xs_p, ys_p = [], []\n",
    "    xs_n, ys_n = [], []\n",
    "    for x, y, c in pred_classes:\n",
    "        if c == 0:\n",
    "            xs_n.append(x)\n",
    "            ys_n.append(y)\n",
    "        else:\n",
    "            xs_p.append(x)\n",
    "            ys_p.append(y)\n",
    "    plt.plot(xs_p, ys_p, 'ro', xs_n, ys_n, 'bo')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# The training data\n",
    "XOR_X = [[0, 0], [0, 1], [1, 0], [1, 1]]  # Features\n",
    "XOR_Y = [0, 1, 1, 0]  # Class labels\n",
    "assert len(XOR_X) == len(XOR_Y)  # sanity check\n",
    "\n",
    "# Transform labels to targets\n",
    "enc = OneHotEncoder()\n",
    "enc.fit(trans_for_ohe(XOR_Y))\n",
    "XOR_T = enc.transform(trans_for_ohe(XOR_Y)).toarray()\n",
    "\n",
    "# The network\n",
    "nb_classes = 2\n",
    "input_ = tf.placeholder(tf.float32,\n",
    "                        shape=[None, len(XOR_X[0])],\n",
    "                        name=\"input\")\n",
    "target = tf.placeholder(tf.float32,\n",
    "                        shape=[None, nb_classes],\n",
    "                        name=\"output\")\n",
    "nb_hidden_nodes = 2\n",
    "# enc = tf.one_hot([0, 1], 2)\n",
    "w1 = tf.Variable(tf.random_uniform([2, nb_hidden_nodes], -1, 1, seed=0),\n",
    "                 name=\"Weights1\")\n",
    "w2 = tf.Variable(tf.random_uniform([nb_hidden_nodes, nb_classes], -1, 1,\n",
    "                                   seed=0),\n",
    "                 name=\"Weights2\")\n",
    "b1 = tf.Variable(tf.zeros([nb_hidden_nodes]), name=\"Biases1\")\n",
    "b2 = tf.Variable(tf.zeros([nb_classes]), name=\"Biases2\")\n",
    "activation2 = tf.sigmoid(tf.matmul(input_, w1) + b1)\n",
    "hypothesis = tf.nn.softmax(tf.matmul(activation2, w2) + b2)\n",
    "cross_entropy = -tf.reduce_sum(target * tf.log(hypothesis))\n",
    "train_step = tf.train.GradientDescentOptimizer(0.1).minimize(cross_entropy)\n",
    "\n",
    "# Start training\n",
    "init = tf.initialize_all_variables()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for i in range(20001):\n",
    "        sess.run(train_step, feed_dict={input_: XOR_X, target: XOR_T})\n",
    "\n",
    "        if i % 10000 == 0:\n",
    "            analyze_classifier(sess, i, w1, b1, w2, b2, XOR_X, XOR_T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[4,2], name='x')\n",
    "y = tf.placeholder(tf.float32, shape=[4,1], name='y')\n",
    "\n",
    "\n",
    "#setup weights and bias z = w*x+b\n",
    "w0 = tf.Variable(tf.random_uniform([2,2], -1, 1), name=\"w0\")\n",
    "w1 = tf.Variable(tf.random_uniform([2,2], -1, 1), name=\"w1\")\n",
    "\n",
    "bias_0 = tf.Variable(tf.zeros([2]), name=\"bias_0\")\n",
    "bias_1 = tf.Variable(tf.zeros([2]), name=\"bias_1\")\n",
    "\n",
    "z = tf.sigmoid(tf.matmul(x,w0)+bias_0)\n",
    "hypothesis = tf.sigmoid(tf.matmul(z,w1)+bias_1)\n",
    "\n",
    "\n",
    "cost = tf.reduce_mean((y*tf.log(hypothesis))+(1-y)*tf.log(1. - hypothesis)*-1.)\n",
    "train = tf.train.GradientDescentOptimizer(0.01).minimize(cost)\n",
    "XOR_X = [[0,0],[0,1],[1,0],[1,1]]\n",
    "XOR_Y = [[0],[1],[1],[0]]\n",
    "\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0\n",
      "Hypothesis  [[ 0.555323    0.44973719]\n",
      " [ 0.58989638  0.44296476]\n",
      " [ 0.59176016  0.4688521 ]\n",
      " [ 0.6246109   0.46289268]]\n",
      "Theta1  [[-0.95617688  0.20052759]\n",
      " [ 0.23060793  0.76648295]]\n",
      "Bias1  [ 0.00093509 -0.0010025 ]\n",
      "Theta2  [[-0.46373156 -0.35478356]\n",
      " [ 0.91880393 -0.03853079]]\n",
      "Bias2  [-0.00499562 -0.00500202]\n",
      "cost  0.0481151\n",
      "Epoch  1000\n",
      "Hypothesis  [[ 0.01337254  0.00831926]\n",
      " [ 0.00960042  0.00501446]\n",
      " [ 0.01437561  0.00815793]\n",
      " [ 0.01007846  0.00495299]]\n",
      "Theta1  [[-0.34479943  0.38638481]\n",
      " [ 0.72448629  0.92102855]]\n",
      "Bias1  [ 1.10510647  0.3428013 ]\n",
      "Theta2  [[-1.88715601 -1.78145075]\n",
      " [-0.65029722 -1.6052649 ]]\n",
      "Bias2  [-2.50308824 -2.50370193]\n",
      "cost  -2.36943\n",
      "Epoch  2000\n",
      "Hypothesis  [[  1.62122233e-05   8.17689761e-06]\n",
      " [  8.81776032e-06   3.94791550e-06]\n",
      " [  1.04886931e-05   4.77578487e-06]\n",
      " [  7.68110749e-06   3.33546177e-06]]\n",
      "Theta1  [[ 0.45592567  1.05010498]\n",
      " [ 1.13563025  1.40735805]]\n",
      "Bias1  [ 2.31787252  1.49381948]\n",
      "Theta2  [[-4.11799812 -4.01232767]\n",
      " [-2.78916597 -3.74424314]]\n",
      "Bias2  [-5.00314903 -5.00384235]\n",
      "cost  -5.97477\n",
      "Epoch  3000\n",
      "Hypothesis  [[  9.84051240e-09   4.58653071e-09]\n",
      " [  5.21143484e-09   2.26395902e-09]\n",
      " [  5.64063640e-09   2.46058085e-09]\n",
      " [  4.61427607e-09   1.97901873e-09]]\n",
      "Theta1  [[ 0.88084257  1.4757148 ]\n",
      " [ 1.38014483  1.72919905]]\n",
      "Bias1  [ 2.98730612  2.2412734 ]\n",
      "Theta2  [[-6.54993582 -6.44426537]\n",
      " [-5.19753885 -6.1526165 ]]\n",
      "Bias2  [-7.50320625 -7.50389957]\n",
      "cost  -9.72434\n",
      "Epoch  4000\n",
      "Hypothesis  [[  5.46615574e-12   2.46846966e-12]\n",
      " [  2.94145611e-12   1.26729736e-12]\n",
      " [  3.07878831e-12   1.32855196e-12]\n",
      " [  2.66154351e-12   1.13897032e-12]]\n",
      "Theta1  [[ 1.15034497  1.74711823]\n",
      " [ 1.5538733   1.94722164]]\n",
      "Bias1  [ 3.43053699  2.73070383]\n",
      "Theta2  [[-9.0203743  -8.91470528]\n",
      " [-7.66075039 -8.61582565]]\n",
      "Bias2  [-10.00288105 -10.00357437]\n",
      "cost  -13.475\n",
      "Epoch  5000\n",
      "Hypothesis  [[  2.98574455e-15   1.32697484e-15]\n",
      " [  1.64211811e-15   7.05159746e-16]\n",
      " [  1.69230036e-15   7.27263653e-16]\n",
      " [  1.50995354e-15   6.45720003e-16]]\n",
      "Theta1  [[ 1.34606016  1.94303012]\n",
      " [ 1.6893692   2.11076927]]\n",
      "Bias1  [ 3.76174426  3.09016418]\n",
      "Theta2  [[-11.50387669 -11.39820766]\n",
      " [-10.1411953  -11.09627438]]\n",
      "Bias2  [-12.50246143 -12.50315475]\n",
      "cost  -17.2251\n",
      "Epoch  6000\n",
      "Hypothesis  [[  1.62538219e-18   7.15646060e-19]\n",
      " [  9.13027174e-19   3.91434791e-19]\n",
      " [  9.32933483e-19   4.00141487e-19]\n",
      " [  8.49523330e-19   3.63190941e-19]]\n",
      "Theta1  [[ 1.49930537  2.09569359]\n",
      " [ 1.80073512  2.24151063]]\n",
      "Bias1  [ 4.02635479  3.37356853]\n",
      "Theta2  [[-13.99332428 -13.88765526]\n",
      " [-12.62910366 -13.58418274]]\n",
      "Bias2  [-15.00204182 -15.00273514]\n",
      "cost  -20.9751\n",
      "Epoch  7000\n",
      "Hypothesis  [[  8.84472563e-22   3.87061280e-22]\n",
      " [  5.06470360e-22   2.16926323e-22]\n",
      " [  5.14815221e-22   2.20559375e-22]\n",
      " [  4.75476432e-22   2.03241530e-22]]\n",
      "Theta1  [[ 1.62506747  2.22056055]\n",
      " [ 1.89540875  2.35041642]]\n",
      "Bias1  [ 4.2467885   3.60734057]\n",
      "Theta2  [[-16.4858551  -16.38022614]\n",
      " [-15.12089157 -16.0759716 ]]\n",
      "Bias2  [-17.50219536 -17.50288963]\n",
      "cost  -24.7251\n",
      "Epoch  8000\n",
      "Hypothesis  [[  4.81331962e-25   2.09800812e-25]\n",
      " [  2.80407388e-25   1.20058233e-25]\n",
      " [  2.84041941e-25   1.21637581e-25]\n",
      " [  2.65089681e-25   1.13328534e-25]]\n",
      "Theta1  [[ 1.7316215   2.32611966]\n",
      " [ 1.97781587  2.44375324]]\n",
      "Bias1  [ 4.43575096  3.80623507]\n",
      "Theta2  [[-18.98061371 -18.87498474]\n",
      " [-17.61521149 -18.57007599]]\n",
      "Bias2  [-20.00272942 -20.00342369]\n",
      "cost  -28.4756\n",
      "Epoch  9000\n",
      "Hypothesis  [[  2.62365946e-28   1.14009564e-28]\n",
      " [  1.55221516e-28   6.64342553e-29]\n",
      " [  1.56853621e-28   6.71414989e-29]\n",
      " [  1.47564102e-28   6.30817830e-29]]\n",
      "Theta1  [[ 1.82400978  2.4175055 ]\n",
      " [ 2.05081201  2.5254302 ]]\n",
      "Bias1  [ 4.60113573  3.97929692]\n",
      "Theta2  [[-21.47662354 -21.37099457]\n",
      " [-20.11056137 -21.06542587]]\n",
      "Bias2  [-22.50326347 -22.50395775]\n",
      "cost  -32.2259\n",
      "Epoch  10000\n",
      "Hypothesis  [[  1.43144437e-31   6.20594132e-32]\n",
      " [  8.58715550e-32   3.67431851e-32]\n",
      " [  8.66216850e-32   3.70672630e-32]\n",
      " [  8.20092568e-32   3.50571148e-32]]\n",
      "Theta1  [[ 1.9055289   2.49805284]\n",
      " [ 2.11635232  2.59804821]]\n",
      "Bias1  [ 4.74819708  4.1324625 ]\n",
      "Theta2  [[-23.9733429  -23.86771393]\n",
      " [-22.60728073 -23.56214523]]\n",
      "Bias2  [-25.00379753 -25.00449181]\n",
      "cost  -35.9763\n",
      "Epoch  11000\n",
      "Hypothesis  [[  7.82838199e-35   3.38786828e-35]\n",
      " [  4.75562139e-35   2.03447307e-35]\n",
      " [  4.79072665e-35   2.04964760e-35]\n",
      " [  4.55904865e-35   1.94886130e-35]]\n",
      "Theta1  [[ 1.97844541  2.57004738]\n",
      " [ 2.17583585  2.66342211]]\n",
      "Bias1  [ 4.88059807  4.26983023]\n",
      "Theta2  [[-26.47006226 -26.36443329]\n",
      " [-25.10400009 -26.05886459]]\n",
      "Bias2  [-27.50433159 -27.50502586]\n",
      "cost  -39.726\n",
      "Epoch  12000\n",
      "Hypothesis  [[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "Theta1  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias1  [ nan  nan]\n",
      "Theta2  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias2  [ nan  nan]\n",
      "cost  nan\n",
      "Epoch  13000\n",
      "Hypothesis  [[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "Theta1  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias1  [ nan  nan]\n",
      "Theta2  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias2  [ nan  nan]\n",
      "cost  nan\n",
      "Epoch  14000\n",
      "Hypothesis  [[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "Theta1  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias1  [ nan  nan]\n",
      "Theta2  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias2  [ nan  nan]\n",
      "cost  nan\n",
      "Epoch  15000\n",
      "Hypothesis  [[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "Theta1  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias1  [ nan  nan]\n",
      "Theta2  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias2  [ nan  nan]\n",
      "cost  nan\n",
      "Epoch  16000\n",
      "Hypothesis  [[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "Theta1  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias1  [ nan  nan]\n",
      "Theta2  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias2  [ nan  nan]\n",
      "cost  nan\n",
      "Epoch  17000\n",
      "Hypothesis  [[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "Theta1  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias1  [ nan  nan]\n",
      "Theta2  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias2  [ nan  nan]\n",
      "cost  nan\n",
      "Epoch  18000\n",
      "Hypothesis  [[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "Theta1  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias1  [ nan  nan]\n",
      "Theta2  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias2  [ nan  nan]\n",
      "cost  nan\n",
      "Epoch  19000\n",
      "Hypothesis  [[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "Theta1  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias1  [ nan  nan]\n",
      "Theta2  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias2  [ nan  nan]\n",
      "cost  nan\n",
      "Epoch  20000\n",
      "Hypothesis  [[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "Theta1  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias1  [ nan  nan]\n",
      "Theta2  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias2  [ nan  nan]\n",
      "cost  nan\n",
      "Epoch  21000\n",
      "Hypothesis  [[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "Theta1  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias1  [ nan  nan]\n",
      "Theta2  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias2  [ nan  nan]\n",
      "cost  nan\n",
      "Epoch  22000\n",
      "Hypothesis  [[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "Theta1  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias1  [ nan  nan]\n",
      "Theta2  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias2  [ nan  nan]\n",
      "cost  nan\n",
      "Epoch  23000\n",
      "Hypothesis  [[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "Theta1  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias1  [ nan  nan]\n",
      "Theta2  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias2  [ nan  nan]\n",
      "cost  nan\n",
      "Epoch  24000\n",
      "Hypothesis  [[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "Theta1  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias1  [ nan  nan]\n",
      "Theta2  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias2  [ nan  nan]\n",
      "cost  nan\n",
      "Epoch  25000\n",
      "Hypothesis  [[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "Theta1  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias1  [ nan  nan]\n",
      "Theta2  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias2  [ nan  nan]\n",
      "cost  nan\n",
      "Epoch  26000\n",
      "Hypothesis  [[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "Theta1  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias1  [ nan  nan]\n",
      "Theta2  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias2  [ nan  nan]\n",
      "cost  nan\n",
      "Epoch  27000\n",
      "Hypothesis  [[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "Theta1  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias1  [ nan  nan]\n",
      "Theta2  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias2  [ nan  nan]\n",
      "cost  nan\n",
      "Epoch  28000\n",
      "Hypothesis  [[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "Theta1  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias1  [ nan  nan]\n",
      "Theta2  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias2  [ nan  nan]\n",
      "cost  nan\n",
      "Epoch  29000\n",
      "Hypothesis  [[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "Theta1  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias1  [ nan  nan]\n",
      "Theta2  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias2  [ nan  nan]\n",
      "cost  nan\n",
      "Epoch  30000\n",
      "Hypothesis  [[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "Theta1  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias1  [ nan  nan]\n",
      "Theta2  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias2  [ nan  nan]\n",
      "cost  nan\n",
      "Epoch  31000\n",
      "Hypothesis  [[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "Theta1  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias1  [ nan  nan]\n",
      "Theta2  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias2  [ nan  nan]\n",
      "cost  nan\n",
      "Epoch  32000\n",
      "Hypothesis  [[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "Theta1  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias1  [ nan  nan]\n",
      "Theta2  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias2  [ nan  nan]\n",
      "cost  nan\n",
      "Epoch  33000\n",
      "Hypothesis  [[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "Theta1  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias1  [ nan  nan]\n",
      "Theta2  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias2  [ nan  nan]\n",
      "cost  nan\n",
      "Epoch  34000\n",
      "Hypothesis  [[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "Theta1  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias1  [ nan  nan]\n",
      "Theta2  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias2  [ nan  nan]\n",
      "cost  nan\n",
      "Epoch  35000\n",
      "Hypothesis  [[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "Theta1  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias1  [ nan  nan]\n",
      "Theta2  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias2  [ nan  nan]\n",
      "cost  nan\n",
      "Epoch  36000\n",
      "Hypothesis  [[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "Theta1  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias1  [ nan  nan]\n",
      "Theta2  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias2  [ nan  nan]\n",
      "cost  nan\n",
      "Epoch  37000\n",
      "Hypothesis  [[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "Theta1  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias1  [ nan  nan]\n",
      "Theta2  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias2  [ nan  nan]\n",
      "cost  nan\n",
      "Epoch  38000\n",
      "Hypothesis  [[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "Theta1  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias1  [ nan  nan]\n",
      "Theta2  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias2  [ nan  nan]\n",
      "cost  nan\n",
      "Epoch  39000\n",
      "Hypothesis  [[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "Theta1  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias1  [ nan  nan]\n",
      "Theta2  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias2  [ nan  nan]\n",
      "cost  nan\n",
      "Epoch  40000\n",
      "Hypothesis  [[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "Theta1  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias1  [ nan  nan]\n",
      "Theta2  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias2  [ nan  nan]\n",
      "cost  nan\n",
      "Epoch  41000\n",
      "Hypothesis  [[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "Theta1  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias1  [ nan  nan]\n",
      "Theta2  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias2  [ nan  nan]\n",
      "cost  nan\n",
      "Epoch  42000\n",
      "Hypothesis  [[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "Theta1  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias1  [ nan  nan]\n",
      "Theta2  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias2  [ nan  nan]\n",
      "cost  nan\n",
      "Epoch  43000\n",
      "Hypothesis  [[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "Theta1  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias1  [ nan  nan]\n",
      "Theta2  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias2  [ nan  nan]\n",
      "cost  nan\n",
      "Epoch  44000\n",
      "Hypothesis  [[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "Theta1  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias1  [ nan  nan]\n",
      "Theta2  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias2  [ nan  nan]\n",
      "cost  nan\n",
      "Epoch  45000\n",
      "Hypothesis  [[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "Theta1  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias1  [ nan  nan]\n",
      "Theta2  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias2  [ nan  nan]\n",
      "cost  nan\n",
      "Epoch  46000\n",
      "Hypothesis  [[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "Theta1  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias1  [ nan  nan]\n",
      "Theta2  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias2  [ nan  nan]\n",
      "cost  nan\n",
      "Epoch  47000\n",
      "Hypothesis  [[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "Theta1  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias1  [ nan  nan]\n",
      "Theta2  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias2  [ nan  nan]\n",
      "cost  nan\n",
      "Epoch  48000\n",
      "Hypothesis  [[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "Theta1  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias1  [ nan  nan]\n",
      "Theta2  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias2  [ nan  nan]\n",
      "cost  nan\n",
      "Epoch  49000\n",
      "Hypothesis  [[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "Theta1  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias1  [ nan  nan]\n",
      "Theta2  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias2  [ nan  nan]\n",
      "cost  nan\n",
      "Epoch  50000\n",
      "Hypothesis  [[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "Theta1  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias1  [ nan  nan]\n",
      "Theta2  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias2  [ nan  nan]\n",
      "cost  nan\n",
      "Epoch  51000\n",
      "Hypothesis  [[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "Theta1  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias1  [ nan  nan]\n",
      "Theta2  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias2  [ nan  nan]\n",
      "cost  nan\n",
      "Epoch  52000\n",
      "Hypothesis  [[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "Theta1  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias1  [ nan  nan]\n",
      "Theta2  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias2  [ nan  nan]\n",
      "cost  nan\n",
      "Epoch  53000\n",
      "Hypothesis  [[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "Theta1  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias1  [ nan  nan]\n",
      "Theta2  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias2  [ nan  nan]\n",
      "cost  nan\n",
      "Epoch  54000\n",
      "Hypothesis  [[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "Theta1  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias1  [ nan  nan]\n",
      "Theta2  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias2  [ nan  nan]\n",
      "cost  nan\n",
      "Epoch  55000\n",
      "Hypothesis  [[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "Theta1  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias1  [ nan  nan]\n",
      "Theta2  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias2  [ nan  nan]\n",
      "cost  nan\n",
      "Epoch  56000\n",
      "Hypothesis  [[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "Theta1  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias1  [ nan  nan]\n",
      "Theta2  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias2  [ nan  nan]\n",
      "cost  nan\n",
      "Epoch  57000\n",
      "Hypothesis  [[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "Theta1  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias1  [ nan  nan]\n",
      "Theta2  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias2  [ nan  nan]\n",
      "cost  nan\n",
      "Epoch  58000\n",
      "Hypothesis  [[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "Theta1  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias1  [ nan  nan]\n",
      "Theta2  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias2  [ nan  nan]\n",
      "cost  nan\n",
      "Epoch  59000\n",
      "Hypothesis  [[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "Theta1  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias1  [ nan  nan]\n",
      "Theta2  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias2  [ nan  nan]\n",
      "cost  nan\n",
      "Epoch  60000\n",
      "Hypothesis  [[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "Theta1  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias1  [ nan  nan]\n",
      "Theta2  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias2  [ nan  nan]\n",
      "cost  nan\n",
      "Epoch  61000\n",
      "Hypothesis  [[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "Theta1  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias1  [ nan  nan]\n",
      "Theta2  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias2  [ nan  nan]\n",
      "cost  nan\n",
      "Epoch  62000\n",
      "Hypothesis  [[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "Theta1  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias1  [ nan  nan]\n",
      "Theta2  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias2  [ nan  nan]\n",
      "cost  nan\n",
      "Epoch  63000\n",
      "Hypothesis  [[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "Theta1  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias1  [ nan  nan]\n",
      "Theta2  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias2  [ nan  nan]\n",
      "cost  nan\n",
      "Epoch  64000\n",
      "Hypothesis  [[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "Theta1  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias1  [ nan  nan]\n",
      "Theta2  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias2  [ nan  nan]\n",
      "cost  nan\n",
      "Epoch  65000\n",
      "Hypothesis  [[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "Theta1  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias1  [ nan  nan]\n",
      "Theta2  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias2  [ nan  nan]\n",
      "cost  nan\n",
      "Epoch  66000\n",
      "Hypothesis  [[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "Theta1  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias1  [ nan  nan]\n",
      "Theta2  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias2  [ nan  nan]\n",
      "cost  nan\n",
      "Epoch  67000\n",
      "Hypothesis  [[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "Theta1  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias1  [ nan  nan]\n",
      "Theta2  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias2  [ nan  nan]\n",
      "cost  nan\n",
      "Epoch  68000\n",
      "Hypothesis  [[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "Theta1  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias1  [ nan  nan]\n",
      "Theta2  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias2  [ nan  nan]\n",
      "cost  nan\n",
      "Epoch  69000\n",
      "Hypothesis  [[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "Theta1  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias1  [ nan  nan]\n",
      "Theta2  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias2  [ nan  nan]\n",
      "cost  nan\n",
      "Epoch  70000\n",
      "Hypothesis  [[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "Theta1  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias1  [ nan  nan]\n",
      "Theta2  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias2  [ nan  nan]\n",
      "cost  nan\n",
      "Epoch  71000\n",
      "Hypothesis  [[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "Theta1  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias1  [ nan  nan]\n",
      "Theta2  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias2  [ nan  nan]\n",
      "cost  nan\n",
      "Epoch  72000\n",
      "Hypothesis  [[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "Theta1  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias1  [ nan  nan]\n",
      "Theta2  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias2  [ nan  nan]\n",
      "cost  nan\n",
      "Epoch  73000\n",
      "Hypothesis  [[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "Theta1  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias1  [ nan  nan]\n",
      "Theta2  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias2  [ nan  nan]\n",
      "cost  nan\n",
      "Epoch  74000\n",
      "Hypothesis  [[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "Theta1  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias1  [ nan  nan]\n",
      "Theta2  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias2  [ nan  nan]\n",
      "cost  nan\n",
      "Epoch  75000\n",
      "Hypothesis  [[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "Theta1  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias1  [ nan  nan]\n",
      "Theta2  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias2  [ nan  nan]\n",
      "cost  nan\n",
      "Epoch  76000\n",
      "Hypothesis  [[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "Theta1  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias1  [ nan  nan]\n",
      "Theta2  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias2  [ nan  nan]\n",
      "cost  nan\n",
      "Epoch  77000\n",
      "Hypothesis  [[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "Theta1  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias1  [ nan  nan]\n",
      "Theta2  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias2  [ nan  nan]\n",
      "cost  nan\n",
      "Epoch  78000\n",
      "Hypothesis  [[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "Theta1  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias1  [ nan  nan]\n",
      "Theta2  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias2  [ nan  nan]\n",
      "cost  nan\n",
      "Epoch  79000\n",
      "Hypothesis  [[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "Theta1  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias1  [ nan  nan]\n",
      "Theta2  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias2  [ nan  nan]\n",
      "cost  nan\n",
      "Epoch  80000\n",
      "Hypothesis  [[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "Theta1  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias1  [ nan  nan]\n",
      "Theta2  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias2  [ nan  nan]\n",
      "cost  nan\n",
      "Epoch  81000\n",
      "Hypothesis  [[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "Theta1  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias1  [ nan  nan]\n",
      "Theta2  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias2  [ nan  nan]\n",
      "cost  nan\n",
      "Epoch  82000\n",
      "Hypothesis  [[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "Theta1  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias1  [ nan  nan]\n",
      "Theta2  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias2  [ nan  nan]\n",
      "cost  nan\n",
      "Epoch  83000\n",
      "Hypothesis  [[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "Theta1  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias1  [ nan  nan]\n",
      "Theta2  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias2  [ nan  nan]\n",
      "cost  nan\n",
      "Epoch  84000\n",
      "Hypothesis  [[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "Theta1  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias1  [ nan  nan]\n",
      "Theta2  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias2  [ nan  nan]\n",
      "cost  nan\n",
      "Epoch  85000\n",
      "Hypothesis  [[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "Theta1  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias1  [ nan  nan]\n",
      "Theta2  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias2  [ nan  nan]\n",
      "cost  nan\n",
      "Epoch  86000\n",
      "Hypothesis  [[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "Theta1  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias1  [ nan  nan]\n",
      "Theta2  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias2  [ nan  nan]\n",
      "cost  nan\n",
      "Epoch  87000\n",
      "Hypothesis  [[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "Theta1  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias1  [ nan  nan]\n",
      "Theta2  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias2  [ nan  nan]\n",
      "cost  nan\n",
      "Epoch  88000\n",
      "Hypothesis  [[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "Theta1  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias1  [ nan  nan]\n",
      "Theta2  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias2  [ nan  nan]\n",
      "cost  nan\n",
      "Epoch  89000\n",
      "Hypothesis  [[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "Theta1  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias1  [ nan  nan]\n",
      "Theta2  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias2  [ nan  nan]\n",
      "cost  nan\n",
      "Epoch  90000\n",
      "Hypothesis  [[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "Theta1  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias1  [ nan  nan]\n",
      "Theta2  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias2  [ nan  nan]\n",
      "cost  nan\n",
      "Epoch  91000\n",
      "Hypothesis  [[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "Theta1  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias1  [ nan  nan]\n",
      "Theta2  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias2  [ nan  nan]\n",
      "cost  nan\n",
      "Epoch  92000\n",
      "Hypothesis  [[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "Theta1  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias1  [ nan  nan]\n",
      "Theta2  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias2  [ nan  nan]\n",
      "cost  nan\n",
      "Epoch  93000\n",
      "Hypothesis  [[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "Theta1  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias1  [ nan  nan]\n",
      "Theta2  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias2  [ nan  nan]\n",
      "cost  nan\n",
      "Epoch  94000\n",
      "Hypothesis  [[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "Theta1  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias1  [ nan  nan]\n",
      "Theta2  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias2  [ nan  nan]\n",
      "cost  nan\n",
      "Epoch  95000\n",
      "Hypothesis  [[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "Theta1  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias1  [ nan  nan]\n",
      "Theta2  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias2  [ nan  nan]\n",
      "cost  nan\n",
      "Epoch  96000\n",
      "Hypothesis  [[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "Theta1  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias1  [ nan  nan]\n",
      "Theta2  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias2  [ nan  nan]\n",
      "cost  nan\n",
      "Epoch  97000\n",
      "Hypothesis  [[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "Theta1  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias1  [ nan  nan]\n",
      "Theta2  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias2  [ nan  nan]\n",
      "cost  nan\n",
      "Epoch  98000\n",
      "Hypothesis  [[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "Theta1  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias1  [ nan  nan]\n",
      "Theta2  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias2  [ nan  nan]\n",
      "cost  nan\n",
      "Epoch  99000\n",
      "Hypothesis  [[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "Theta1  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias1  [ nan  nan]\n",
      "Theta2  [[ nan  nan]\n",
      " [ nan  nan]]\n",
      "Bias2  [ nan  nan]\n",
      "cost  nan\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(100000):\n",
    "    sess.run(train, feed_dict={x: XOR_X, y: XOR_Y})\n",
    "    if i % 1000 == 0:\n",
    "        print('Epoch ', i)\n",
    "        print('Hypothesis ', sess.run(hypothesis, feed_dict={x: XOR_X, y: XOR_Y}))\n",
    "        print('Theta1 ', sess.run(w0))\n",
    "        print('Bias1 ', sess.run(bias_0))\n",
    "        print('Theta2 ', sess.run(w1))\n",
    "        print('Bias2 ', sess.run(bias_1))\n",
    "        print('cost ', sess.run(cost, feed_dict={x: XOR_X, y: XOR_Y}))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python [conda env:tf35]",
   "language": "python",
   "name": "conda-env-tf35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
