{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>NLP1</h6>\n",
    "Improve on document term occurence matrix/one hot encoded vectors for word/document representations\n",
    "with embeddings. The advantage of embeddings over matrix representations followed by dimensionality reduction\n",
    "is embeddings encode word similarity directly without the dimensionsionality reduction step. \n",
    "<img src=\"embed1.png\">\n",
    "If you have the following sentences:\n",
    "<li>Mathematician can run</li>\n",
    "<li>Mathematician likes coffee</li>\n",
    "<li>Mathematician majored in physics</li>\n",
    "<li>Physicst can run</li>\n",
    "<li>Physicst likes coffee</li>\n",
    "<li>Physicst majored in physics</li>\n",
    "We can give each word a vector, a vector for mathematician and one for physicist and we can see the words\n",
    "surrouding these 2 words are similar. \n",
    "<img src=\"embed2.png\">\n",
    "\n",
    "We can use a dot product and cos between the 2 vectors to represent similarity. This concept of word vectors\n",
    "doesn't scale so we use word embeddings which is a mini DL vector. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2823\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "\n",
    "vocab = set()\n",
    "with open('howard.txt','r') as fh:\n",
    "    for x in fh:\n",
    "        #print(x)\n",
    "        vocab = vocab | set(x.split())\n",
    "\n",
    "print (len(vocab))\n",
    "index_to_word= {}\n",
    "word_to_index = {}\n",
    "#create index to word and word to index mappings\n",
    "for index,word in enumerate(vocab):\n",
    "    index_to_word[word] = i\n",
    "    word_to_index[index] = word\n",
    "\n",
    "def return_2gram(vocab):\n",
    "    d = []\n",
    "    for idx in range(0,2,len()-2):\n",
    "        before = [vocab[idx-2], vocab[idx-1],vocab[idx+1], vocab[idx+2]]\n",
    "        word = vocab[idx]\n",
    "        d.append((before, word))\n",
    "    return d\n",
    "\n",
    "data = return_2gram(vocab)\n",
    "#how to pick embedding size?\n",
    "embeddings = nn.embeddings(len(vocab), embedding_dim)\n",
    "linear1 = nn.Linear(embedding_dim,128)\n",
    "linear2 = nn.Linear(128,len(vocab))\n",
    "\n",
    "activation_function1 = nn.ReLU()\n",
    "activation_function2 = nn.LogSoftmax()\n",
    "\n",
    "\n",
    "def make_context_vector(context,word_to_index):\n",
    "    indexes = [word_to_index[x] for x in context]\n",
    "    tensor = torch.LongTensor(indexes)\n",
    "    return Variable(tensor)\n",
    "    \n",
    "def get_index_of_max(input):\n",
    "    index = 0\n",
    "    for i in range(0,len(input)):\n",
    "        if input[i] > input[index]:\n",
    "            index = i\n",
    "    return index\n",
    "\n",
    "def get_max_prob_result(input, index_to_word):\n",
    "    return index_to_word[get_index_of_max(input)]\n",
    "    \n",
    "\n",
    "def get_sim(input,index_to_word):\n",
    "    return index_to_word[get_index_of_max(input)]\n",
    "\n",
    "def forward(inputs):\n",
    "    embeds = sum(embeddings(inputs)).view(1,-1)\n",
    "    out = linear1(embeds)\n",
    "    out = activation_function1(out)\n",
    "    out = linear2(out)\n",
    "    out = activation_function2(out)\n",
    "    return out\n",
    "\n",
    "\n",
    "def get_word_embedding(word):\n",
    "    word = Variable(torch.LongTensor(word_to_index[word]))\n",
    "    return embeddings(word).view(1,-1)\n",
    "\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
