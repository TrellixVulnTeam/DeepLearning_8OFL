{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Random Variables</h6>\n",
    "<p>\n",
    "A random variable is a variable representing a real world event. We use a random variable to describe a variation\n",
    "of the event properties. A dice throw, a sensor measurement. When we measure again the values are different. If\n",
    "this is the case then we can try to rationalize the event as a type of random variable. \n",
    "</p>\n",
    "<p>\n",
    "What are the types of random variables? \n",
    "</p>\n",
    "Start with a coin. HTHTHTTTTH. This is pretty useless since categorical variables are not numbers. Convert\n",
    "to a numerical number first. \n",
    "Our event space or the events when represented by a 0 or 1 becomes this sequence of numbers: {0101011110}. This assignment\n",
    "    of 0 and 1 is arbitrary. We can do -200 and 1000. We would get different values for the mean and variance if we did \n",
    "    this. \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Bias Variance Tradeoff</h6>\n",
    "https://www.cs.rit.edu/~rlaz/prec20092/slides/Bagging_and_Boosting.pdf\n",
    "    \n",
    "Given a data set x and a set of values y, y=f(x)+AWGN. This models a sensor. \n",
    "\n",
    "We want to model f(x) with a given set of weights, h(x) = wx+b and want to minimize the squared error between h(x)\n",
    "and y over a training set. \n",
    "\n",
    "<p>Mimimize this squared error:</p>\n",
    "$\\sum_{i}^{training set}[y_i - h(x_i)]^2$\n",
    "<p>Given new data point we want to minimize the expected prediction error:</p>\n",
    "$E[(y* - h(x*))^2]$\n",
    "<p> where y* is the prediction from the new data point x*</p>\n",
    "<p></p>\n",
    "<p>Before we do this we need a lemma from expectation:</p>\n",
    "$E[(Z-{\\bar Z})^2] = E[Z^2-2Z{\\bar Z}+{\\bar Z}^2]$\n",
    "<p></p>\n",
    "$E[{\\bar Z}]=E[Z]$\n",
    "<p></p>\n",
    "$=E[Z^2] - 2E[Z]{\\bar Z}+{\\bar Z}^2 $\n",
    "<p></p>\n",
    "$=E[Z^2] -2{\\bar Z}^2 +{\\bar Z}^2$ \n",
    "<p> </p>\n",
    "$=E[Z^2] -{\\bar Z}^2$\n",
    "<p>OK back to our expected predicted error</p>\n",
    "$E[(y^* - h(x^*))^2]=E([{y^*}^2]- 2E[{y^*}h(x^*)]+[h({x^*})^2])$\n",
    "<p></p>\n",
    "$E([(y^*)^2]=E((y^*-{\\bar y^*})^2)+{\\bar y^*}^2=E[(y^*-f(x^*))^2]+f(x^*)^2$\n",
    "<p></p>\n",
    "$E[h(x^*)^2]=E((h(x^*)-{\\bar h(x^*)})^2)+{\\bar h(x^*)^2}$\n",
    "<p></p>\n",
    "$=E[(y^*-f(x^*))^2]+f(x^*)^2 - 2E[{y^*}h(x^*)] + E((h(x^*)-{\\bar h(x^*)})^2)+{\\bar h(x^*)^2}$\n",
    "<p></p>\n",
    "$=E((h(x^*)-{\\bar h(x^*)})^2)$\n",
    "<p></p>\n",
    "$+[{\\bar h(x^*)}^2-f(x^*)]^2$\n",
    "<p></p>\n",
    "$+E[(y^*-f(x^*))^2]$<p>the first term is variance, the second bias and the third noise</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Bagging is using an ensemble of models to reduce variance. Works for unstable classifiers like Decision trees and Neural\n",
    "networks. Bias means the classifier cannot learn the target class. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Boosting is applying weights to each model to make the most probable one the more likely. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
