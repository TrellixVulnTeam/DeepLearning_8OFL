{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#soruce activate twitter. \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import math,random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>pytorch tensors</h6>\n",
    "pytorch is built on tensors which are numpy arrays accelerated by the GPU. The tensor operations are \n",
    "GPU kernels. To move a tensor onto a GPU, use if <br/>\n",
    "torch.cuda.is_available():<br/>\n",
    "    &nbsp;&nbsp;&nbsp;device = torch.device(\"cuda\")<br/>\n",
    "    &nbsp;&nbsp;&nbsp;y = torch.ones_like(x,device=device)<br/>\n",
    "    &nbsp;&nbsp;&nbsp;x = x.to(device)<br/>\n",
    "    &nbsp;&nbsp;&nbsp;z = x + y<br/>\n",
    "    &nbsp;&nbsp;&nbsp;print(z)<br/>\n",
    "    &nbsp;&nbsp;&nbsp;print(z.to(\"cpu\",torch.double))<br/>\n",
    "#\n",
    "<h6>Variables</h6>\n",
    "For autodiff wrap tensors in pytorch variables from cs230. Conflicting information, you can add requires_grad to a tensor\n",
    "but you also need a Variable.grad_fn attribute refrencing a function that has created a Tensor? Could be if a \n",
    "tensor has a grad_fn as a result of an operation is is now a Variable? Not clear. An operation on a tensor results \n",
    "in a new function object; torch.autograd.Function. \n",
    "<h6>Autograd</h6>\n",
    "Backpropagation in pytorch is defined by defining the function in the forward direction then calling\n",
    ".backward(). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.]])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-1ae912aed614>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y' is not defined"
     ]
    }
   ],
   "source": [
    "#temp experiment code\n",
    "x = torch.ones(100,1, requires_grad=True)\n",
    "print(x)\n",
    "print (y.grad_fn,x.grad_fn)\n",
    "y = (x**3)\n",
    "print(x.requires_grad,y.requires_grad)\n",
    "\n",
    "print (y.grad_fn,x.grad_fn)\n",
    "y.sum().backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#autodiff requires the tensor reduce to 1 scalar. The point is the last node\n",
    "# of the simplest NN has a sum node of all the hideen outputs. We have this in NN archicture as last node\n",
    "#where y=x but is\n",
    "# really y = sum(x). This is what sum() represents. Sum the outputs of the hidden layer outpts. \n",
    "\n",
    "#Else you have to add a gradient to each member of the tensoras mentioned in pytorch docs, dont know how to do yet.\n",
    "\n",
    "a = torch.randn(2, 2,requires_grad=True)\n",
    "c = ((a * 3) / (a - 1))\n",
    "b = (c * c).sum()\n",
    "print(a.size())\n",
    "print(b.grad_fn) #should be tensor object here\n",
    "b.backward() # this is the d(output) step. \n",
    "print(a.grad) # this is the da to get d(output)/da\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#temp experiment code\n",
    "import torch\n",
    "x = [0,1]\n",
    "y = np.array([[1,1]])\n",
    "print(\"x:\",torch.Tensor([x]))\n",
    "print(\"y:\",torch.Tensor(y))\n",
    "a = np.array([[1,1],[2,2],[3,3]])\n",
    "for x in a:\n",
    "    print(x)\n",
    "#this does seem better than a lambda bc less typing. dont need the map or list conversion\n",
    "b = [torch.Tensor(x) for x in a]\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 inp: tensor([ 0.,  0.])\n",
      "loss:0.13435368239879608\n",
      "epoch: 0 inp: tensor([ 0.,  1.])\n",
      "loss:0.06609690934419632\n",
      "epoch: 0 inp: tensor([ 1.,  0.])\n",
      "loss:0.028277065604925156\n",
      "epoch: 0 inp: tensor([ 1.,  1.])\n",
      "loss:0.7900764346122742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dc/miniconda3/envs/twitter/lib/python3.6/site-packages/ipykernel_launcher.py:39: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1000 inp: tensor([ 0.,  0.])\n",
      "loss:0.0\n",
      "epoch: 1000 inp: tensor([ 0.,  1.])\n",
      "loss:5.083849828224629e-06\n",
      "epoch: 1000 inp: tensor([ 1.,  0.])\n",
      "loss:1.1643376637948677e-05\n",
      "epoch: 1000 inp: tensor([ 1.,  1.])\n",
      "loss:2.932440474978648e-05\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Simple(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Simple,self).__init__()\n",
    "        self.fc1 = nn.Linear(2,50)\n",
    "        self.fc2 = nn.Linear(50,1)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = F.sigmoid(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return x\n",
    "    \n",
    "x = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "output = np.array([[0],[0],[0],[1]])\n",
    "input = [Variable(torch.Tensor(i)) for i in x]\n",
    "target = [Variable(torch.Tensor(i)) for i in output]\n",
    "#print(input,target)\n",
    "net = Simple()\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(net.parameters(),lr=0.01)\n",
    "for e in range(0,2000):\n",
    "    for inp,t in zip(input,target):\n",
    "        optimizer.zero_grad()\n",
    "        out_predicted = net(inp)\n",
    "        loss = criterion(out_predicted,t)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (e%1000==0):\n",
    "            print(\"epoch:\",e,\"inp:\",inp)\n",
    "            #print(\"input:\",input[i])\n",
    "            #print(\"out_predicted:\",out_predicted)\n",
    "            #print(\"target:\",target[i])\n",
    "            print(\"loss:{}\".format(loss.data[0]))\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loop:\n",
      "Epoch        0 loss:0.033160094171762466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dc/miniconda3/envs/twitter/lib/python3.6/site-packages/ipykernel_launcher.py:48: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch     5000 loss:0.25002312660217285\n",
      "Epoch    10000 loss:0.00021975017443764955\n",
      "Epoch    15000 loss:6.284395226430206e-11\n",
      "Epoch    20000 loss:6.004086117172847e-11\n",
      "Epoch    25000 loss:6.004086117172847e-11\n",
      "Epoch    30000 loss:6.004086117172847e-11\n",
      "Epoch    35000 loss:6.004086117172847e-11\n",
      "Epoch    40000 loss:6.004086117172847e-11\n",
      "Epoch    45000 loss:6.004086117172847e-11\n",
      "\n",
      "Final results:\n",
      "Input:[0,0] Target:[0] Predicted:[0.0] Error:[0.0]\n",
      "Input:[0,1] Target:[1] Predicted:[1.0] Error:[0.0]\n",
      "Input:[1,0] Target:[1] Predicted:[1.0] Error:[0.0]\n",
      "Input:[1,1] Target:[0] Predicted:[0.0] Error:[0.0]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "EPOCHS_TO_TRAIN = 50000\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(2, 3, True)\n",
    "        self.fc2 = nn.Linear(3, 1, True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.sigmoid(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "inputs = list(map(lambda s: Variable(torch.Tensor([s])), [\n",
    "    [0, 0],\n",
    "    [0, 1],\n",
    "    [1, 0],\n",
    "    [1, 1]\n",
    "]))\n",
    "targets = list(map(lambda s: Variable(torch.Tensor([s])), [\n",
    "    [0],\n",
    "    [1],\n",
    "    [1],\n",
    "    [0]\n",
    "]))\n",
    "\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "print(\"Training loop:\")\n",
    "for idx in range(0, EPOCHS_TO_TRAIN):\n",
    "    for input, target in zip(inputs, targets):\n",
    "        optimizer.zero_grad()   # zero the gradient buffers\n",
    "        output = net(input)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()    # Does the update\n",
    "    if idx % 5000 == 0:\n",
    "        print(\"Epoch {: >8} loss:{}\".format(idx,loss.data[0]))\n",
    "\n",
    "\n",
    "\n",
    "print(\"\")\n",
    "print(\"Final results:\")\n",
    "for input, target in zip(inputs, targets):\n",
    "    output = net(input)\n",
    "    print(\"Input:[{},{}] Target:[{}] Predicted:[{}] Error:[{}]\".format(\n",
    "        int(input.data.numpy()[0][0]),\n",
    "        int(input.data.numpy()[0][1]),\n",
    "        int(target.data.numpy()[0]),\n",
    "        round(float(output.data.numpy()[0]), 4),\n",
    "        round(float(abs(target.data.numpy()[0] - output.data.numpy()[0])), 4)\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wrong? dont see output change to 1. \n",
    "class XOR(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(XOR,self).__init__()\n",
    "        self.fc1 = nn.Linear(2,50)\n",
    "        self.fc2 = nn.Linear(50,1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        return x\n",
    "\n",
    "net = XOR()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(list(net.parameters()), lr=0.001, momentum=0.1)\n",
    "    \n",
    "trainingdataX = [[[0.01, 0.01], [0.01, 0.90], [0.90, 0.01], [0.95, 0.95]], [[0.02, 0.03], [0.04, 0.95], [0.97, 0.02], [0.96, 0.95]]]\n",
    "trainingdataY = [[[0.01], [0.90], [0.90], [0.01]], [[0.04], [0.97], [0.98], [0.1]]]\n",
    "num_epochs=200\n",
    "for epochs in range(num_epochs):\n",
    "    running_loss=0.0\n",
    "    for i , data in enumerate(trainingdataX,0):\n",
    "        inputs = data\n",
    "        labels = trainingdataY[i]\n",
    "        print(\"i:\",i,\" data:\",data,\" labels:\",labels)\n",
    "        inputs = Variable(torch.FloatTensor(inputs))\n",
    "        labels = Variable(torch.FloatTensor(labels))\n",
    "        print(\"inputs size:\",inputs.size(), \"labels.size():\",labels.size())\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss+=loss.data[0]\n",
    "            \n",
    "        print (\"epochs:\",epochs,\"loss: \", running_loss,\"inputs:\",inputs,\"outputs:\",outputs)\n",
    "        #running_loss = 0.0\n",
    "print (\"Finished training...\")\n",
    "# this is correct for a prediction? \n",
    "print (net(Variable(torch.FloatTensor(trainingdataX[0]))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"rnn.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "np.random.seed(2)\n",
    "\n",
    "T = 20\n",
    "L = 1000\n",
    "N = 100\n",
    "\n",
    "x = np.empty((N, L), 'int64')\n",
    "x[:] = np.array(range(L)) + np.random.randint(-4 * T, 4 * T, N).reshape(N, 1)\n",
    "data = np.sin(x / 1.0 / T).astype('float64')\n",
    "torch.save(data, open('traindata.pt', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class SineWave(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SineWave,self).__init__()\n",
    "        self.lstm1 = nn.LSTMCell(1,51)\n",
    "        self.lstm2 = nn.LSTMCell(51,51)\n",
    "        self.linear = nn.Linear(51,1)\n",
    "        \n",
    "    def forward(self,input,future=0):\n",
    "        outputs = []\n",
    "        print('input size:',input.size())\n",
    "        h_t = torch.zeros(input.size(0), 51,dtype=torch.double )\n",
    "        c_t = torch.zeros(input.size(0), 51,dtype=torch.double )\n",
    "        h_t2 = torch.zeros(input.size(0), 51,dtype=torch.double )\n",
    "        c_t2 = torch.zeros(input.size(0), 51,dtype=torch.double )\n",
    "        print('h_t',h_t.size())\n",
    "        print('c_t',c_t.size())\n",
    "        print('h_t2',h_t2.size())\n",
    "        print('c_t2',c_t2.size())\n",
    "        \n",
    "        for i, input_t in enumerate(input.chunk(input.size(1),dim=1)):\n",
    "            h_t, c_t = self.lstm1(input_t, (h_t,c_t))\n",
    "            h_t2, c_t2 = self.lstm2(h_t, (h_t2,c_t2))\n",
    "            output = self.linear(h_t2)\n",
    "            outputs += [output]\n",
    "        for i in range(future):# if we should predict the future\n",
    "            h_t, c_t = self.lstm1(output, (h_t, c_t))\n",
    "            h_t2, c_t2 = self.lstm2(h_t, (h_t2, c_t2))\n",
    "            output = self.linear(h_t2)\n",
    "            outputs += [output]\n",
    "        outputs = torch.stack(outputs,1).squeeze(2)\n",
    "        \n",
    "        return outputs\n",
    "       \n",
    "if __name__ == '__main__':\n",
    "    data = torch.load('traindata.pt')\n",
    "    print('after load:',data.shape)\n",
    "    input = torch.from_numpy(data[3:,:-1])\n",
    "    target = torch.from_numpy(data[3:,1:])\n",
    "    test_input = torch.from_numpy(data[:3, :-1])\n",
    "    test_target = torch.from_numpy(data[:3,1:])\n",
    "    print('input:',input.size())\n",
    "    print('target:',target.size())\n",
    "    print('test_input:',test_input.size())\n",
    "    print('test_target:',test_target.size())\n",
    "    \n",
    "    seq = SineWave()\n",
    "    seq.double()\n",
    "    criterion = nn.MSELoss()\n",
    "    for i in range(15):\n",
    "        print(\"i:\",i)\n",
    "        def closure():\n",
    "            optimizer.zero_grad()\n",
    "            out = seq(input)\n",
    "            loss = criterion(out,target)\n",
    "            print(\"loss:\",loss.item())\n",
    "            loss.backward()\n",
    "            return loss\n",
    "        #predict\n",
    "        with torch.no_grad():\n",
    "            future = 1000\n",
    "            pred = seq(test_input,future=future)\n",
    "            loss = criterion(pred[:,:-future],test_target)\n",
    "            print('testloss:',loss.item())\n",
    "            y = pred.detach().numpy()\n",
    "    print(\"y:\",y.shape)\n",
    "    plt.figure(figsize=(30,10))\n",
    "    plt.title('sine wave')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.xticks(fontsize=20)\n",
    "    plt.yticks(fontsize=20)\n",
    "    def draw(yi, color):\n",
    "        plt.plot(np.arange(input.size(1)), yi[:input.size(1)],color,linewidth=2.0)\n",
    "        plt.plot(np.arange(input.size(1), input.size(1) + future), yi[input.size(1):], color+\":\", linewidth=2.0)\n",
    "    draw(y[0],'r')\n",
    "    draw(y[1],'g')\n",
    "    draw(y[2],'b')\n",
    "    #plt.savefig(\"predict%d.pdf\"%i)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rnn attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRANSFORMER FOR translaton doesnt use attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stack overflow tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text summarization as topics. text summarization in tensorflow blog post\n",
    "# vs running topic LDA only\n",
    "# how to develop quality metric? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
