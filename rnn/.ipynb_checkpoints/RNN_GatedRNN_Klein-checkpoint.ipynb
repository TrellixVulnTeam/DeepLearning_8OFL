{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h6>RNN and Gated RNN</h6>\n",
    "<p>Intro to RNNs</p>\n",
    "<p>Source: INDBA lecture Stephan Gouws and Richard Klein</p>\n",
    "<p>Slides and Video: http://www.deeplearningindaba.com/videos.html?utm_content=bufferfd4cc&utm_medium=social&utm_source=twitter.com&utm_campaign=buffer</p>\n",
    "\n",
    "<p>What a mesmerizing lecture!!!!</p>\n",
    "\n",
    "When looking at CNNs for image classificaiton what happens if given an image\n",
    "we want to match a patttern in an image, is there an efficient way to limit weights to section of image? \n",
    "Looking at idea of sequence data. RNNs and various gates and architectures. \n",
    "\n",
    "1) What is an RNN\n",
    "2) Gated ARchitectures\n",
    "\n",
    "Recurrent Models. \n",
    "<img src=\"rnn1.png\">\n",
    "We saw where we feed image into a FF network. We feed it an input and we get an output. What do we do if the image\n",
    "is 3x as wide? Do we train a new network that is 3x as large or do we feed in sequence of 3 data images? We need to be able to remember some state from previous \n",
    "time steps if we get a sequence of images. RNN intuition is we are telling someone something about state. Each time sequence\n",
    "requires some state information to be passed along. \n",
    "<img src=\"rnn2.png\">\n",
    "Long term dependencies important in language modeling. \n",
    "<img src=\"rnn3.png\">\n",
    "We have to remember the state at the beginning of the sentence using RNNs. Can do with convolution but not as efficient. \n",
    "There are different types of sequence models, a single input to many outputs, or many inputs to a singput output. \n",
    "In the one to many mapping: One input image in the case of image captioning we want NN to tell us there is a straw hat. Other type of model where we \n",
    "Get a sequence of outputs which are words in the captioning. \n",
    "<img src=\"rnn4.png\">\n",
    "<img src=\"rnn5.png\">\n",
    "In the many to one sequence example we input a sequence of pixels from an image and it outputs a predicted class!! This \n",
    "is an alternative way to classify MNIST digits. We pick an area of the image and move it around. \n",
    "The pixel sequence is only part of the image!\n",
    "In the many to one model we input a sequence and get a binary output like one for postitive or negative\n",
    "sentiment. \n",
    "Input a sentence and get a yes/no answer on classification like \n",
    "sentiment. \n",
    "<img src=\"rnn6.png\">\n",
    "<img src=\"rnn7.png\">\n",
    "Many inputs to many outputs. Get inputs and outputs at same time. Every time step you get new input and new output. For sequence\n",
    "labeling like NER, named entity recognition, machine translation are all many to many applications.  \n",
    "\n",
    "<img src=\"rnn8.png\">\n",
    "Classifiy the following examples, FFN vs RNN. For FFN there is now way for the model to say 192 can only calssify\n",
    "an input at one time. RNN give some input then the input updates the state inside. For an RNN we need to remember what the\n",
    "previous state was and process with new input. Every time step we need the previous state. \n",
    "\n",
    "<img src=\"rnn9.png\">\n",
    "Have Xt input coming in and have previous state which wraps around from t-1. When you unroll it you get:\n",
    "the following graph. We unroll the RNN to make it look like a Feed Forward network. \n",
    "<img src=\"rnn10.png\">\n",
    "There are 2 hiddden states, h0 is from the past. h1 is the current hidden layer. There are 2 sets of weights, one multipled by \n",
    "h0 and one by the input to get h1. You have one network and 1 set of parameters which you repeat multiple times. If you have \n",
    "a 100 items in a sequence you have to unroll 100 times. This has implications on gradient. \n",
    "The same parameters at every step. \n",
    "\n",
    "<img src=\"rnn11.png\">\n",
    "There are 3 sets of weights, Wxh and Whh which form the next state based on a multiplication wiht teh previous state\n",
    "and teh current input. \n",
    "The new output vector is created with the mulitplication with a 3rd set of weights; Why which is not on the image.\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"rnn12.png\">\n",
    "\n",
    "<img src=\"rnn13.png\">\n",
    "<img src=\"rnn14.png\">\n",
    "<img src=\"rnn15.png\">\n",
    "\n",
    "When we are running forward propagation we are inputting the entire sequence. A whole sequnece of \n",
    "pixels or a whole sequence of images. \n",
    "Equation for RNNs vs FFNs. We apply Chain rule and BP differently through the states. \n",
    "How to train? In echo state networks we initialize Wxh, Whh, Who then only train Who.(verify this claim w/the \n",
    " ESN code) BPTT, backprob through time, \n",
    "propagate error backwards through time. The problem in the image above is we can't fit the complete unrolled\n",
    "rnn into memory so we backprop sections at a time.\n",
    "<img src=\"rnn16.png\">\n",
    "From above we can see we keep multiplying stages until we get to the start. This introduces exploding or vanishing\n",
    "gradients as we keep on adding stages. If the singular value of the input*weight matrix is >1 we have an exploding gradient\n",
    "if the singular value is <1 we get a vanishing gradient.\n",
    "<img src=\"rnn16.png\">\n",
    "Back prop for RNNS requires us to go all the way back to the initial state for each training example.\n",
    "To do back prop the idea is to save the previous states in memory for speed.  We have a worked out example in the backprop\n",
    "section. \n",
    "<img src=\"rnn17.png\">\n",
    "Truncated BPTT is an alternative to full back prop w/gradient descent. This requires doing a foward pass with a sequence\n",
    "and then calculating the loss and doing  a gradient update with all the training data for each iteration! This is not \n",
    "practical so we simplify the proceure by using smaller segments of the sequence. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "<h6>Code Examples</h6>\n",
    "Basic RNN: char rnn\n",
    "Many to One: sentiment analysis\n",
    "Many-to-Many: image captioning\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "<h6>Image Captioning</h6>\n",
    "Source: cs231n Videos: LEcture 10\n",
    "<p>There were several image captioning systems built as listed in the papers below. </p>\n",
    "<img src=\"imagecap1.png\">\n",
    "<p>There are 2 parts to the system,\n",
    "1) which is a CNN where the images are fed into and will produce a summary vector of the image. \n",
    "2) The RNN which generates the captions. The CNN output summary vector is fed into the first\n",
    "timestep of the trained RNN. </p>\n",
    "<img src=\"imagecap2.png\">\n",
    "We take the imagenet CNN model and remove teh last FC1000 and softmax layers. This produces the summary vector. \n",
    "We add a START token to the 4096 bit summary vector to tell the RNN to start generating text. \n",
    "We add a separate weight matrix for the image and multiply this w/the summary vector at every timestep. \n",
    "<img src=\"imagecap3.png\">\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
