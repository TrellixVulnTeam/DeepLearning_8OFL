{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tf word2vec\n",
    "An embedding is a mapping from words represetnted as a list of integer tokens to a lookup table. \n",
    "We can learn this mapping using DL where the mapping is the mapping function. \n",
    "\n",
    "10s of thousands of words one hot encoded is waste of space bc of all the 0s. Embeddings are where you skip the\n",
    "matrix multiplication. You just look up the row of the W matrix. In the image below return the 4th row. \n",
    "<img src=\"embed1.png\">\n",
    "<img src=\"embed2.png\">\n",
    "Convert the words to integers, store the words in a lookup table where the index or row of the lookup table \n",
    "corresponds to the integer token. Token heart is integer 958, store this in row 958 of the LUT. The hidden layers \n",
    "are the rows of the LUT? \n",
    "\n",
    "<p>word2vec is an embedding. Use teh vectors from the embedding layer. Similar words will have similar \n",
    "vector values. There are 2 implementations, contiunous bag of words or skip grams. Look at words around a\n",
    "target word and predict the word from the words around it. The bag of worda model and skip-gram models\n",
    "are opposite. The BOW model you use the words around a target word to predict the target word. In SG you \n",
    "use the target word to predict the words around it. </p>\n",
    "<img src=\"embed3.png\">\n",
    "<p></p>\n",
    "\n",
    "Embeddings are described here: https://arxiv.org/pdf/1512.05287.pdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-4b87e756ac6e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mDL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import zipfile\n",
    "import utils\n",
    "\n",
    "class DL(tqdm):\n",
    "    last_block=0\n",
    "    \n",
    "    def hook(self,block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update()\n",
    "        self.last_block = block_num\n",
    "#end class\n",
    "\n",
    "if not isfile(\"text8.zip\"):\n",
    "    with DL(unit=\"B\", unit_scale=True, miniters=1, desc=\"Text 8 dataset\") as pbar:\n",
    "        urlretrieve(\"http://mattmahoney.net/dc/text8.zip\", \"text8.zip\", pbar.hook)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'utils' has no attribute 'preprocess'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-ab4c60dec429>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mwords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'utils' has no attribute 'preprocess'"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "\n",
    "words=utils.preprocess(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'this': 15, 'grey': 8, 'is': 1, 'and': 16, 'like': 13, 'september': 7, 'beautiful': 10, 'it': 14, 'things': 18, 'shining': 5, 'the': 3, 'in': 2, 'august': 11, 'i': 12, 'life': 9, 'sun': 4, 'june': 6, 'other': 17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dc/anaconda/envs/tf35/lib/python3.5/site-packages/keras/preprocessing/text.py:139: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
      "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(nb_words=3)\n",
    "\n",
    "tokenizer.fit_on_texts([\"The sun is shining in June!\",\"September is grey.\",\"Life is beautiful in August.\",\"I like it\",\"This and other things?\"])\n",
    "print(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences([\"June is beautiful and I like it!\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
