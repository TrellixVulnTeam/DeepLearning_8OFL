{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h6>CNN Architectures</h6>\n",
    "https://www.youtube.com/watch?v=6niqTuYFZLQ\n",
    "<p>Starting in 2012 we saw different CNN/DNN architectures win the imagenet competition. </p>\n",
    "<img src=\"cs231n1.png\">\n",
    "<p>Alexnet in 2012 started the CNN/DNN implementations to computer vision applications. \n",
    "</p>\n",
    "\n",
    "<img src=\"cs231n2.png\">\n",
    "<p>In 2014 had VGG and GoogleNet which were much deeper. VGG had a 16 and 19 layer model. GoogleNet a 22 layer model.  \n",
    "These models were invented before batch normalization. Was extremely difficult to get these models to converge. VGG\n",
    "first used a 11 layer model to converge then they added more layers and ran training with extra layers. GoogleNet used \n",
    "extra classifiers to injecct additional gradient into lower layers of the network. Both of these were hacks to \n",
    "get around small gradients not converging. With batch normalization you don't need these hacks to get these deeper models\n",
    "to converge. \n",
    "</p>\n",
    "<img src=\"cs231n3.png\">\n",
    "<p>In 2015 came resnet which had skip connections to inject more gradient signal by lettign input pass through the \n",
    "residual blocks. One output of the residual block goes to the output of the convolution layer. The pass \n",
    "through or skip layer does 2 things 1) if the weights are set to 0 in the residual block then block is computing identity function. Easy for \n",
    "this model to learn to ignore the layers it doesnt't need.\n",
    "and 2) get interpretation of L2 regularization. Putting L2 regularization onto the weights drives them all to 0.In a standard\n",
    "model driving to 0 doesn't mean anything. In this model driving to 0 means to not use layers it doesn't need. The skip connections\n",
    "allow the backprop gradient to flow through 2 paths, first through the conv layers and second through the skip connections. \n",
    "thia allows the network to train faster. \n",
    "</p>\n",
    "<img src=\"cs231n4.png\">\n",
    "<p>The other archtectures dense net and fractal net give other paths for the gradient to flow to allow\n",
    "faster training of deep layers.</p>\n",
    "<img src=\"cs231n5.png\">\n",
    "<p>Most of the 62M parameters for Alexnet are in the last Fully connected layers. 6*6*256*4096 Those are not efficient and\n",
    "dont exist in the later models. GoogleNet and resnet use global average pooling instead of FC.  </p>\n",
    "<img src=\"cs231n6.png\">\n",
    "<p>RNNs can also move around an image looking to classify digits instead of doing convolution on each pixel. \n",
    "</p>\n",
    "<img src=\"cs231n7.png\">\n",
    "<img src=\"cs231n8.png\">\n",
    "<img src=\"cs231n9.png\">\n",
    "\n",
    "The simplest  RNN contains 3 sets of weights, \n",
    "one at the input, one between the previous state and current state.\n",
    "and one betwen the current state and output. When we are reusing the same node then during back propagation the \n",
    "gradient losses for each time step get summed into the weight matrix from all the contributions. Similaryly for the loss\n",
    "you add all the losses from each time step to a total loss. \n",
    "\n",
    "<img src=\"cs231n10.png\">\n",
    "<img src=\"cs231n11.png\">\n",
    "<img src=\"cs231n12.png\">\n",
    "\n",
    "For the many to one where we have a sequence and only 1 output say a binary output for sentiment analysis then \n",
    "we only need an output at the final state. For the one to many case wehere you have a fixed siez input like an image\n",
    "you want a sequence of varying length like an image caption for an output. \n",
    "One example is machine translation where putting many different modules together in an encoder/decoder form is the network. The encoder is a many to one for \n",
    "translation and the decoder is a one to many output. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"../rnn_cs231n/cs231n12.png\">\n",
    "For character level modeling the output y is the probability of the next characater. The image displays the character\n",
    "with the highest probability. \n",
    "<img src=\"../rnn_cs231n/cs231n13.png\">\n",
    "You can generate data by sampling the probability distirbution from the output of a softmax or you can take\n",
    "the argmax. This generates data which may be similar; if image data this may be true. Text not always valid.\n",
    "<img src=\"../rnn_cs231n/cs231n14.png\">\n",
    "The original way to do back propagation via gradient descent is to take the set of training data,\n",
    "compute the gradient, sum up the loss and backpropagate back for 1 iteration. This isn't practical so we\n",
    "approximage using Truncated BPTT. BPTT looks as small segments of the sequence. This isn't SGD, there is \n",
    "no sampling for sequences. \n",
    "<img src=\"../rnn_cs231n/cs231n15.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Bidirectional LSTM</h6>\n",
    "https://arxiv.org/pdf/1711.05717.pdf\n",
    "<h6>Squeeze and Excitation Networks</h6>\n",
    "https://arxiv.org/pdf/1709.01507.pdf\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
