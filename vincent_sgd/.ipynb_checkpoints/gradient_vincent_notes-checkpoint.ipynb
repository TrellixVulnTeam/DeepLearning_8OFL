{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#From the Google Class Vincent Van...\n",
    "<h6>Vincent Gradient</h6>\n",
    "<h6>Numerical Stability</h6>\n",
    "Take 1B, add 10^-6 1M times so you are really adding 1, print result. \n",
    ".95 is not 1. \n",
    "<p>\n",
    "\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.95367431640625\n"
     ]
    }
   ],
   "source": [
    "a = 1000000000\n",
    "for i in range(1000000):\n",
    "    a = a + 1e-6\n",
    "print(a - 1000000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "If we replace the 1B number with 1 we see the error becomes very small. This is an argument for\n",
    "normalization of input values to be [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.999999999177334e-07\n"
     ]
    }
   ],
   "source": [
    "a = 1\n",
    "for i in range(1):\n",
    "    a = a + 1e-6\n",
    "print(a - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h6>Gradients</h6>\n",
    "<img src=\"vincent_grad1.png\">\n",
    "<p>\n",
    "When computing the gradient for gradient descent, if computing the loss takes N floating point operations then computing\n",
    "the gradient takes 3x the number of operations. The loss function consists of computing the loss or floating \n",
    "point operations over all the training set examples. This dataset has to be big by definition for deep learning to\n",
    "work. \n",
    "</p>\n",
    "<p>\n",
    "To reduce the amount of computation we sample the training set instead of processing all the training examples. We saw\n",
    "this earlier on the derivative page. This sampling is called stochastic gradient descent. Not only are we going to sample\n",
    "but we are going to compute the average loss for a random set of the training data. Random is key. If the way we \n",
    "pick the samples is not random enough this no longer works. \n",
    "</p>\n",
    "<img src=\"vincent_grad2.png\">\n",
    "<p>\n",
    "This gradient is not in the same direction as the actual gradient calculated over all the training samples \n",
    "but it should be close and as we see above it eventually gets us close to the target. The key is we are going to do\n",
    "this many many times taking very very very small steps each time. This is important. The small steps help us converge. \n",
    "</p>\n",
    "<img src=\"vincent_grad3.png\">\n",
    "<img src=\"vincent_grad4.png\">\n",
    "<img src=\"vincent_grad5.png\">\n",
    "Learning rate decay field of active reasearch. Some like to reduce LR when loss reaches plateau, some like to reduce\n",
    "on each step. \n",
    "<img src=\"vincent_grad6.png\">\n",
    "<h6>Summary</h6>\n",
    "<li>Normalize input to 0 mean and equal variance. A random variable with samles will not have variation\n",
    "in the variance values for equal variance. Or a fixed small standard deviation. </li>\n",
    "<li>Learning rate decay. Worst case is reduce the jitter at end of training</li>\n",
    "<li>Weight initialization to small distribituion to keep output distribution uncertain</li>\n",
    "<li>images are normalized by subracting 128 and dividing by 128</li>\n",
    "<li>random sample the training set for each batch and calculate the average gradient</li>\n",
    "<li>use momentuim which is the running average of the gradients. Do not use the sampled gradient directly. Gradient\n",
    "= .9 *momentum + new sampled gradient </li>\n",
    "<li></li>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "<h6>Numerical Stable Gradients</h6>\n",
    "log_sum_exp_over_rows\n",
    " % This computes log(sum(exp(a), 1)) in a numerically stable way\n",
    "  maxs_small = max(a, [], 1);\n",
    "  maxs_big = repmat(maxs_small, [size(a, 1), 1]);\n",
    "  ret = log(sum(exp(a - maxs_big), 1)) + maxs_small;\n",
    "\n",
    " % The following three lines of code implement the softmax.\n",
    "  % However, it's written differently from what the lectures say.\n",
    "  % In the lectures, a softmax is described using an exponential divided by a sum of exponentials.\n",
    "  % What we do here is exactly equivalent (you can check the math or just check it in practice), but this is more numerically stable. \n",
    "  % \"Numerically stable\" means that this way, there will never be really big numbers involved.\n",
    "  % The exponential in the lectures can lead to really big numbers, which are fine in mathematical equations, but can lead to all sorts of problems in Octave.\n",
    "  % Octave isn't well prepared to deal with really large numbers, like the number 10 to the power 1000. Computations with such numbers get unstable, so we avoid them.\n",
    "  class_normalizer = log_sum_exp_over_rows(class_input); % log(sum(exp of class_input)) is what we subtract to get properly normalized log class probabilities. size: <1> by <number of data cases>\n",
    "  log_class_prob = class_input - repmat(class_normalizer, [size(class_input, 1), 1]); % log of probability of each class. size: <number of classes, i.e. 10> by <number of data cases>\n",
    "  class_prob = exp(log_class_prob); % probability of each class. Each column (i.e. each case) sums to 1. size: <number of classes, i.e. 10> by <number of data cases>\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "<h6>Torch Code/tf Code</h6>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'__globals__': [], '__version__': '1.0', '__header__': b'MATLAB 5.0 MAT-file, written by Octave 3.2.4, 2012-10-31 21:15:39 UTC', 'data': <scipy.io.matlab.mio5_params.mat_struct object at 0x11f8d1e48>}\n",
      "<class 'scipy.io.matlab.mio5_params.mat_struct'>\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print (train[0][0][0][0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
