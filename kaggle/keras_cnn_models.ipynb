{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Load pickled data\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle, cv2, csv, os, time\n",
    "import numpy as np\n",
    "from numpy import newaxis\n",
    "from random import randint\n",
    "import tensorflow as tf\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import gridspec\n",
    "%matplotlib inline\n",
    "\n",
    "import copy\n",
    "from keras.models import Sequential #, Graph\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten, Reshape\n",
    "from keras.optimizers import SGD, RMSprop\n",
    "from keras.utils import np_utils\n",
    "from keras.regularizers import l2\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D, AveragePooling2D\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.applications import ResNet50\n",
    "from keras.applications import InceptionV3\n",
    "from keras.applications import Xception # TensorFlow ONLY\n",
    "from keras.applications import VGG16\n",
    "from keras.applications import VGG19\n",
    "from keras.applications import imagenet_utils\n",
    "from keras.applications.inception_v3 import preprocess_input\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.preprocessing.image import load_img\n",
    "import argparse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] -i IMAGE [-model MODEL]\n",
      "ipykernel_launcher.py: error: the following arguments are required: -i/--image\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dc/anaconda/envs/py35/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2855: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "ap = argparse.ArgumentParser()\n",
    "ap.add_argument(\"-i\", \"--image\", required=True,help=\"path to the input image\")\n",
    "ap.add_argument(\"-model\", \"--model\", type=str, default=\"vgg16\",help=\"name of pre-trained network to use\")\n",
    "args = vars(ap.parse_args())\n",
    "# define a dictionary that maps model names to their classes\n",
    "# inside Keras\n",
    "MODELS = {\n",
    "    \"vgg16\": VGG16,\n",
    "    \"vgg19\": VGG19,\n",
    "    \"inception\": InceptionV3,\n",
    "    \"xception\": Xception, # TensorFlow ONLY\n",
    "    \"resnet\": ResNet50\n",
    "}\n",
    " \n",
    "# esnure a valid model name was supplied via command line argument\n",
    "if args[\"model\"] not in MODELS.keys():\n",
    "    raise AssertionError(\"The --model command line argument should \"\n",
    "        \"be a key in the `MODELS` dictionary\")\n",
    "    \n",
    "# initialize the input image shape (224x224 pixels) along with\n",
    "# the pre-processing function (this might need to be changed\n",
    "# based on which model we use to classify our image)\n",
    "inputShape = (224, 224)\n",
    "preprocess = imagenet_utils.preprocess_input\n",
    " \n",
    "# if we are using the InceptionV3 or Xception networks, then we\n",
    "# need to set the input shape to (299x299) [rather than (224x224)]\n",
    "# and use a different image processing function\n",
    "if args[\"model\"] in (\"inception\", \"xception\"):\n",
    "    inputShape = (299, 299)\n",
    "    preprocess = preprocess_input\n",
    "\n",
    "    \n",
    "# load our the network weights from disk (NOTE: if this is the\n",
    "# first time you are running this script for a given network, the\n",
    "# weights will need to be downloaded first -- depending on which\n",
    "# network you are using, the weights can be 90-575MB, so be\n",
    "# patient; the weights will be cached and subsequent runs of this\n",
    "# script will be *much* faster)\n",
    "print(\"[INFO] loading {}...\".format(args[\"model\"]))\n",
    "Network = MODELS[args[\"model\"]]\n",
    "model = Network(weights=\"imagenet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n"
     ]
    }
   ],
   "source": [
    "import h5py as h5py\n",
    "\n",
    "model_vgg16_conv = VGG16(weights='imagenet', include_top=True)\n",
    "model_vgg16_conv.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer_0\n",
      "layer_1\n",
      "layer_10\n",
      "layer_11\n",
      "layer_12\n",
      "layer_13\n",
      "layer_14\n",
      "layer_15\n",
      "layer_16\n",
      "layer_17\n",
      "layer_18\n",
      "layer_19\n",
      "layer_2\n",
      "layer_20\n",
      "layer_21\n",
      "layer_22\n",
      "layer_23\n",
      "layer_24\n",
      "layer_25\n",
      "layer_26\n",
      "layer_27\n",
      "layer_28\n",
      "layer_29\n",
      "layer_3\n",
      "layer_30\n",
      "layer_31\n",
      "layer_32\n",
      "layer_33\n",
      "layer_34\n",
      "layer_35\n",
      "layer_36\n",
      "layer_4\n",
      "layer_5\n",
      "layer_6\n",
      "layer_7\n",
      "layer_8\n",
      "layer_9\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "f=h5py.File('vgg16_weights.h5')\n",
    "for name in f:\n",
    "    print(name) #need all keys for f. \n",
    "#now what\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layer_names = [layer.name for layer in model_vgg16_conv.layers]\n",
    "print (layer_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
