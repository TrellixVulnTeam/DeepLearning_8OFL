{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "\"\"\"\n",
    "# Fully Differentiable Deep Neural Decision Forest\n",
    "This is an implementation of a simple modification to the deep-neural decision\n",
    "forest [Kontschieder et al.] usng TensorFlow. The modification allows the joint\n",
    "optimization of decision nodes and leaf nodes which speeds up the training\n",
    "(haven't compared yet).\n",
    "## Motivation:\n",
    "Deep Neural Deicision Forest, ICCV 2015, proposed a great way to incorporate a\n",
    "neural network with a decision forest. During the optimization (training), the\n",
    "terminal (leaf) node has to be updated after each epoch.\n",
    "This alternating optimization scheme is usually slower than joint optimization\n",
    "since other variable that is not being optimized slows down the optimization.\n",
    "This code is just a proof-of-concept that\n",
    "1. one can train both decision nodes and leaf nodes $\\pi$ jointly using\n",
    "parametric formulation of leaf node.\n",
    "2. one can implement the idea in a symbolic math library very easily.\n",
    "## Formulation\n",
    "The leaf node probability can be parametrized using a $softmax(W_{leaf})$.\n",
    "i.e. let a vector $W_{leaf} \\in \\mathbb{R}^N$ where N is the number of classes.\n",
    "Then taking the soft max operation on W_{leaf} would give us\n",
    "$$\n",
    "softmax(W_{leaf}) = \\frac{e^{-w_i}}{\\sum_j e^{-w_j}}\n",
    "$$\n",
    "which is always in a simplex. Thus, without any constraint, we can parametrize\n",
    "the leaf nodes and can compute the gradient of $L$ w.r.t $W_{leaf}$. This\n",
    "allows us to jointly optimize both leaf nodes and decision nodes.\n",
    "## Experiment\n",
    "I tested a simple (3 convolution + 2 fc) network for the experiment.\n",
    "On the MNIST, the simple Deep-NDF got 99.5% accuracy on the test set after 100\n",
    "epochs of training. After just 10 epochs, it reaches 99.1% and after 100\n",
    "epochs, it reaches 99.5%\n",
    "The following is the number of epoch and training accuracy after each epoch.\n",
    "```\n",
    "0 0.955829326923\n",
    "1 0.979166666667\n",
    "2 0.982572115385\n",
    "3 0.988080929487\n",
    "4 0.988181089744\n",
    "5 0.988481570513\n",
    "6 0.987980769231\n",
    "7 0.989583333333\n",
    "8 0.991185897436\n",
    "9 0.991586538462\n",
    "10 0.991987179487\n",
    "11 0.992888621795\n",
    "12 0.993088942308\n",
    "13 0.992988782051\n",
    "14 0.992988782051\n",
    "15 0.992588141026\n",
    "16 0.993289262821\n",
    "17 0.99358974359\n",
    "18 0.992387820513\n",
    "19 0.993790064103\n",
    "20 0.994090544872\n",
    "21 0.993289262821\n",
    "22 0.993489583333\n",
    "23 0.99358974359\n",
    "24 0.993990384615\n",
    "25 0.993689903846\n",
    "26 0.99469150641\n",
    "27 0.994491185897\n",
    "28 0.994090544872\n",
    "29 0.994090544872\n",
    "30 0.99469150641\n",
    "31 0.994090544872\n",
    "32 0.994791666667\n",
    "33 0.993790064103\n",
    "34 0.994190705128\n",
    "35 0.994591346154\n",
    "36 0.993990384615\n",
    "37 0.995092147436\n",
    "38 0.994391025641\n",
    "39 0.993389423077\n",
    "40 0.994991987179\n",
    "41 0.994991987179\n",
    "42 0.994991987179\n",
    "43 0.994491185897\n",
    "44 0.995192307692\n",
    "45 0.995192307692\n",
    "46 0.994791666667\n",
    "47 0.995092147436\n",
    "48 0.994991987179\n",
    "49 0.994290865385\n",
    "50 0.994591346154\n",
    "51 0.994791666667\n",
    "52 0.995092147436\n",
    "53 0.995492788462\n",
    "54 0.994591346154\n",
    "55 0.995092147436\n",
    "56 0.994190705128\n",
    "57 0.99469150641\n",
    "58 0.99469150641\n",
    "59 0.994090544872\n",
    "60 0.994290865385\n",
    "61 0.994891826923\n",
    "62 0.994791666667\n",
    "63 0.994491185897\n",
    "64 0.994591346154\n",
    "65 0.994290865385\n",
    "66 0.99469150641\n",
    "67 0.994391025641\n",
    "68 0.994791666667\n",
    "69 0.99469150641\n",
    "70 0.994791666667\n",
    "71 0.994591346154\n",
    "72 0.994891826923\n",
    "73 0.994791666667\n",
    "74 0.995192307692\n",
    "75 0.995392628205\n",
    "76 0.995392628205\n",
    "77 0.995292467949\n",
    "78 0.994791666667\n",
    "79 0.995092147436\n",
    "80 0.995392628205\n",
    "81 0.994891826923\n",
    "82 0.995092147436\n",
    "83 0.994891826923\n",
    "84 0.995092147436\n",
    "85 0.995092147436\n",
    "86 0.995292467949\n",
    "87 0.994891826923\n",
    "88 0.995693108974\n",
    "89 0.994391025641\n",
    "90 0.994591346154\n",
    "91 0.995592948718\n",
    "92 0.995292467949\n",
    "93 0.995192307692\n",
    "94 0.994791666667\n",
    "95 0.995192307692\n",
    "96 0.995092147436\n",
    "97 0.994591346154\n",
    "98 0.995292467949\n",
    "99 0.995392628205\n",
    "```\n",
    "## Slides\n",
    "[SDL Reading Group Slides](https://docs.google.com/presentation/d/1Ze7BAiWbMPyF0ax36D-aK00VfaGMGvvgD_XuANQW1gU/edit?usp=sharing)\n",
    "## References\n",
    "[Kontschieder et al.] Deep Neural Decision Forests, ICCV 2015\n",
    "## License\n",
    "The MIT License (MIT)\n",
    "Copyright (c) 2016 Christopher B. Choy (chrischoy@ai.stanford.edu)\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "of this software and associated documentation files (the \"Software\"), to deal\n",
    "in the Software without restriction, including without limitation the rights\n",
    "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "copies of the Software, and to permit persons to whom the Software is\n",
    "furnished to do so, subject to the following conditions:\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "SOFTWARE.\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow.examples.tutorials.mnist.input_data as input_data\n",
    "\n",
    "DEPTH   = 3                 # Depth of a tree\n",
    "N_LEAF  = 2 ** (DEPTH + 1)  # Number of leaf node\n",
    "N_LABEL = 10                # Number of classes\n",
    "N_TREE  = 5                 # Number of trees (ensemble)\n",
    "N_BATCH = 128               # Number of data points per mini-batch\n",
    "\n",
    "\n",
    "def init_weights(shape):\n",
    "    return tf.Variable(tf.random_normal(shape, stddev=0.01))\n",
    "\n",
    "\n",
    "def init_prob_weights(shape, minval=-5, maxval=5):\n",
    "    return tf.Variable(tf.random_uniform(shape, minval, maxval))\n",
    "\n",
    "\n",
    "def model(X, w, w2, w3, w4_e, w_d_e, w_l_e, p_keep_conv, p_keep_hidden):\n",
    "    \"\"\"\n",
    "    Create a forest and return the neural decision forest outputs:\n",
    "        decision_p_e: decision node routing probability for all ensemble\n",
    "            If we number all nodes in the tree sequentially from top to bottom,\n",
    "            left to right, decision_p contains\n",
    "            [d(0), d(1), d(2), ..., d(2^n - 2)] where d(1) is the probability\n",
    "            of going left at the root node, d(2) is that of the left child of\n",
    "            the root node.\n",
    "            decision_p_e is the concatenation of all tree decision_p's\n",
    "        leaf_p_e: terminal node probability distributions for all ensemble. The\n",
    "            indexing is the same as that of decision_p_e.\n",
    "    \"\"\"\n",
    "    assert(len(w4_e) == len(w_d_e))\n",
    "    assert(len(w4_e) == len(w_l_e))\n",
    "\n",
    "    l1a = tf.nn.relu(tf.nn.conv2d(X, w, [1, 1, 1, 1], 'SAME'))\n",
    "    l1 = tf.nn.max_pool(l1a, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n",
    "    l1 = tf.nn.dropout(l1, p_keep_conv)\n",
    "\n",
    "    l2a = tf.nn.relu(tf.nn.conv2d(l1, w2, [1, 1, 1, 1], 'SAME'))\n",
    "    l2 = tf.nn.max_pool(l2a, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n",
    "    l2 = tf.nn.dropout(l2, p_keep_conv)\n",
    "\n",
    "    l3a = tf.nn.relu(tf.nn.conv2d(l2, w3, [1, 1, 1, 1], 'SAME'))\n",
    "    l3 = tf.nn.max_pool(l3a, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "    l3 = tf.reshape(l3, [-1, w4_e[0].get_shape().as_list()[0]])\n",
    "    l3 = tf.nn.dropout(l3, p_keep_conv)\n",
    "\n",
    "    decision_p_e = []\n",
    "    leaf_p_e = []\n",
    "    for w4, w_d, w_l in zip(w4_e, w_d_e, w_l_e):\n",
    "        l4 = tf.nn.relu(tf.matmul(l3, w4))\n",
    "        l4 = tf.nn.dropout(l4, p_keep_hidden)\n",
    "\n",
    "        decision_p = tf.nn.sigmoid(tf.matmul(l4, w_d))\n",
    "        leaf_p = tf.nn.softmax(w_l)\n",
    "\n",
    "        decision_p_e.append(decision_p)\n",
    "        leaf_p_e.append(leaf_p)\n",
    "\n",
    "    return decision_p_e, leaf_p_e\n",
    "\n",
    "##################################################\n",
    "# Load dataset\n",
    "##################################################\n",
    "mnist = input_data.read_data_sets(\"MNIST/\", one_hot=True)\n",
    "trX, trY = mnist.train.images, mnist.train.labels\n",
    "teX, teY = mnist.test.images, mnist.test.labels\n",
    "trX = trX.reshape(-1, 28, 28, 1)\n",
    "teX = teX.reshape(-1, 28, 28, 1)\n",
    "\n",
    "# Input X, output Y\n",
    "X = tf.placeholder(\"float\", [N_BATCH, 28, 28, 1])\n",
    "Y = tf.placeholder(\"float\", [N_BATCH, N_LABEL])\n",
    "\n",
    "##################################################\n",
    "# Initialize network weights\n",
    "##################################################\n",
    "w = init_weights([3, 3, 1, 32])\n",
    "w2 = init_weights([3, 3, 32, 64])\n",
    "w3 = init_weights([3, 3, 64, 128])\n",
    "\n",
    "w4_ensemble = []\n",
    "w_d_ensemble = []\n",
    "w_l_ensemble = []\n",
    "for i in range(N_TREE):\n",
    "    w4_ensemble.append(init_weights([128 * 4 * 4, 625]))\n",
    "    w_d_ensemble.append(init_prob_weights([625, N_LEAF], -1, 1))\n",
    "    w_l_ensemble.append(init_prob_weights([N_LEAF, N_LABEL], -2, 2))\n",
    "\n",
    "p_keep_conv = tf.placeholder(\"float\")\n",
    "p_keep_hidden = tf.placeholder(\"float\")\n",
    "\n",
    "##################################################\n",
    "# Define a fully differentiable deep-ndf\n",
    "##################################################\n",
    "# With the probability decision_p, route a sample to the right branch\n",
    "decision_p_e, leaf_p_e = model(X, w, w2, w3, w4_ensemble, w_d_ensemble,\n",
    "                               w_l_ensemble, p_keep_conv, p_keep_hidden)\n",
    "\n",
    "flat_decision_p_e = []\n",
    "\n",
    "# iterate over each tree\n",
    "for decision_p in decision_p_e:\n",
    "    # Compute the complement of d, which is 1 - d\n",
    "    # where d is the sigmoid of fully connected output\n",
    "    decision_p_comp = tf.sub(tf.ones_like(decision_p), decision_p)\n",
    "\n",
    "    # Concatenate both d, 1-d\n",
    "    decision_p_pack = tf.pack([decision_p, decision_p_comp])\n",
    "\n",
    "    # Flatten/vectorize the decision probabilities for efficient indexing\n",
    "    flat_decision_p = tf.reshape(decision_p_pack, [-1])\n",
    "    flat_decision_p_e.append(flat_decision_p)\n",
    "\n",
    "# 0 index of each data instance in a mini-batch\n",
    "batch_0_indices = \\\n",
    "    tf.tile(tf.expand_dims(tf.range(0, N_BATCH * N_LEAF, N_LEAF), 1),\n",
    "            [1, N_LEAF])\n",
    "\n",
    "###############################################################################\n",
    "# The routing probability computation\n",
    "#\n",
    "# We will create a routing probability matrix \\mu. First, we will initialize\n",
    "# \\mu using the root node d, 1-d. To efficiently implement this routing, we\n",
    "# will create a giant vector (matrix) that contains all d and 1-d from all\n",
    "# decision nodes. The matrix version of that is decision_p_pack and vectorized\n",
    "# version is flat_decision_p.\n",
    "#\n",
    "# The suffix `_e` indicates an ensemble. i.e. concatenation of all responsens\n",
    "# from trees.\n",
    "#\n",
    "# For depth = 2 tree, the routing probability for each leaf node can be easily\n",
    "# compute by multiplying the following vectors elementwise.\n",
    "# \\mu =       [d_0,   d_0,   d_0,   d_0, 1-d_0, 1-d_0, 1-d_0, 1-d_0]\n",
    "# \\mu = \\mu * [d_1,   d_1, 1-d_1, 1-d_1,   d_2,   d_2, 1-d_2, 1-d_2]\n",
    "# \\mu = \\mu * [d_3, 1-d_3,   d_4, 1-d_4,   d_5, 1-d_5,   d_6, 1-d_6]\n",
    "#\n",
    "# Tree indexing\n",
    "#      0\n",
    "#    1   2\n",
    "#   3 4 5 6\n",
    "##############################################################################\n",
    "in_repeat = N_LEAF / 2\n",
    "out_repeat = N_BATCH\n",
    "\n",
    "# Let N_BATCH * N_LEAF be N_D. flat_decision_p[N_D] will return 1-d of the\n",
    "# first root node in the first tree.\n",
    "batch_complement_indices = \\\n",
    "    np.array([[0] * in_repeat, [N_BATCH * N_LEAF] * in_repeat]\n",
    "             * out_repeat).reshape(N_BATCH, N_LEAF)\n",
    "\n",
    "# First define the routing probabilities d for root nodes\n",
    "mu_e = []\n",
    "\n",
    "# iterate over each tree\n",
    "for i, flat_decision_p in enumerate(flat_decision_p_e):\n",
    "    mu = tf.gather(flat_decision_p,\n",
    "                   tf.add(batch_0_indices, batch_complement_indices))\n",
    "    mu_e.append(mu)\n",
    "\n",
    "# from the second layer to the last layer, we make the decision nodes\n",
    "for d in xrange(1, DEPTH + 1):\n",
    "    indices = tf.range(2 ** d, 2 ** (d + 1)) - 1\n",
    "    tile_indices = tf.reshape(tf.tile(tf.expand_dims(indices, 1),\n",
    "                                      [1, 2 ** (DEPTH - d + 1)]), [1, -1])\n",
    "    batch_indices = tf.add(batch_0_indices, tf.tile(tile_indices, [N_BATCH, 1]))\n",
    "\n",
    "    in_repeat = in_repeat / 2\n",
    "    out_repeat = out_repeat * 2\n",
    "\n",
    "    # Again define the indices that picks d and 1-d for the node\n",
    "    batch_complement_indices = \\\n",
    "        np.array([[0] * in_repeat, [N_BATCH * N_LEAF] * in_repeat]\n",
    "                 * out_repeat).reshape(N_BATCH, N_LEAF)\n",
    "\n",
    "    mu_e_update = []\n",
    "    for mu, flat_decision_p in zip(mu_e, flat_decision_p_e):\n",
    "        mu = tf.mul(mu, tf.gather(flat_decision_p,\n",
    "                                  tf.add(batch_indices, batch_complement_indices)))\n",
    "        mu_e_update.append(mu)\n",
    "\n",
    "    mu_e = mu_e_update\n",
    "\n",
    "##################################################\n",
    "# Define p(y|x)\n",
    "##################################################\n",
    "py_x_e = []\n",
    "for mu, leaf_p in zip(mu_e, leaf_p_e):\n",
    "    # average all the leaf p\n",
    "    py_x_tree = tf.reduce_mean(\n",
    "        tf.mul(tf.tile(tf.expand_dims(mu, 2), [1, 1, N_LABEL]),\n",
    "               tf.tile(tf.expand_dims(leaf_p, 0), [N_BATCH, 1, 1])), 1)\n",
    "    py_x_e.append(py_x_tree)\n",
    "\n",
    "py_x_e = tf.pack(py_x_e)\n",
    "py_x = tf.reduce_mean(py_x_e, 0)\n",
    "\n",
    "##################################################\n",
    "# Define cost and optimization method\n",
    "##################################################\n",
    "\n",
    "# cross entropy loss\n",
    "cost = tf.reduce_mean(-tf.mul(tf.log(py_x), Y))\n",
    "\n",
    "# cost = tf.reduce_mean(tf.nn.cross_entropy_with_logits(py_x, Y))\n",
    "train_step = tf.train.RMSPropOptimizer(0.001, 0.9).minimize(cost)\n",
    "predict = tf.argmax(py_x, 1)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.initialize_all_variables())\n",
    "\n",
    "for i in range(100):\n",
    "    # One epoch\n",
    "    for start, end in zip(range(0, len(trX), N_BATCH), range(N_BATCH, len(trX), N_BATCH)):\n",
    "        sess.run(train_step, feed_dict={X: trX[start:end], Y: trY[start:end],\n",
    "                                        p_keep_conv: 0.8, p_keep_hidden: 0.5})\n",
    "\n",
    "    # Result on the test set\n",
    "    results = []\n",
    "    for start, end in zip(range(0, len(teX), N_BATCH), range(N_BARCH, len(teX), N_BATCH)):\n",
    "        results.extend(np.argmax(teY[start:end], axis=1) ==\n",
    "            sess.run(predict, feed_dict={X: teX[start:end], p_keep_conv: 1.0,\n",
    "                                         p_keep_hidden: 1.0}))\n",
    "\n",
    "    print 'Epoch: %d, Test Accuracy: %f' % (i + 1, np.mean(results))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf35]",
   "language": "python",
   "name": "conda-env-tf35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
